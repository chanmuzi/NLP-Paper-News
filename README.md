ğŸ“œ: Paper link
ğŸ§‘ğŸ»â€ğŸ’»: Developer blog & Github link
ğŸ—ï¸: News

---
# 2024
## â˜ƒ February
<details>
  <summary>1st ~ 3rd week</summary>

- ğŸ“œÂ [Cohere] [Aya Model: An Instruction Finetuned Open-Access Multilingual Language Model](https://arxiv.org/abs/2402.07827)
    - 119ê°œêµ­, 3,000ì—¬ ëª…ì˜ ì—°êµ¬ìê°€ ì°¸ì—¬í•œ ë‹¤êµ­ì–´ ëª¨ë¸ ì—°êµ¬ í”„ë¡œì íŠ¸ì˜ ê²°ê³¼ë¬¼. ë°ì´í„°ì…‹ë„ ì˜¤í”ˆì†ŒìŠ¤ë¡œ ì œê³µ (513M ê°œ instruction fine-tuning ë°ì´í„°ì…‹)
- ğŸ“œÂ [OS-Copilot: Towards Generalist Computer Agents with Self-Improvement](https://arxiv.org/abs/2402.07456)
- ğŸ§‘ğŸ»â€ğŸ’»Â [OpenAI] [Memory and new controls for ChatGPT](https://openai.com/blog/memory-and-new-controls-for-chatgpt)
    - ChatGPTë¥¼ ì´ìš©í•  ë•Œ ê³¼ê±°ì˜ ì±„íŒ… ë‚´ì—­ì„ í˜„ì¬ ì±„íŒ…ì—ì„œì˜ memoryë¡œ í™œìš©í•˜ì—¬ ê°œì¸ ë§ì¶¤ìœ¼ë¡œ ë§Œë“¤ ìˆ˜ ìˆë‹¤. ì•„ì§ ì¼ë¶€ ìœ ì € ëŒ€ìƒìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ì¤‘ì¸ ê¸°ëŠ¥.
- ğŸ§‘ğŸ»â€ğŸ’»Â [NVIDIA] [Say What? Chat With RTX Brings Custom Chatbot to NVIDIA RTX AI PCs](https://blogs.nvidia.com/blog/chat-with-rtx-available-now/)
- ğŸ—ï¸Â [Nvidia briefly beats Amazon and nears Alphabetâ€™s market cap amid AI hype](https://aibeat.co/nvidia-briefly-beats-amazon-in-market-value/)
- ğŸ§‘ğŸ»â€ğŸ’»Â [DeepLearning.AI] [Serverless LLM apps with Amazon Bedrock](https://www.deeplearning.ai/short-courses/serverless-llm-apps-amazon-bedrock/)
- ğŸ“œÂ [On the Self-Verification Limitations of Large Language Models on Reasoning and Planning Tasks](https://arxiv.org/abs/2402.08115)
- ğŸ“œÂ [Google DeepMind] [Transformers Can Achieve Length Generalization But Not Robustly](https://arxiv.org/abs/2402.09371)
    - íŠ¸ëœìŠ¤í¬ë¨¸ë„ ì œí•œì ìœ¼ë¡œ ì…ë ¥ ê¸¸ì´ë¥¼ ëŠ˜ë¦´(extrapolate) ìˆ˜ ìˆë‹¤. (ì•½ 2.5ë°°). í•˜ì§€ë§Œ ì¼ë°˜í™” ê°€ëŠ¥í•œ ì„¸íŒ…ì€ ì•„ë‹˜.
- ğŸ“œÂ [Google DeepMind] [Chain-of-Thought Reasoning Without Prompting](https://arxiv.org/abs/2402.10200)
    - ë§ ê·¸ëŒ€ë¡œ í”„ë¡¬í”„íŠ¸ ì—†ì´ CoT Reasoningì„ ìœ ë„í•  ìˆ˜ ìˆë‹¤. Decoding processë¥¼ ì¡°ì •í•¨
- ğŸ§‘ğŸ»â€ğŸ’»Â [Google] [Our next-generation model: Gemini 1.5](https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/)
    - ë¬´ë ¤ ì…ë ¥ì„ 1M í† í°ìœ¼ë¡œ ë°›ì„ ìˆ˜ ìˆë‹¤ê³  ì£¼ì¥í•˜ëŠ” Gemini 1.5 ë²„ì „ì´ ë“±ì¥. ë°°í¬ ì¤€ë¹„ëŠ” ë˜ì—ˆìœ¼ë‚˜ ì•„ì§ ë°°í¬í•˜ì§€ ì•Šì€ ê²ƒìœ¼ë¡œ ì•Œë ¤ì§.
- ğŸ§‘ğŸ»â€ğŸ’»Â [OpenAI] [Sora: Creating video from text](https://openai.com/sora)
    - OpenAIì—ì„œ ë§Œë“  ìµœì´ˆì˜ Text-to-Video ëª¨ë¸. ì…ì´ ë–¡ ë²Œì–´ì§ˆ ì •ë„ì˜ ì„±ëŠ¥ìœ¼ë¡œ ì—¬ëŸ¬ ì»¤ë®¤ë‹ˆí‹°ì—ì„œ í™”ì œë¥¼ ë¶ˆëŸ¬ì¼ìœ¼í‚¤ëŠ” ì¤‘.
- ğŸ“œÂ [Apple] [Guiding Instruction-based Image Editing via Multimodal Large Language Models](https://arxiv.org/abs/2309.17102)
    - ì´ë¯¸ì§€ í¸ì§‘ì— ìˆì–´ì„œ ì „ë¬¸ì ì¸ ì§€ì‹ ì—†ì´ í…ìŠ¤íŠ¸ë§Œì„ ì´ìš©í•˜ëŠ”ë° ê·¸ ê²°ê³¼ë¬¼ì´ ì•„ì£¼ ë›°ì–´ë‚¨. ICLRâ€™24 Spotlight ë…¼ë¬¸.
- ğŸ“œÂ [Using Counterfactual Tasks to Evaluate the Generality of Analogical Reasoning in Large Language Models](https://arxiv.org/abs/2402.08955)
- ğŸ—ï¸Â [Slack AI is here, letting you catch up on lengthy threads and unread messages](https://www.theverge.com/2024/2/14/24070590/slack-ai-launch-thread-summaries-search-recap)
    - ì½ì§€ ì•Šì€ ìŠ¤ë ˆë“œ ìš”ì•½ ê¸°ëŠ¥. ì•„ì§ UK & USì—ì„œë§Œ ì´ìš© ê°€ëŠ¥
- ğŸ“œÂ [Google DeepMind & Research] [A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts](https://arxiv.org/abs/2402.09727)
    - [gist memories]ì— ì—í”¼ì†Œë“œë¥¼ ì €ì¥í•˜ì—¬ ReadAgentê°€ taskì™€ ê´€ë ¨ ìˆëŠ” ì •ë³´ë¥¼ ë¹ ë¥´ê²Œ ê°€ì ¸ì˜¤ë„ë¡ í•˜ëŠ” ë°©ì‹. ì‚¬ëŒì´ ê¸´ ê¸€ì„ ì½ëŠ” ë°©ì‹ì—ì„œ ì°©ì•ˆ.
- ğŸ“œÂ [DoRA: Weight-Decomposed Low-Rank Adaptation](https://arxiv.org/abs/2402.09353)
    - LoRAì™€ FT ì‚¬ì´ì˜ gapì„ ì¤„ì´ê¸° ìœ„í•´ pre-trained weightë¥¼ magnitudeì™€ directionìœ¼ë¡œ ë¶„í•´í•˜ëŠ” ë°©ë²•ì„ ë„ì…
- ğŸ“œÂ [Can We Verify Step by Step for Incorrect Answer Detection?](https://arxiv.org/abs/2402.10528)
    - CoTì˜ ê° stepì— ëŒ€í•´ process discernibility score (PDS)ë¥¼ êµ¬í•˜ì—¬ answer-checking baselineì„ ì œê³µ
- ğŸ§‘ğŸ»â€ğŸ’»Â [minbpe](https://github.com/karpathy/minbpe)
    - Karpathyê°€ OpenAIë¥¼ í‡´ì‚¬í•˜ë©° ê³µê°œí•œ BPE ì½”ë“œ. ë‚˜ë§Œì˜ í† í¬ë‚˜ì´ì €ë¥¼ ë§Œë“¤ ìˆ˜ ìˆë‹¤.
- ğŸ§‘ğŸ»â€ğŸ’»Â [Meta] [V-JEPA](https://ai.meta.com/research/publications/revisiting-feature-prediction-for-learning-visual-representations-from-video/)
    - ì•„ì£¼ ì ì€ ì–‘ì˜ labeled dataë¡œ self-superviseí•œ ëª¨ë¸ë¡œ, ìƒì„±í˜•ì´ ì•„ë‹˜. ìƒˆë¡œìš´ ì»¨ì…‰ Joint Embedding Predictive Architectureë¥¼ ì œì•ˆ.
</details>

<details>
  <summary>4th week</summary>
  
- ğŸ“œÂ [Linear Transformers with Learnable Kernel Functions are Better In-Context Models](https://arxiv.org/abs/2402.10644)
  - Transformer ê¸°ë°˜ì˜ ëª¨ë¸ë“¤ì˜ ì„±ëŠ¥ì„ ëŠ¥ê°€í•œë‹¤ê³  ì œì•ˆë˜ì—ˆë˜ State Space Modelsì—ê²Œ ë¶€ì¡±í•œ In-Context Learning ëŠ¥ë ¥ì„ ì±„ì›Œì£¼ê¸° ìœ„í•œ ë°©ë²•ì„ ë„ì…. Taylor Expansionì„ í™œìš©.
- ğŸ“œÂ [DataDreamer: A Tool for Synthetic Data Generation and Reproducible LLM Workflows](https://arxiv.org/abs/2402.10379)
    - LLM í•™ìŠµì— í™œìš©ë˜ëŠ” ë°ì´í„°ì…‹ ê´€ë ¨ ì›Œí¬ í”Œë¡œìš°ë¥¼ ì¬í˜„ ê°€ëŠ¥í•˜ë„ë¡ ë„ì™€ì£¼ëŠ” í”„ë ˆì„ì›Œí¬. íŠ¹íˆ í•©ì„± ë°ì´í„° ìƒì„±ì´ í¬í•¨ëœ ê²ƒì´ íŠ¹ì§•.
- ğŸ“œÂ [AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling](https://arxiv.org/abs/2402.12226)
    - ìŒì„±, í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ìŒì•…ì„ discrete tokenìœ¼ë¡œ ì…ë ¥ ë°›ì•„ autoregressiveí•˜ê²Œ ì²˜ë¦¬í•˜ëŠ” ë©€í‹°ëª¨ë‹¬ ëª¨ë¸. ë°ì´í„° ìˆ˜ì¤€ì˜ ì „ì²˜ë¦¬ë§Œ í•„ìš”.
- ğŸ“œÂ [Direct Evaluation of Chain-of-Thought in Multi-hop Reasoning with Knowledge Graphs](https://arxiv.org/abs/2402.11199)
    - Knowledge Graphë¥¼ í™œìš©í•˜ì—¬ ì˜¬ë°”ë¥¸ ì¶”ë¡  ê³¼ì •ì„ í†µí•´ ìµœì¢… ì •ë‹µì´ ë„ì¶œë˜ì—ˆëŠ”ì§€ ê²€ì¦
- ğŸ“œÂ [Boosting of Thoughts: Trial-and-Error Problem Solving with Large Language Models](https://arxiv.org/abs/2402.11140)
    - Tree of Thoughtsë¥¼ ë°˜ë³µì ìœ¼ë¡œ trial-and-error ê³¼ì •ì— í¬í•¨ì‹œì¼œ ìµœì¢… ê²°ê³¼ë¥¼ ë„ì¶œí•´ë‚´ëŠ” ë°©ì‹
- ğŸ—ï¸Â [SoftBankâ€™s Masayoshi Son is reportedly seeking $100B to build a new AI chip venture](https://techcrunch.com/2024/02/19/softbanks-masayoshi-son-is-reportedly-seeking-100b-to-build-a-new-ai-chip-venture/)
    - ì†Œí”„íŠ¸ë±…í¬ ì†ì •ì˜ íšŒì¥ì´ ìƒˆë¡œìš´ AI ì¹© ê°œë°œì„ ìœ„í•´ 133ì¡° ê·œëª¨ì˜ ìê¸ˆì„ ëª¨ì§‘
- ğŸ“œÂ [The FinBen: An Holistic Financial Benchmark for Large Language Models](https://arxiv.org/abs/2402.12659)
    - ê¸ˆìœµ ë„ë©”ì¸ ì˜¤í”ˆ ì†ŒìŠ¤ ë²¤ì¹˜ë§ˆí¬
- ğŸ§‘ğŸ»â€ğŸ’»Â [cosmopedia](https://huggingface.co/datasets/HuggingFaceTB/cosmopedia)
    - Mistral-8x7B-Instruct-v0.1ì— ì˜í•´ ìƒì„±ëœ textbooks, blogposts, stories, post, WikiHow articles í•©ì„± ë°ì´í„°ì…‹. 30M files, 25B tokens
- ğŸ§‘ğŸ»â€ğŸ’»Â [Andrej Karphathy] [Letâ€™s build the GPT Tokenizer](https://www.youtube.com/watch?v=zduSFxRajkE)
    - ìµœê·¼ ê³µê°œí•œ GPT Tokenizerì™€ ê´€ë ¨í•´ì„œ ì¹´íŒŒì‹œê°€ ì§ì ‘ ì´¬ì˜í•œ 2ì‹œê°„ ë¶„ëŸ‰ì˜ ê°•ì˜ ì˜ìƒ
- ğŸ“œÂ [Microsoft] [Synthetic Data (Almost) from Scratch: Generalized Instruction Tuning for Language Models](https://arxiv.org/abs/2402.13064)
    - human knowledgeì™€ capabilityì— ê´€í•œ taxonomyë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ê³  ì´ë¥¼ decomposition â†’ recombineí•˜ì—¬ ë‹¤ì•™í–” instruction dataë¥¼ ìƒì„±
- ğŸ§‘ğŸ»â€ğŸ’» [Google DeepMind] [Gemma: Introducing new state-of-the-art open models](https://blog.google/technology/developers/gemma-open-models/)
    - 6T í† í°ìœ¼ë¡œ í•™ìŠµí•˜ì—¬ ì˜¤í”ˆ ì†ŒìŠ¤ë¡œ ê³µê°œí•œ 2B, 7B ëª¨ë¸. instruction versionë„ ìˆìŒ.
- ğŸ§‘ğŸ»â€ğŸ’» [Kaggle] [Google â€“ AI Assistants for Data Tasks with Gemma](https://www.kaggle.com/competitions/data-assistants-with-gemma/)
    - data science concepts, Python programming, Kaggle solution ë“±ì— ëŒ€í•´ ë‹µë³€í•  ìˆ˜ ìˆëŠ” Gemma ë…¸íŠ¸ë¶ì„ ë§Œë“œëŠ” ê²ƒì´ goal
- ğŸ“œÂ [ARL2: Aligning Retrievers for Black-box Large Language Models via Self-guided Adaptive Relevance Labeling](https://arxiv.org/abs/2402.13542)
    - (1) LLM ìŠ¤ìŠ¤ë¡œ diverse & high-quality training datasetì„ êµ¬ì¶• â†’ (2) relevance supervisionì„ ë°”íƒ•ìœ¼ë¡œ retrieverë¥¼ í•™ìŠµ â†’ (3) augmented evidenceë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±
- ğŸ“œÂ [Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning](https://arxiv.org/abs/2402.13950)
    - small-sized LMì´ ì˜¬ë°”ë¥¸ reasoning stepì„ ìƒì„±í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” í”„ë ˆì„ì›Œí¬ FRODOë¥¼ ì œì•ˆ. ì´ëŠ” inference moduleê³¼ reasoning moduleë¡œ êµ¬ì„±ë¨
- ğŸ§‘ğŸ»â€ğŸ’»Â [Aria Everyday Activities Dataset](https://huggingface.co/papers/2402.13349)
    - 143ì¼ ê°„ì˜ í™œë™ì„ ë‹´ì€ 3D ì˜¤í”ˆì†ŒìŠ¤ ë°ì´í„°ì…‹
- ğŸ“œÂ [Microsoft Research] [LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens](https://arxiv.org/abs/2402.13753)
    - 256k training lengthë¡œ 1k fine-tuning step ì ìš© ê°€ëŠ¥ â†’ 2048k í† í°ê¹Œì§€ ì»¤ë²„. ë‘ ê°€ì§€ í˜•íƒœì˜ non-uniformities in positional interpolation & second positional interpolation & 8k ê¸¸ì´ì˜ short contextë¥¼ ì»¤ë²„í•  ìˆ˜ ìˆë„ë¡ readjust
- ğŸ“œÂ [Yonsei University] [KMMLU: Measuring Massive Multitask Language Understanding in Korean](https://arxiv.org/abs/2402.11548)
    - 45ê°œì˜ ì£¼ì œë¥¼ ì•„ìš°ë¥´ëŠ” 35,030ê°œì˜ expert-level multiple-choice questions. human performanceëŠ” 62.6%ë¡œ GPT-4, HyperCLOVA XëŠ” ê°ê° 59.95%, 53.40%ì˜ ì„±ëŠ¥ì„ ë³´ì„
- ğŸ“œÂ [OpenCodeInterpreter: Integrating Code Generation with Execution and Refinement](https://arxiv.org/abs/2402.14658)
    - Code-Feedback (iterative refinement) í…Œí¬ë‹‰ ì ìš©, 68K multi-turn interactions ë°ì´í„°ì…‹, GPT-4 ì¸í„°í”„ë¦¬í„°ì™€ ê°™ì€ ëª¨ë¸ì„ ì˜¤í”ˆ ì†ŒìŠ¤ë¡œ ê³µê°œ
- ğŸ—ï¸Â [Adobe Acrobat adds generative AI to â€˜easily chat with documentsâ€™](https://www.theverge.com/2024/2/20/24077217/adobe-acrobat-generative-ai-assistant-chatbot-pdf-document)
    - AI Assistant in Acrobat (conversational engine)
- ğŸ“œÂ [Hint-before-Solving Prompting: Guiding LLMs to Effectively Utilize Encoded Knowledge](https://arxiv.org/abs/2402.14310)
    - Reasoning tasksì—ì„œ ë¬¸ì œë¥¼ í’€ê¸° ì „ì— hintë¥¼ ì œê³µí•˜ëŠ” prompting ë°©ì‹ìœ¼ë¡œ ë” ì¢‹ì€ í¼í¬ë¨¼ìŠ¤ë¥¼ ì´ëŒì–´ëƒ„
- ğŸ“œÂ [CriticBench: Benchmarking LLMs for Critique-Correct Reasoning](https://arxiv.org/abs/2402.14809)
    - LLMì˜ critique and rectify their reasoning ëŠ¥ë ¥ì„ í‰ê°€í•  ìˆ˜ ìˆëŠ” 15ê°œì˜ ë°ì´í„°ì…‹ìœ¼ë¡œ êµ¬ì„±
- ğŸ“œÂ [YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information](https://arxiv.org/abs/2402.13616)
- ğŸ§‘ğŸ»â€ğŸ’» [Stability.ai] [Stable Diffusion 3](https://stability.ai/news/stable-diffusion-3?utm_source=www.theaivalley.com)
</details>

<details>
  <summary>5th week</summary>
  
- ğŸ“œÂ [UC Berkely] [LoRA+: Efficient Low Rank Adaptation of Large Models](https://arxiv.org/abs/2402.12354)
    - ê¸°ì¡´ LoRAê°€ suboptimalí•˜ë‹¤ëŠ” ë¬¸ì œì ì„ ì§€ì í•˜ë©° ì„±ëŠ¥ì„ 1~2% ê°œì„ í•¨ê³¼ ë™ì‹œì— ì†ë„ëŠ” ìµœëŒ€ 2ë°°ê¹Œì§€ í–¥ìƒì‹œí‚¨ adaptation ê¸°ë²•ì„ ì œì‹œ
    - ê¸°ì¡´ì˜ LoRAì—ì„œ ì‚¬ìš©í•˜ëŠ” adapater í–‰ë ¬ Aì™€ BëŠ” ê³ ì •ëœ learning rateë¡œ ì—…ë°ì´íŠ¸ëœë‹¤ëŠ” ì ì´ ë¬¸ì œì„ â†’ ë‘ í–‰ë ¬ì˜ learning rateë¥¼ ì¡°ì ˆí•¨ìœ¼ë¡œì¨ í¼í¬ë¨¼ìŠ¤ì™€ í•™ìŠµ ì†ë„ë¥¼ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆëŠ” ì•Œê³ ë¦¬ì¦˜ LoRA+ ë¥¼ ì œì‹œ
- ğŸ“œÂ [OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems](https://arxiv.org/abs/2402.14008)
    - ì˜¬ë¦¼í”¼ì•„ë“œ ìˆ˜ì¤€ì˜ ê³¼í•™ ë¬¸ì œë¡œ êµ¬ì„±ëœ ë²¤ì¹˜ë§ˆí¬. 8,952ê°œì˜ ìˆ˜í•™ ë° ë¬¼ë¦¬ ë¬¸ì œë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©° ì „ë¬¸ê°€ ìˆ˜ì¤€ì˜ step-by-step reasoning annotationì„ í¬í•¨
- ğŸ“œÂ [Large Language Models for Data Annotation: A Survey](https://arxiv.org/abs/2402.13446)
    - LLMì„ annotationì— í™œìš©í•œ í•™ìŠµ ê¸°ë²•ì´ë‚˜ ë°©ë²•ë¡ ì— ëŒ€í•œ ì„œë² ì´ í˜ì´í¼
- ğŸ“œÂ [Purifying Large Language Models by Ensembling a Small Language Model](https://arxiv.org/abs/2402.14845)
    - ì–¸ì–´ ëª¨ë¸ í•™ìŠµì— ì‚¬ìš©ëœ ë¯¼ê°í•œ ì •ë³´ë“¤ì´ë‚˜ data poisioning ê´€ë ¨ ì´ìŠˆ ë“±ì„ ì²˜ë¦¬í•˜ëŠ” ë°©ë²•ë¡ ìœ¼ë¡œ SLM ensemebleì„ ì œì‹œ
- ğŸ“œÂ [Distillation Contrastive Decoding: Improving LLMs Reasoning with Contrastive Decoding and Distillation](https://arxiv.org/abs/2402.14874)
    - expert & amateur ëª¨ë¸ì„ í•„ìš”ë¡œ í•˜ëŠ” Contrastive Decoding ë°©ì‹ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ dropoutê³¼ quantizationì„ ì ìš©
- ğŸ“œÂ [tinyBenchmarks: evaluating LLMs with fewer examples](https://arxiv.org/abs/2402.14992)
    - í˜„ì¡´í•˜ëŠ” ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ì€ ì§€ë‚˜ì¹˜ê²Œ ë§ì€ ì¼€ì´ìŠ¤ë¥¼ í¬í•¨í•˜ê³  ìˆë‹¤. ì´ì™€ ë™ì¼í•œ ìˆ˜ì¤€ì˜ í‰ê°€ê°€ ê°€ëŠ¥í•œ ì†Œìˆ˜ì˜ examplesë¥¼ curate.
- ğŸ§‘ğŸ»â€ğŸ’»Â [Google DeepMind] ğŸ§ [Genie: Generative Interactive Environments](https://sites.google.com/view/genie-2024)
    - single image promptë¡œ ê²Œì„ ë§Œë“¤ê¸°..
- ğŸ§‘ğŸ»â€ğŸ’»Â [Mistral AI] [Le Chat Mistral](https://chat.mistral.ai/chat)
    - Mistralì—ì„œ ì œê³µí•˜ëŠ” ì±—ë´‡ ì„œë¹„ìŠ¤
- ğŸ§‘ğŸ»â€ğŸ’»Â [Mitral AI] [Au Large](https://mistral.ai/news/mistral-large/)
    - Mistralì—ì„œ ì¶œì‹œí•œ ìƒˆë¡œìš´ í”Œë˜ê·¸ì‹­ ëª¨ë¸. GPT-4ì˜ ë’¤ë¥¼ ì‡ëŠ” ìˆ˜ì¤€ì˜ ì„±ëŠ¥ì´ë©° APIë¥¼ í†µí•´ ì´ìš© ê°€ëŠ¥ (Le Plateforme, Azure, Self-deployment)
- ğŸ“œÂ [Microsoft Research] ğŸ³ [Orca-Math: Unlocking the potential of SLMs in Grade School Math](https://arxiv.org/abs/2402.14830)
    - Mistral-7B ëª¨ë¸ì„ ë² ì´ìŠ¤ë¡œ í•™ìŠµí•œ 7B ëª¨ë¸ Orca-Math. 200K ê°œì˜ ê³ í’ˆì§ˆ í•©ì„± ë°ì´í„°, feedbackì„ í†µí•©ì‹œí‚¤ëŠ” í•™ìŠµ ë°©ì‹ ë“±ì´ í™œìš©ë¨. Llama-2-70B, ChatGPT-3.5 ë“±ì„ ëŠ¥ê°€í•˜ëŠ” í¼í¬ë¨¼ìŠ¤
- ğŸ§‘ğŸ»â€ğŸ’»Â [Argilla] [OpenHermesPreferences - a dataset of 1M AI preferences for RLAIF and DPO](https://huggingface.co/datasets/argilla/OpenHermesPreferences)
    - Mixtral-8x7B-Instruct-v0.1, Nous-Hermes-2-Yi-34B, PairRM ë“±ìœ¼ë¡œë¶€í„° íšë“í•œ 1M ê°œì˜ AI preferences ë°ì´í„°ì…‹. DPO or RLAIF ì— í™œìš© ê°€ëŠ¥
- ğŸ“œÂ [LLMs with Chain-of-Thought Are Non-Causal Reasoners](https://arxiv.org/abs/2402.16048)
    - CoTëŠ” ì˜¬ë°”ë¥´ì§€ë§Œ ì •ë‹µì„ ë„ì¶œí•˜ì§€ ëª»í•œ ì¼€ì´ìŠ¤, ê·¸ë¦¬ê³  ê·¸ ë°˜ëŒ€ì˜ ì¼€ì´ìŠ¤ë“¤ì— ëŒ€í•œ ë¶„ì„
- ğŸ“œÂ [Look Before You Leap: Problem Elaboration Prompting Improves Mathematical Reasoning in Large Language Models](https://arxiv.org/abs/2402.15764)
    - ë³µì¡í•œ ì¶”ë¡  íƒœìŠ¤í¬ì— ëŒ€í•´ì„œ problem contextë¥¼ ë¶„í•´ ë° ì„¤ëª…í•¨ìœ¼ë¡œì¨ ë¬¸ì œ í•´ê²° ëŠ¥ë ¥ì„ í–¥ìƒ ì‹œí‚´ (Problem Elaboration Prompting, PEP)
- ğŸ—ï¸Â [Apple cancels work on electric car, shifts team to generative AI](https://economictimes.indiatimes.com/tech/technology/apple-cancels-work-on-electric-car-shifts-team-to-generative-ai/articleshow/108052606.cms)
    - ì• í”Œì´ ë”ì´ìƒ ì „ê¸°ì°¨ë¥¼ ë§Œë“¤ì§€ ì•Šê³  ìƒì„±í˜• AI ê°œë°œì— ì§‘ì¤‘í•œë‹¤ëŠ” ì†Œì‹
- ğŸ“œÂ [Reasoning in Conversation: Solving Subjective Tasks through Dialogue Simulation for Large Language Models](https://arxiv.org/abs/2402.17226)
    - LLMì´ ì£¼ê´€ì ì¸ íƒœìŠ¤í¬ë¥¼ ì²˜ë¦¬í•  ë•ŒëŠ” ê°ê´€ì ì¸ íƒœìŠ¤í¬ë¥¼ ì²˜ë¦¬í•  ë•Œì— ë¹„í•´ ì—´ë“±í•œ ì„±ëŠ¥ì„ ë³´ì„. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ë°©ë²•ìœ¼ë¡œ CoTì™€ ê°™ì€ rationale ì œì‹œ ë°©ì‹ ëŒ€ì‹  dialogueë¥¼ ë„ì….
- ğŸ§‘ğŸ»â€ğŸ’»Â [DeepLearning.AI] [Prompt Engineering with Llama 2](https://www.deeplearning.ai/short-courses/prompt-engineering-with-llama-2/)
    - Metaì˜ Llama 2ë¥¼ í™œìš©í•˜ì—¬ few-shot promptingê³¼ ê°™ì€ prompt engineeringì— ëŒ€í•´ í•™ìŠµ
</details>


## ğŸŒ± March
<details>
  <summary>1st ~ 2nd week</summary>
  
- ğŸ§‘ğŸ»â€ğŸ’»Â OpenAI APIâ€™s change on log probabilities from 5 to 20 return
- ğŸ—ï¸Â [Robotics startup Figure raises $675 mln from Microsoft, Nvidia, OpenAI](https://www.reuters.com/technology/robotics-startup-figure-raises-675-mln-microsoft-nvidia-other-big-techs-2024-02-29/)
    - IT ê³µë£¡ ê¸°ì—…ë“¤ì´ ë¡œë´‡ ë¶„ì•¼ì—ë„ ì ê·¹ì ìœ¼ë¡œ íˆ¬ìí•˜ê³  ìˆë‹¤ëŠ” ì†Œì‹
- ğŸ“œÂ [IIT] [How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning](https://arxiv.org/abs/2402.18312)
    - CoTì— ëŒ€í•´ layerë³„ë¡œ ë¶„ì„. token representationì„ í™•ì¸í•œ ê²°ê³¼ ì¤‘ê°„ ì´ì „ì˜ layerì—ì„œëŠ” ì‚¬ì „ í•™ìŠµë°ì´í„°ì— ëŒ€í•´ í¸í–¥ë˜ì–´ ìˆìœ¼ë‚˜ ì¤‘ê°„ ì´í›„ë¶€í„°ëŠ” ê¸‰ê²©íˆ in-contextì— ì§‘ì¤‘
- ğŸ“œÂ [Rice University] [Learning to Compress Prompt in Natural Language Formats](https://arxiv.org/abs/2402.18700)
    - APIì— ëŒ€í•´ì„œëŠ” soft prompt compressionì„ ì ìš©í•  ìˆ˜ ì—†ê¸° ë•Œë¬¸ì— ìì—°ì–´ í˜•íƒœë¡œ compressioní•˜ëŠ” ë°©ë²•ì„ ì œì‹œ. ì—¬ê¸°ì— ì‚¬ìš©ë˜ëŠ” ê²ƒì´ Natrual Language Prompt Encapsulation (Nano-Capsulator) framework.
- ğŸ“œÂ [Microsoft] [ResLoRA: Identity Residual Mapping in Low-Rank Adaption](https://arxiv.org/abs/2402.18039)
    - original modelì˜ long calculation pathë¥¼ ë™ì¼í•˜ê²Œ ê±°ì³ì•¼ í•˜ëŠ” LoRAì˜ í•œê³„ë¥¼ ë³´ì™„í•˜ê¸° ìœ„í•´ í•™ìŠµ ë™ì•ˆì— residual pathë¥¼ ë”í•˜ê³ , ì¶”ë¡  ë™ì•ˆì—ëŠ” ì´ëŸ¬í•œ extra pathë¥¼ ì œê±°í•˜ê¸° ìœ„í•œ merging approachë¥¼ ì‚¬ìš© â†’ LoRAì™€ ëŒ€ë¹„ í•™ìŠµ ë° ì¶”ë¡  costëŠ” ë” ë‚®ìœ¼ë©´ì„œë„ performanceëŠ” ë” ì¢‹ìŒ
- ğŸ“œÂ [Datasets for Large Language Models: A Comprehensive Survey](https://arxiv.org/abs/2402.18041)
    - 8ê°œ ì–¸ì–´, 32ê°œ ë„ë©”ì¸, 444ê°œ ë°ì´í„°ì…‹ì— ëŒ€í•œ ì„œë² ì´ ë…¼ë¬¸. ì´ 774.5TBì— ë‹¬í•˜ëŠ” ì‚¬ì „í•™ìŠµ corporaë¥¼ ë¶„ë¥˜
- ğŸ“œÂ [Apple] [LUCID: LLM-Generated Utterances for Complex and Interesting Dialogues](https://arxiv.org/abs/2403.00462)
    - 4,277ê°œì— ë‹¬í•˜ëŠ” multi-domain, multi-intent conversationë¥¼ ìƒì„±í•˜ê¸° ìœ„í•´ LUCIDë¥¼ ì‚¬ìš© (LLM-generated Utterances for Complex and Interesting Dialogues)
- ğŸ“œÂ [An Empirical Categorization of Prompting Techniques for Large Language Models: A Practitioner's Guide](https://arxiv.org/abs/2402.14837)
    - 7ê°œì˜ ì¹´í…Œê³ ë¦¬ë¡œ êµ¬ë¶„í•˜ì—¬ academicí•˜ë©´ì„œë„ pragmaticí•œ ë‚´ìš©ì˜ prompting í…Œí¬ë‹‰ì„ ì •ë¦¬í•œ ì„œë² ì´ í˜ì´í¼
- ğŸ“œÂ [Meta] [Learning and Leveraging World Models in Visual Representation Learning](https://arxiv.org/abs/2403.00504)
    - Joint-Embedding Predictive Architecture (JEPA)ì— conditioning, prediction difficulty, capacity ê°œë…ì„ ë”í•œ Image Word Modelsë¥¼ ì œì‹œ. ì–€ ë¥´ì¿¤ì´ ì—°êµ¬ì— ì°¸ì—¬
- ğŸ§‘ğŸ»â€ğŸ’»Â [Anthropic] [Introducing the next generation of Claude](https://www.anthropic.com/news/claude-3-family)
    - Haiku, Sonnet, Opusë¡œ êµ¬ì„±ëœ Claude 3 familyë¥¼ ê³µê°œ. 159ê°œ êµ­ê°€ì—ì„œ API ì´ìš© ê°€ëŠ¥. (ìì‹ ë“¤ì˜ ì£¼ì¥ìœ¼ë¡œëŠ”) ì—¬ëŸ¬ ë²¤ì¹˜ë§ˆí¬ì—ì„œ GPT-4ë¥¼ ëŠ¥ê°€í•˜ëŠ” ì„±ëŠ¥. Vision ê´€ë ¨ ëŠ¥ë ¥ë„ ë›°ì–´ë‚œ í¸. ë¶ˆí•„ìš”í•œ ê±°ì ˆ ë©”ì„¸ì§€ ë°˜í™˜ìœ¨ë„ í¬ê²Œ ë–¨ì–´ì§ (ì´ì „ ë²„ì „ì—ì„œì˜ ì´ìŠˆ). 200Kì˜ window sizeë¡œ ì¶œì‹œë˜ì—ˆìœ¼ë‚˜ íŠ¹ì • ê³ ê°ë“¤ì— í•œí•´ 1M í† í°ë„ ì²˜ë¦¬ ê°€ëŠ¥í•˜ê²Œë” í•  ìˆ˜ ìˆìŒì„ ì–¸ê¸‰.
- ğŸ“œÂ [Distilling Text Style Transfer With Self-Explanation From LLMs](https://arxiv.org/abs/2403.01106)
    - test style transfer ë¶„ì•¼ì—ì„œ ë¶€ì¡±í•œ parallel ë°ì´í„°ì…‹ì„ êµ¬ì¶•. ì—¬ê¸°ì— LLM distillationì„ í™œìš©
- ğŸ“œÂ [Stanford, Georgia Tech, Microsoft, Google DeepMind] [Design2Code: How Far Are We From Automating Front-End Engineering?](https://arxiv.org/abs/2403.03163)
    - ì‹¤ì œ 484ê°œì˜ ì›¹í˜ì´ì§€ë¥¼ í…ŒìŠ¤í¬ ì¼€ì´ìŠ¤ë¡œ ë‘ê³  Design2Code taskë¥¼ í‰ê°€í•˜ëŠ” ë²¤ì¹˜ë§ˆí¬ë¥¼ êµ¬ì¶•. Gemini Pro Visionì— ë²„ê¸ˆê°€ëŠ” Design2Code-18B ëª¨ë¸ì„ fine-tuning
- ğŸ“œÂ [PHAnToM: Personality Has An Effect on Theory-of-Mind Reasoning in Large Language Models](https://arxiv.org/abs/2403.02246)
    - Theory of Mind (ToM) Reasoningì„ ì´ëŒì–´ë‚´ê¸° ìœ„í•´ í•„ìš”í•œ personalityê°€ ì–´ë–¤ ê²ƒì¸ì§€ì— ëŒ€í•œ ì—°êµ¬. íŠ¹ì • personalityê°€ ToM ê´€ë ¨ íƒœìŠ¤í¬ì˜ ì„±ëŠ¥ì„ ë†’ì´ëŠ” ë° ë„ì›€ì´ ë˜ëŠ” ê²ƒì„ í™•ì¸.
- ğŸ§‘ğŸ»â€ğŸ’» [2024 ì˜¤í”ˆì†ŒìŠ¤ ì»¨íŠ¸ë¦¬ë·°ì…˜ ì•„ì¹´ë°ë¯¸ [ì²´í—˜í˜•] ë©˜í‹° ëª¨ì§‘](https://www.contribution.ac/)
    - â€˜Git í™œìš© ë° Gemmaë¥¼ ì´ìš©í•œ LLM ì•± ê°œë°œâ€™
- ğŸ§‘ğŸ»â€ğŸ’»Â [Elon Musk and OpenAIâ€™s fiery battle](https://openai.com/blog/openai-elon-musk)
    - OpenAIâ€™s blog posting about Elon Muskâ€™s accusation
- ğŸ§‘ğŸ»â€ğŸ’»Â [Claude 3â€™s system prompt](https://twitter.com/AmandaAskell/status/1765207842993434880?) (X link)
- ğŸ“œÂ [Benchmarking Hallucination in Large Language Models based on Unanswerable Math Word Problem](https://arxiv.org/abs/2403.03558)
    - ê¸°ì¡´ Math Word Problem ë°ì´í„°ì…‹ì„ ê¸°ë°˜ìœ¼ë¡œ unanswerable problemsë¥¼ í¬í•¨í•˜ëŠ” ìƒˆë¡œìš´ ë²¤ì¹˜ë§ˆí¬ë¥¼ êµ¬ì¶•. ëŒ€ë‹µ ê°€ëŠ¥í•œ ë¬¸ì œì™€ ê·¸ë ‡ì§€ ì•Šì€ ë¬¸ì œ ê° 2,600ê°œì”© êµ¬ì„±. InstructGPT, Claude, LLaMA ì‹œë¦¬ì¦ˆë¡œ ê²€ì¦.
- ğŸ“œÂ [ShortGPT: Layers in Large Language Models are More Redundant Than You Expect](https://arxiv.org/abs/2403.03853)
    - LLMì˜ íŠ¹ì • layerë“¤ì´ ë†’ì€ ìœ ì‚¬ë„ë¥¼ ê°€ì§„ë‹¤ëŠ” ê²ƒì€ ë¶ˆí•„ìš”í•œ layerê°€ í¬í•¨ë˜ì–´ ìˆë‹¤ëŠ” ëœ» â†’ Block Influence (BI)ë¼ëŠ” metricì„ ì •ì˜í•˜ì—¬ ê° layerì˜ ì¤‘ìš”ë„ë¥¼ ì¸¡ì • â†’ pruningì—ì„œ SoTAë¥¼ ë‹¬ì„±í•œ ShortGPTë¥¼ ê°œë°œ
- ğŸ“œÂ [GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection](https://arxiv.org/abs/2403.03507)
    - full parameter learningì„ ì‚¬ìš©í•˜ì§€ë§Œ LoRAë³´ë‹¤ë„ memory-efficientí•œ í•™ìŠµ ì „ëµì¸ Graident Low-Rank Projection (GaLore)ë¥¼ ì œì‹œ. 7B ëª¨ë¸ì„ 24GB ë©”ëª¨ë¦¬ GPU í•œ ëŒ€ë¡œ ë³‘ë ¬ ì²˜ë¦¬ ì—†ì´ pre-training ê°€ëŠ¥í•˜ë„ë¡ ë§Œë“œëŠ” í…Œí¬ë‹‰.
- ğŸ“œÂ [SaulLM-7B: A pioneering Large Language Model for Law](https://arxiv.org/abs/2403.03883)
    - Mistral 7B ëª¨ë¸ì„ ë² ì´ìŠ¤ë¡œ ë²•ë¥  ë°ì´í„°ë¡œ continual pre-training & instruction fine-tuningí•œ ëª¨ë¸ SaulLM-7B ëª¨ë¸ì„ ê³µê°œ. 30B í† í°ì˜ ë²•ë¥  ë°ì´í„°ë¡œ í•™ìŠµí–ˆë‹¤ê³  í•¨.
- ğŸ—ï¸Â [Salesforce announces new AI tools for doctors](https://www.cnbc.com/2024/03/07/salesforce-announces-new-ai-tools-for-doctors.html)
    - ì„¸ì¼ì¦ˆí¬ìŠ¤ì—ì„œ ì˜ë£Œ ë¶„ì•¼ì˜ í–‰ì •ì  ì—…ë¬´ ë¶€ë‹´ì„ ì™„í™”í•´ì¤„ ìˆ˜ ìˆëŠ” Einstein Copilotì„ ì¶œì‹œ
- ğŸ“œÂ [Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference](https://arxiv.org/abs/2403.04132)
    - LLM ì„±ëŠ¥ í‰ê°€ ê²°ê³¼ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë¦¬ë”ë³´ë“œë¡œ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” [ì±—ë´‡ ì•„ë ˆë‚˜](https://chat.lmsys.org/)ì— ëŒ€í•œ ì„¤ëª…ì´ ë‹´ê¸´ ë…¼ë¬¸. ì‚¬ìš©ëœ ë©”íŠ¸ë¦­ì´ë‚˜ ì§€ê¸ˆê¹Œì§€ì˜ í‰ê°€ ê²°ê³¼ì— ëŒ€í•œ ë¶„ì„ì„ í¬í•¨í•˜ê³  ìˆìŒ
- ğŸ“œÂ [Yi: Open Foundation Models by 01.AI](https://arxiv.org/abs/2403.04652)
    - 01.AIì—ì„œ ì¶œì‹œí•œ LLM, Yi. 6B, 34B ì‚¬ì´ì¦ˆì˜ ì‚¬ì „í•™ìŠµ ëª¨ë¸ì´ë©° 200Kì˜ context length, depth-upscaled model, vision-language model ì´ë¼ëŠ” íŠ¹ì§•ì„ ì§€ë‹˜
- ğŸ“œÂ [Meta] [Teaching Large Language Models to Reason with Reinforcement Learning](https://arxiv.org/abs/2403.04642)
    - feedbackìœ¼ë¡œë¶€í„° ë°°ìš°ëŠ” ì—¬ëŸ¬ ì•Œê³ ë¦¬ì¦˜ (Expert Iteration, Proximal Policy Optimization, Return-Conditioned RL)ì— ëŒ€í•œ ë¹„êµ ì—°êµ¬
- ğŸ§‘ğŸ»â€ğŸ’»Â ğŸ¦ [WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild](https://huggingface.co/spaces/allenai/WildBench)
    - ë³´ë‹¤ í˜„ì‹¤ì ì´ê³  ë‚œì´ë„ê°€ ë†’ì€, real-worldì—ì„œ ë‚˜ì˜¬ ë²•í•œ ê²ƒë“¤ë¡œ Benchmarkë¥¼ êµ¬ì„±. [ê¹ƒí—ˆë¸Œ](https://github.com/allenai/WildBench), [ë¦¬ë”ë³´ë“œ](https://huggingface.co/spaces/allenai/WildBench), [í—ˆê¹…í˜ì´ìŠ¤](https://huggingface.co/datasets/allenai/WildBench)
- ğŸ§‘ğŸ»â€ğŸ’»Â [mamba_peft.py on HuggingFace](https://gist.github.com/ArthurZucker/743dd7962f21b6ab4a21f692c82b9246)
    - mambaë¥¼ ì´ì œ transformersì—ì„œ ì´ìš©í•  ìˆ˜ ìˆìŒ. ìœ„ ë§í¬ëŠ” PEFT example ì½”ë“œ.
- ğŸ§‘ğŸ»â€ğŸ’»Â [Foundation Model Development Cheatsheet](https://fmcheatsheet.org/)
    - ê°ì¢… ëª¨ë¸ ë° ë°ì´í„°ì…‹ì„ ì¹´í…Œê³ ë¦¬ì™€ ëª¨ë‹¬ë¦¬í‹°ë¡œ êµ¬ë¶„í•˜ì—¬ í•œ ë²ˆì— í™•ì¸í•  ìˆ˜ ìˆëŠ” ì‚¬ì´íŠ¸
- ğŸ“œÂ [Learning to Generate Instruction Tuning Datasets for Zero-Shot Task Adaptation](https://arxiv.org/abs/2402.18334)
    - 1.65M ê°œì˜ examplesë¡œ í•™ìŠµëœ ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ for conditional task generation. unannotated textë¥¼ instruction tuningì„ ìœ„í•œ task-specific training datasetsìœ¼ë¡œ ë³€í™˜
</details>

<details>
  <summary>3rd week</summary>
  
- ğŸ§‘ğŸ»â€ğŸ’»Â [Gen AI Korea 2024] [ìƒì„±í˜• AI ë ˆë“œíŒ€ ì±Œë¦°ì§€](https://www.aiignite.org/)
    - 4ì›” 11ì¼ (ëª©) ~ 4ì›” 12ì¼ (ê¸ˆ), ì½”ì—‘ìŠ¤ì—ì„œ ì§„í–‰ë˜ëŠ” ì±Œë¦°ì§€ ë° ì»¨í¼ëŸ°ìŠ¤. Cohere ëŒ€í‘œ, Kakao ì´ì‚¬, ë„¤ì´ë²„ AI ìˆ˜ì¥ ë“± ìœ ëª… ì¸ì‚¬ë“¤ì´ ì°¸ì—¬
- ğŸ“œÂ [Anthropic] [The Claude 3 Model Family: Opus, Sonnet, Haiku](https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf)
    - Anthropicì—ì„œ ìµœê·¼ ì¶œì‹œí•œ Claude 3 ëª¨ë¸ íŒ¨ë°€ë¦¬ì— ëŒ€í•œ model card. ì£¼ë¡œ ë²¤ì¹˜ë§ˆí¬ ì„±ëŠ¥ í‰ê°€ ê²°ê³¼ê°€ ì œì‹œë˜ì–´ ìˆëŠ” ë“¯í•¨
- ğŸ“œÂ [Microsoft] [Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models](https://arxiv.org/abs/2402.17177v2)
    - OpenAIì—ì„œ ì¶œì‹œí•œ text-to-video ìƒì„± AI ëª¨ë¸, Soraì— ëŒ€í•œ comprehensive review paper
- ğŸ“œÂ [Google Research] [Beyond Sparse Rewards: Enhancing Reinforcement Learning with Language Model Critique in Text Generation](https://arxiv.org/abs/2401.07382)
    - ê¸°ì¡´ì—ëŠ” ì „ì²´ outputì— ëŒ€í•œ single rewardë¥¼ ë°˜í™˜í–ˆê¸° ë•Œë¬¸ì— reward signal ìì²´ê°€ spareí•˜ë‹¤ëŠ” ë¬¸ì œê°€ ìˆì—ˆìŒ â†’ LLMì˜ ë¹„íŒ(critique) ëŠ¥ë ¥ì„ í™œìš©í•˜ì—¬ RL í•™ìŠµ ê³¼ì •ì—ì„œ ì‚¬ìš©ë  ìˆ˜ ìˆëŠ” intermediate-step rewardsë¥¼ ìƒì„±
- ğŸ“œÂ [Birbal: An efficient 7B instruct-model fine-tuned with curated datasets](https://arxiv.org/abs/2403.02247)
    - NeurIPS workshopìœ¼ë¡œ ì§„í–‰ëœ LLM Efficiency Challenge. RTX 4090 ë˜ëŠ” A00 with 40GB í•œ ëŒ€ë¡œ 24ì‹œê°„ ë‚´ì— í•™ìŠµí•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•¨. ë³¸ ëª¨ë¸ì€ Mistral-7Bë¥¼ ë² ì´ìŠ¤ë¡œ ì‚¼ê³  ìˆìœ¼ë©° RTX 4090ìœ¼ë¡œ 16ì‹œê°„ ë™ì•ˆ í•™ìŠµí•¨. ì´ëŠ” ë‹¤ì–‘í•œ íƒœìŠ¤í¬ë¥¼ ì•„ìš°ë¥´ëŠ” ê³ í’ˆì§ˆ instruction datasetì—ì„œ ê¸°ì¸í•¨
- ğŸ“œÂ [Google DeepMind] [Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context](https://arxiv.org/abs/2403.05530)
    - contextì˜ ê¸¸ì´ê°€ ê¸´ ìƒí™©ì—ì„œ, Gemini 1.5 ëª¨ë¸ íŒ¨ë°€ë¦¬ê°€ ì–´ë–¤ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ëŠ”ì§€ ë¹„êµ ë¶„ì„í•œ êµ¬ê¸€ì˜ technical report. MMLUì—ì„œ ì‚¬ëŒì˜ ìµœê³  ì ìˆ˜ë¥¼ ë„˜ì€ ìµœì´ˆì˜ ëª¨ë¸ì´ë¼ê³  ì£¼ì¥í•˜ì§€ë§Œ ëŒ€ì¤‘ì˜ í‰ê°€ëŠ” ìƒì´í•¨.
- ğŸ“œÂ [MuseGraph: Graph-oriented Instruction Tuning of Large Language Models for Generic Graph Mining](https://arxiv.org/abs/2403.04780)
    - task-specific Chain-of-Thought-based insturction generation mechanism
- ğŸ“œÂ [Harnessing Multi-Role Capabilities of Large Language Models for Open-Domain Question Answering](https://arxiv.org/abs/2403.05217)
    - ODQA íƒœìŠ¤í¬ì—ì„œ â€˜retrieve-then-readâ€™ì™€ â€˜generate-then-readâ€™ íŒ¨ëŸ¬ë‹¤ì„ì„ í•©ì¹œ ë°©ì‹. query expansion, document selection, answer generationì˜ ì„¸ ê°€ì§€ ìŠ¤í…ìœ¼ë¡œ êµ¬ì„±ë¨.
- ğŸ§‘ğŸ»â€ğŸ’»Â [Cohere] [Command-R: Retrieval Augmented Generation at Production Scale](https://txt.cohere.com/command-r/)
    - long contextë¥¼ í™œìš©í•˜ëŠ” RAGë‚˜ ì™¸ë¶€ API, ë˜ëŠ” tool ì‚¬ìš©ì— ì í•©í•œ ìƒì„±í˜• ëª¨ë¸ Command-Rì„ ê³µê°œ. Embed & Rerank ëª¨ë¸ê³¼ í•¨ê»˜ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë¨. Cohere APIë¥¼ í†µí•´ ì´ìš© ê°€ëŠ¥.
- ğŸ“œÂ [MIT] [RA-ISF: Learning to Answer and Understand from Retrieval Augmentation via Iterative Self-Feedback](https://arxiv.org/abs/2403.06840)
    - queryì™€ ë¬´ê´€í•œ ë¬¸ì„œê°€ retrieve ë˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´ Iterative Self-Feedback ë°©ì‹ì„ ì œì•ˆ
- ğŸ§‘ğŸ»â€ğŸ’»Â [OpenAI] [transfromer-debugger (TBD)](https://github.com/openai/transformer-debugger)
    - Small Language Modelsì˜ íŠ¹ì • í–‰ë™ì„ ì¡°ì‚¬í•˜ê¸° ìœ„í•œ ëª©ì ìœ¼ë¡œ ì œì‘ëœ ë””ë²„ê¹… íˆ´ (ê¹ƒí—ˆë¸Œ ë ˆí¬ ë§í¬)
- ğŸ“œÂ [Google DeepMind, OpenAI] [Stealing Part of a Production Language Model](https://arxiv.org/abs/2403.06634)
    - proprietary ëª¨ë¸ì˜ embedding projector layerë¥¼ hackingìœ¼ë¡œ ì–»ì„ ìˆ˜ ìˆë‹¤ëŠ” í™”ì œì˜ ë…¼ë¬¸
- ğŸ“œÂ [Meta] [Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM](https://arxiv.org/abs/2403.07816)
    - seed ëª¨ë¸ë¡œë¶€í„° ê° ë°ì´í„°ì— ë”°ë¼ ë‹¤ë¥¸ expert LLMì„ í•™ìŠµì‹œí‚¤ê³ , routerë¥¼ í†µí•´ ì¶”ê°€ì ì¸ FeedForward layerë¥¼ í•™ìŠµì‹œí‚¤ëŠ” ë°©ì‹ì¸ Branch-Train-Mixë¥¼ ì œì•ˆ. MoE finetuningì´ í•„ìš”í•˜ì§€ ì•Šì€ Branch-Train-Merge ë°©ì‹ì—ë„ ì ìš© ê°€ëŠ¥.
- ğŸ§‘ğŸ»â€ğŸ’»Â [DeepLearning.AI] [Knowledge Graph for RAG](https://learn.deeplearning.ai/courses/knowledge-graphs-rag/lesson/1/introduction)
    - Neo4jì™€ì˜ collaboration. RAG ë‚´ì—ì„œ knowledge graphë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ ë°°ìš°ëŠ” ê³¼ì • (graph store)
- ğŸ§‘ğŸ»â€ğŸ’»Â [Google DeepMind] [A generalist AI agent for 3D virtual environments](https://deepmind.google/discover/blog/sima-generalist-ai-agent-for-3d-virtual-environments/)
    - ë‹¤ì–‘í•œ video-game í™˜ê²½ì—ì„œ natural language instructionì„ ë”°ë¥¼ ìˆ˜ ìˆëŠ” Multiworld Agentë¥¼ ê°œë°œ
- ğŸ§‘ğŸ»â€ğŸ’»Â [Microsoft Research] [Rethinking Generative Large Language Model Evaluation for Semantic Comprehension](https://arxiv.org/abs/2403.07872)
    - ì—¬ëŸ¬ ì„ íƒì§€ ì¤‘ì—ì„œ í•˜ë‚˜ë¥¼ ê³ ë¥´ëŠ” Multiple Choice Question Answering (MCQA) ëŒ€ì‹  24ê°œì˜ ëª¨ë¸ì´ ì°¸ì—¬í•˜ëŠ” RWQ-Elo ranking systemì„ ì œì•ˆ
- ğŸ§‘ğŸ»â€ğŸ’»Â [OpenAI] [Figure Status Update - OpenAI Speech-to-Speech Reasoning](https://www.youtube.com/watch?v=Sq1QZB5baNw)
    - OpenAIì—ì„œ Figureë¼ëŠ” ë¡œë´‡ íšŒì‚¬ì™€ ì œí’ˆì„ ê²°í•©í•˜ì—¬ ì¸ì§€ ë° ì¶”ë¡  ëŠ¥ë ¥ì´ ì•„ì£¼ ë›°ì–´ë‚œ ë¡œë´‡ì„ ê°œë°œ
- ğŸ“œÂ [Tancent] [Large Language Models are Contrastive Reasoners](https://arxiv.org/abs/2403.08211)
    - â€œLetâ€™s give a correct and a wrong answerâ€, promptë¥¼ ì•ì— ë¶™ì—¬ì¤Œ. ì´ë¡œì¨ LLMì´ í›Œë¥­í•œ contrastive reasonerë¼ëŠ” ê²ƒì„ ì…ì¦í•œ ì—°êµ¬.
- ğŸ“œÂ [Logits of API-Protected LLMs Leak Proprietary Information](https://arxiv.org/abs/2403.09539)
    - proprietary ëª¨ë¸ë“¤ì˜ hidden size, full-vocabulary output ë“±ì— ê´€í•œ ì •ë³´ë¥¼ ì ì€ API ë¹„ìš©ìœ¼ë¡œ hackingí•  ìˆ˜ ìˆë‹¤ëŠ” ë…¼ë¬¸. gpt-3.5-turboì˜ ê²½ìš° $1000 ì´í•˜ê°€ í•„ìš”í•˜ë‹¤ê³  ì£¼ì¥.
- ğŸ“œÂ [Apple] [MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training](https://arxiv.org/abs/2403.09611)
    - Multimodal Large Language Modelsì— ê´€í•œ ì‚¬ì „í•™ìŠµìš© ë°ì´í„° ì„ ì •, í•™ìŠµ ê¸°ë²•, ì´ë¯¸ì§€ ì¸ì½”ë” ë“±ì— ëŒ€í•œ ì—°êµ¬. dense ëª¨ë¸ê³¼ mixture-of-experts (MoE) ë°©ì‹ì„ ê²°í•©í•œ MM1 ëª¨ë¸ íŒ¨ë°€ë¦¬ë¥¼ ê°œë°œ
- ğŸ—ï¸Â [Ex-Activision CEO Bobby Kotick pitched buying TikTok to potential partners, including Sam Altman: report](https://www.businessinsider.in/tech/news/ex-activision-ceo-bobby-kotick-pitched-buying-tiktok-to-potential-partners-including-sam-altman-report/articleshow/108409188.cms)
    - ë¯¸êµ­ì—ì„œëŠ” í‹±í†¡ì„ ê·œì œí•˜ëŠ” ì™€ì¤‘ì— Activisionì˜ ì „ CEOê°€ í‹±í†¡ì„ ì¸ìˆ˜í•˜ê³  OpenAIì™€ í˜‘ë ¥í•  ê³„íšì„ ê°–ê³  ìˆìŒì— ê´€í•œ ë³´ë„
- ğŸ§‘ğŸ»â€ğŸ’»Â [xAI] [Open ReleaseÂ of Grok-1](https://x.ai/blog/grok-os)
    - ì¼ë¡  ë¨¸ìŠ¤í¬ì˜ AI íšŒì‚¬ xAIì—ì„œ LLM Grok-1 (314B)ì„ ì˜¤í”ˆ ì†ŒìŠ¤ë¡œ ê³µê°œ. ì•½ì†ì„ ì§€í‚¤ëŠ” ìƒë‚¨ì.. OpenAIì™€ì˜ ê´€ê³„ì— ê¸°ì¸í•œ í˜„ìƒê°™ê¸°ë„ í•˜ê³ .. ([ê¹ƒí—ˆë¸Œ ë§í¬](https://github.com/xai-org/grok-1))
- ğŸ§‘ğŸ»â€ğŸ’»Â [Cohere] [C4AI Command-R (HuggingFace)](https://huggingface.co/CohereForAI/c4ai-command-r-v01)
    - Cohereì—ì„œ ê³µê°œí•œ RAGì— íŠ¹í™”ëœ LLM. ì§€ë‚œ ë²ˆ APIë¡œ ê³µê°œí•œ ì´í›„ ëª¨ë¸ë„ í—ˆê¹…í˜ì´ìŠ¤ì— ê³µê°œ.
- ğŸ“œÂ [Stanford University] [Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking](https://arxiv.org/abs/2403.09629)
    - ì–¸ì–´ ëª¨ë¸ì´ reasoningì„ ìˆ˜í–‰í•˜ëŠ” ê³¼ì •ì—ì„œ, ë§¤ ìŠ¤í…ë§ˆë‹¤ â€˜thoughtâ€™ë¥¼ ë³‘ë ¬ì ìœ¼ë¡œ ìƒì„±í•˜ì—¬ ë” ì¢‹ì€ ì¶”ë¡ ì´ ê°€ëŠ¥í•˜ë„ë¡ ìœ ë„í•˜ëŠ” ë°©ë²•ë¡ ì„ ì œì•ˆ
- ğŸ“œÂ [Peking University] [RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation](https://arxiv.org/abs/2403.05313)
    - CoT ë¬¸ì¥ì˜ ê° ìš”ì†Œì™€ ê´€ë ¨ëœ contentë¥¼ ì°¾ì•„ì„œ ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•„ìš”í•œ ê²½ìš° revise. revised ë¬¸ì¥ë“¤ë¡œ CoTë¥¼ ì¬êµ¬ì„±
</details>

<details>
  <summary>4th week</summary>
  
- ğŸ—ï¸Â [Nvidia] [Nvidia reveals Blackwell B200 GPU, the â€˜worldâ€™s most powerful chipâ€™ for AI](https://www.theverge.com/2024/3/18/24105157/nvidia-blackwell-gpu-b200-ai)
    - H100ì˜ ë’¤ë¥¼ ìˆëŠ” í”Œë˜ê·¸ì‹­ GPU, B200 ê³µê°œ
- ğŸ§‘ğŸ»â€ğŸ’»Â [Open-Sora](https://github.com/hpcaitech/Open-Sora)
    - OpenAIì˜ Soraì— ì˜ê°ì„ ë°›ì•„ ë§Œë“  ê³ í’ˆì§ˆ video ìƒì„± ëª¨ë¸. ì˜¤í”ˆì†ŒìŠ¤ë¡œ ê³µê°œ.
- ğŸ“œÂ [CMU-LTI] [Enhancing LLM Factual Accuracy with RAG to Counter Hallucinations: A Case Study on Domain-Specific Queries in Private Knowledge-Bases](https://arxiv.org/abs/2403.10446)
    - upstream datasets processingê³¼ downstrea performance evaluationì„ í†µí•©í•œ ì‹œìŠ¤í…œì„ êµ¬ì¶•. ë°ì´í„° í¬ë¡¤ë§ë¶€í„° QA ì‹œìŠ¤í…œ ì „ë°˜ì— ëŒ€í•œ ë‚´ìš©ì„ ë‹¤ë£¨ê³  ìˆìŒ
- ğŸ“œÂ [UC Berkeley] [RAFT: Adapting Language Model to Domain Specific RAG](https://arxiv.org/abs/2403.10131)
    - Test ë‹¨ê³„ì—ì„œ ëª¨ë¸ì´ ì™¸ë¶€ ë¬¸ì„œë¥¼ í™œìš©í•˜ëŠ” ë°©ì‹ì— ëŒ€í•´ í•™ìŠµí•˜ë„ë¡ í•¨. ì´ë•Œ golden only ë°©ì‹ì´ ì•„ë‹Œ sampled negative documentsë„ í™œìš©.
- ğŸ“œÂ [Google Research] [PERL: Parameter Efficient Reinforcement Learning from Human Feedback](https://arxiv.org/abs/2403.10704)
    - RLHFì— LoRAë¥¼ í™œìš©í•˜ëŠ” ë°©ë²•ë¡ ì„ ì œì•ˆ. ì •í™•íˆëŠ” reward model í•™ìŠµì— LoRAê°€ í™œìš©ë¨
- ğŸ“œÂ [EACL 2024] [Aligning Large and Small Language Models via Chain-of-Thought Reasoning](https://aclanthology.org/2024.eacl-long.109/)
    - SLMì´ íŠ¹ì • ì–‘ì‹ì„ ì˜ ë”°ë¥¼ ìˆ˜ ìˆë„ë¡ Instruction-tuning-CoT Methodë¥¼ ì œì•ˆ
- ğŸ“œÂ [RankPrompt: Step-by-Step Comparisons Make Language Models Better Reasoners](https://arxiv.org/abs/2403.12373)
    - LLMì´ reasoning ê³¼ì • ì¤‘ì— ë§Œë“œëŠ” ì‹¤ìˆ˜ë¥¼ ì¤„ì´ê¸° ìœ„í•œ ë°©ì‹ìœ¼ë¡œ LLMì´ ìŠ¤ìŠ¤ë¡œ ìì‹ ì˜ responseì— ëŒ€í•´ ranking í•˜ëŠ” ë°©ì‹ì„ ì œì•ˆ. ì¶”ê°€ì ì¸ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ì´ ë°œìƒí•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ì ì´ íŠ¹ì§•.
- ğŸ“œÂ [KAIST] [SuRe: Summarizing Retrievals using Answer Candidates for Open-domain QA of LLMs](https://openreview.net/pdf?id=w4DW6qkRmt)
    - ODQA íƒœìŠ¤í¬ì—ì„œ retrieved passageë¥¼ ë°”íƒ•ìœ¼ë¡œ â€˜ë‹µë³€ í›„ë³´ ìƒì„± - ì¡°ê±´ë¶€ ìš”ì•½ - ê²€ì¦â€™ ê³¼ì¦ì„ ê±°ì³ ë²¤ì¹˜ë§ˆí¬ ì„±ëŠ¥ì„ í¬ê²Œ ëŒì–´ì˜¬ë¦° LK Labì˜ ì—°êµ¬
- ğŸ“œÂ [Microsoft Corporation] [LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression](https://arxiv.org/abs/2403.12968)
    - LLMìœ¼ë¡œë¶€í„° data distillationë¥¼ í†µí•´ ì••ì¶•ëœ í…ìŠ¤íŠ¸ë¥¼ íšë“í•˜ê³  ì´ì— ëŒ€í•´ annotationì„ ìˆ˜í–‰í•œ ë’¤ í•„í„°ë§ì„ ê±°ì³ ë‚˜ì˜¨ ê²°ê³¼ë¥¼ ì••ì¶•í•˜ì—¬ ëª¨ë¸ì— í”„ë¡¬í”„íŠ¸ë¥¼ ì „ë‹¬
- ğŸ§‘ğŸ»â€ğŸ’»Â [Google DeepMind] [TacticAI: an AI assistant for football tactics](https://deepmind.google/discover/blog/tacticai-ai-assistant-for-football-tactics/)
    - ë¦¬ë²„í’€ì˜ ë°ì´í„°ë¥¼ í™œìš©í•´ì„œ ì½”ë„ˆí‚¥ ê²°ê³¼ë¥¼ ì˜ˆì¸¡í•˜ëŠ” AI ëª¨ë¸ì„ ê°œë°œ. ì´ì „ì—ë„ ë¦¬ë²„í’€ ë°ì´í„°ë¥¼ í™œìš©í•œ ê²°ê³¼ê°€ ìˆì—ˆëŠ”ë° í›„ì†ì‘ìœ¼ë¡œ ë‚˜ì˜¨ ë“¯í•¨.
- ğŸ“œÂ [Google DeepMind] [Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models](https://arxiv.org/abs/2310.06117) (ICLRâ€™ 2024)
    - LLMì´ ì£¼ì–´ì§„ ë¬¸ì œë¡œë¶€í„° high-level conceptê³¼ ì›ì¹™ë“¤ì„ ì¶”ì¶œí•´ë‚´ê³  ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ reasoning í•˜ëŠ” Step-Back Promptingì„ ì œì•ˆ. ê°„ë‹¨íˆ ë§í•˜ìë©´ Abstraction â†’ Reasoning ê³¼ì •ì„ ê±°ì¹¨.
- ğŸ“œÂ [AI2] [RewardBench: Evaluating Reward Models for Language Modeling](https://arxiv.org/abs/2403.13787)
    - RLHFì— ê°€ì¥ ì¤‘ìš”í•œ ìš”ì†Œ ì¤‘ í•˜ë‚˜ì¸ Reward Modelì´ rewardë¥¼ ì œëŒ€ë¡œ ë°˜í™˜í•˜ê³  ìˆëŠ”ì§€ í™•ì¸í•  ìˆ˜ ìˆëŠ” ë²¤ì¹˜ë§ˆí¬ë¥¼ ê°œë°œí•˜ì—¬ ê³µê°œ. prompt-win-lose trios ë°ì´í„°ì…‹.
- ğŸ“œÂ [LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models](https://arxiv.org/abs/2403.13372)
    - ë‹¤ì–‘í•œ Efficient fine-tuning ê¸°ë²•ë“¤ì„ ë‚´ì¥ web UI LlamaBoardë¥¼ í†µí•´ ì½”ë”©í•  í•„ìš” ì—†ì´ ê°„ë‹¨í•˜ê³  í¸ë¦¬í•˜ê²Œ ì ìš©í•  ìˆ˜ ìˆëŠ” í”„ë ˆì„ì›Œí¬ë¥¼ ì†Œê°œ
- ğŸ“œÂ [MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?](https://arxiv.org/abs/2403.14624)
    - ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ì´ ê·¸ë¦¼ì„ ì •í™•íˆ ì´í•´í•˜ê³  ë¬¸ì œë¥¼ í‘¸ëŠ”ì§€ í™•ì¸í•˜ê¸° ìœ„í•´ ì‚¬ëŒì´ ì§ì ‘ annotationí•œ í…ŒìŠ¤íŠ¸ ë°ì´í„° 15K ê°œë¥¼ í¬í•¨í•˜ëŠ” MathVerse ë²¤ì¹˜ë§ˆí¬ë¥¼ ê³µê°œ
- ğŸ“œÂ [KAIST] [Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity](https://arxiv.org/abs/2403.14403)
    - classifier (ì‚¬ì´ì¦ˆê°€ ì‘ì€ LM)ì„ í†µí•´ queryë¥¼ straightforward/simple/complex queryë¡œ êµ¬ë¶„í•˜ê³  ê°ê° ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ retrievalì„ ìˆ˜í–‰
- ğŸ“œ [Sakana AI] [Evolutionary Optimization of Model Merging Recipes](https://arxiv.org/abs/2403.13187)
    - ëª¨ë¸ mergeì™€ ê´€ë ¨í•˜ì—¬ ì„ íƒëœ ëª¨ë¸ë“¤ì˜ layerë¥¼ ìë™ì ìœ¼ë¡œ ë³‘í•©í•˜ëŠ” ë°©ë²•ì„ ì œì‹œí•¨.
</details>

<details>
  <summary>5th week</summary>
  
- ğŸ“œÂ [Instructing Large Language Models to Identify and Ignore Irrelevant Conditions](https://arxiv.org/abs/2403.12744)
    - Math Word Problem (MWP)ë¥¼ í’€ ë•Œ ìì£¼ ì‚¬ìš©ë˜ëŠ” CoT promptingì— ëŒ€í•œ ì—°êµ¬. I3Cë¼ëŠ” ë°©ë²•ë¡ ì„ ì œì‹œí–ˆëŠ”ë°, LLMìœ¼ë¡œ í•˜ì—¬ê¸ˆ irrelevant conditionsë¥¼ ë¬´ì‹œí•˜ë„ë¡ instructí•˜ëŠ” ë°©ì‹ì„. ì´ê²ƒì´ RAGì—ë„ ì ìš©ë  ìˆ˜ ìˆì§€ ì•Šì„ê¹Œ í•˜ëŠ” ìƒê°ì´ ë“¦.
- ğŸ“œÂ [Microsoft Research, CMU] [Can large language models explore in-context?](https://arxiv.org/abs/2403.15371)
    - GPT-3.5, GPT-4, Llama2ë¥¼ ëŒ€ìƒìœ¼ë¡œ ë‹¤ì–‘í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ë””ìì¸í•´ì„œ ì‹¤í—˜ì„ ìˆ˜í–‰. ê²°êµ­ ì§€ê¸ˆê¹Œì§€ì˜ ì–¸ì–´ ëª¨ë¸ë“¤ì€ ìƒë‹¹í•œ interventions(ì˜ˆë¥¼ ë“¤ì–´ fine-tuning) ì—†ì´ëŠ” robustí•œ í–‰ë™ ì–‘ìƒì„ ë³´ì¼ ìˆ˜ ì—†ë‹¤ëŠ” ê²°ë¡ ì„ ë‚´ë¦¼
- ğŸ§‘ğŸ»â€ğŸ’»Â [Lightning AI] [lightning-thunder](https://github.com/Lightning-AI/lightning-thunder?tab=readme-ov-file)
    - íŒŒì´í† ì¹˜ë¥¼ í™œìš©í•œ LLM í•™ìŠµ ì†ë„ë¥¼ 40% ê°€ëŸ‰ í–¥ìƒì‹œì¼œì£¼ëŠ” compilerë¥¼ ê³µê°œ. single accelerator & multi-GPU í™˜ê²½ì—ì„œ ëª¨ë‘ í™œìš© ê°€ëŠ¥.
- ğŸ“œÂ [Johns Hopkins, Yale, AI2] [FOLLOWIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions](https://arxiv.org/abs/2403.15246)
    - Information Retrieval (IR) ì— LLMì„ ì‚¬ìš©í•˜ë”ë¼ë„ ì§€ê¸ˆê¹Œì§€ëŠ” ë‹¨ìˆœíˆ queryë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì„ ë¿ì´ì—ˆìŒ â†’ instruction following retrieval model, FollowIRì„ ì œì•ˆ
- ğŸ“œÂ [UC Berkeley] [LLM2LLM: Boosting LLMs with Novel Iterative Data Enhancement](https://arxiv.org/abs/2403.15042)
    - baseline student LLMì„ ì´ˆê¸° ë°ì´í„°ì…‹ì— ëŒ€í•´ í•™ìŠµ â†’ í•™ìŠµ ê²°ê³¼ë¥¼ í‰ê°€í•˜ì—¬ ì˜ëª»ëœ ì¼€ì´ìŠ¤ë“¤ì„ ëª¨ìŒ â†’ teacher LLMì´ ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•©ì„± ë°ì´í„°ë¥¼ ìƒì„±í•˜ì—¬ í•™ìŠµ ë°ì´í„°ì— ì¶”ê°€
- ğŸ“œ [Rutgers University] [AIOS: LLM Agent Operating System](https://arxiv.org/abs/2403.16971)
    - LLM agentë¥¼ operating systemì— ì§‘ì–´ ë„£ì–´ OSì˜ ë‡Œ ì—­í• ì„ ìˆ˜í–‰í•˜ë„ë¡ í•¨
- ğŸ“œÂ [MIT, Berkeley, Chicago, Texas] [Decoding Compressed Trust: Scrutinizing the Trustworthiness of Efficient LLMs Under Compression](https://arxiv.org/abs/2403.15447)
    - 3ê°œì˜ LLMì— 4ê°œì˜ compression techniqueì„ ì ìš©í•´ 8ê°œ ì°¨ì›ìœ¼ë¡œ í‰ê°€. 3-bitì™€ ê°™ì€ low bit ìˆ˜ì¤€ì˜ quantizationì€ trustworthinessë¥¼ í¬ê²Œ í•˜ë½ì‹œí‚´
- ğŸ§‘ğŸ»â€ğŸ’»Â [OpenAI] [Sora: first impressions](https://openai.com/blog/sora-first-impressions)
    - ì—¬ëŸ¬ ì•„í‹°ìŠ¤íŠ¸ë“¤ì´ Soraì„ ì´ìš©í•´ì„œ ë§Œë“  ë™ì˜ìƒ ê²°ê³¼ë¬¼ë“¤ì„ OpenAI ë¸”ë¡œê·¸ì— ê³µê°œ. ìì—°ìŠ¤ëŸ¬ìš´ ë‚´ìš© ì „ê°œê°™ì€ ê±´ ì—†ì§€ë§Œ ì‹ ë¹„ìŠ¤ëŸ¬ìš´ ëŠë‚Œì„ ì£¼ëŠ” ì´ˆê³ í€„ë¦¬í‹°ì˜ ì˜ìƒë“¤ì„.
- ğŸ§‘ğŸ»â€ğŸ’»Â [Databricks] [Introducing DBRX: A New State-of-the-Art Open LLM](https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm)
    - Grok-1ì˜ 40% ì‚¬ì´ì¦ˆë°–ì— ë˜ì§€ ì•Šìœ¼ë©´ì„œë„ LLaMA2-70Bë³´ë‹¤ ì¶”ë¡ ë„ ë‘ ë°°ë‚˜ ë¹ ë¥´ê³  GPT-3.5-turboë¥¼ ëŠ¥ê°€í•˜ë©° Gemini Pro 1.0ì— ì¤€í•˜ëŠ” ì„±ëŠ¥ì˜ LLM, DBRXì„ [í—ˆê¹…í˜ì´ìŠ¤ì— ê³µê°œ](https://huggingface.co/spaces/databricks/dbrx-instruct)
    - MoEë¥¼ í™œìš©í•˜ì—¬ 132B/32B ì „ì²´/í™œì„± íŒŒë¼ë¯¸í„° ì‚¬ì´ì¦ˆë¥¼ ê°€ì§. 32K context length ì§€ì›
- ğŸ§‘ğŸ»â€ğŸ’»Â [Anthropic] [Claude-3-Opus vs GPT-4](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard)
    - Chatbot Arenaì—ì„œ GPT-4ì˜ ì™•ì¢Œë¥¼ Claudeê°€ íƒˆí™˜..!
- ğŸ“œÂ [Meta, MIT] [The Unreasonable Ineffectiveness of the Deeper Layers](https://arxiv.org/abs/2403.17887)
    - layer pruningì´ ë‹¤ë¥¸ PEFT ì „ëµì„ ë³´ì™„/ëŒ€ì²´í•  ìˆ˜ ìˆëŠ” ë°©ë²•ë¡ ì„ì„ í™•ì¸í•¨ê³¼ ë™ì‹œì—, í˜„ì¬ì˜ ì‚¬ì „í•™ìŠµ ë°©ì‹ë“¤ì€ deep layersì— ì†í•œ íŒŒë¼ë¯¸í„°ë“¤ì„ ì˜¨ì „íˆ í™œìš©í•˜ê³  ìˆì§€ ëª»í•¨ì„ ì…ì¦í•œ ì—°êµ¬
- ğŸ“œÂ [Univ. of Hong Kong] [Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models](https://arxiv.org/abs/2403.18814)
    - visual tokenì„ ê°•í™”í•˜ê¸° ìœ„í•´ additional visual encoderë¥¼ ì‚¬ìš©. MoEë¥¼ í™œìš©í•˜ì—¬ 2B-34B ì‚¬ì´ì¦ˆì˜ ëª¨ë¸ë“¤ì„ ì§€ì›
- ğŸ“œÂ [Meta, Mila, McGil, Montreal] [Improving Text-to-Image Consistency via Automatic Prompt Optimization](https://arxiv.org/abs/2403.17804)
    - text-to-image (T2I)ì—ì„œì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ í”„ë ˆì„ì›Œí¬ë¡œ T2I optimization-by-prompting (OPT2I)ì„ ì œì‹œ.
- ğŸ“œÂ [MIT, Microsoft] [Supervisory Prompt Training](https://arxiv.org/abs/2403.18051)
    - dual LLM systemì„ ì´ìš©í•˜ì—¬ promptë¥¼ ìë™ì ìœ¼ë¡œ ìƒì„±. ë¬¸ì¥ ìˆ˜ì¤€ì—ì„œì˜ íš¨ìš©ì„±ì„ í™•ì¸í•˜ê¸° ìœ„í•œ impact score ê°œë…ì„ ê³ ì•ˆ.
- ğŸ“œÂ [Upstage] [sDPO: Don't Use Your Data All at Once](https://arxiv.org/abs/2403.19270)
    - alignment tuning ë‹¨ê³„ì—ì„œ ì‚¬ìš©ë  ìˆ˜ ìˆëŠ” stepwise DPO (sDPO)ë¥¼ ì œì•ˆ. ì´ìš© ê°€ëŠ¥í•œ ì„ í˜¸ ë°ì´í„°ì…‹ì„ ë¶„í• í•˜ì—¬ stepwise ë°©ì‹ìœ¼ë¡œ ì‚¬ìš© (í•œêº¼ë²ˆì— ì‚¬ìš©í•˜ëŠ” ëŒ€ì‹ ì—)
- ğŸ§‘ğŸ»â€ğŸ’»Â [HuggingFace] [A little guide to building Large Language Models in 2024](https://www.youtube.com/watch?v=2-SPH9hIKT8)
    - í—ˆê¹…í˜ì´ìŠ¤ cofounder ì¤‘ í•œëª…ì´ ì§ì ‘ ì´¬ì˜í•˜ì—¬ ì—…ë¡œë“œí•œ LLM ê¸°ì´ˆ ê°•ì˜ (1ì‹œê°„ 15ë¶„)
- ğŸ§‘ğŸ»â€ğŸ’»Â [AI21labs] [Introducing Jamba: AI21's Groundbreaking SSM-Transformer Model](https://www.ai21.com/blog/announcing-jamba)
    - transformer ì•„í‚¤í…ì³ì™€ structured State Space Model (SSM) ê¸°ìˆ ì„ ê²°í•©í•˜ì—¬ ë” ë†’ì€ throughputì„ ê°€ì§€ë©´ì„œë„ ì¢‹ì€ ì„±ëŠ¥ì„ ê°€ì§„ ëª¨ë¸ (256K ìœˆë„ìš° ì‚¬ì´ì¦ˆ)
- ğŸ“œÂ [Can multiple-choice questions really be useful in detecting the abilities of LLMs?](https://arxiv.org/abs/2403.17752)
    - Multiple-choice question(MQA)ê°€ LLMì„ í‰ê°€í•˜ëŠ” ë° ì í•©í•˜ì§€ ì•Šì€ ë°©ì‹ì„ì„ ì„¤ëª…. ê²°ê³¼ê°€ ì§ˆë¬¸ì´ ì œì‹œë˜ëŠ” ìˆœì„œì— í° ì˜í–¥ì„ ë°›ëŠ”ë‹¤ëŠ” ì ê³¼ long-form generation(LFG)ë¡œ í‰ê°€í–ˆì„ ë•Œ ê²°ê³¼ì™€ì˜ ë‚®ì€ ìƒê´€ê´€ê³„ë¥¼ ê·¸ ê·¼ê±°ë¡œ ë“¦
- ğŸ“œÂ [Understanding Emergent Abilities of Language Models from the Loss Perspective](https://arxiv.org/abs/2403.15796)
    - LLMì—ì„œì˜ emergent abilityë¥¼ ëª¨ë¸ ì‚¬ì´ì¦ˆ ëŒ€ì‹  ë¡œìŠ¤ ê¸°ì¤€ìœ¼ë¡œ ë¶„ì„. ë™ì¼í•œ ì‚¬ì „ í•™ìŠµ lossë¥¼ ê°–ëŠ” ê²½ìš°, ëª¨ë¸ì˜ ì‚¬ì´ì¦ˆê°€ í¬ë”ë¼ë„ ë™ì¼í•œ í¼í¬ë¨¼ìŠ¤ë¥¼ ë‚¸ë‹¤ëŠ” ê²°ê³¼ë¥¼ ì œì‹œ
</details>


## ğŸŒ¸ April
<details>
  <summary>1st week</summary>

- ğŸ§‘ğŸ»â€ğŸ’»Â [Anthropic] [Prompt library](https://docs.anthropic.com/claude/prompt-library)
    - ê°ì¢… ìƒí™©ì— ì í•©í•œ í”„ë¡¬í”„íŠ¸ë“¤ì„ ê²€ìƒ‰í•  ìˆ˜ ìˆëŠ” í”„ë¡¬í”„íŠ¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
- ğŸ§‘ğŸ»â€ğŸ’»Â [xAI] [Announcing Grok-1.5](https://x.ai/blog/grok-1.5)
    - 128K í† í°ì„ ì»¨í…ìŠ¤íŠ¸ë¡œ ê°–ëŠ” ì‹ ëª¨ë¸. Xì—ì„œ ì¼ë¶€ ìœ ì €ë“¤ì—ê²Œ ì„ ê³µê°œë  ì˜ˆì •
- ğŸ“œÂ [Can LLMs Learn from Previous Mistakes? Investigating LLMs' Errors to Boost for Reasoning](https://arxiv.org/abs/2403.20046)
    - LLMì´ ì˜ëª»ëœ ë‚´ìš©ë“¤ë¡œë¶€í„° ì–»ëŠ” ì´ë“ì´ ìˆëŠ”ì§€ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•´ ê´€ë ¨ ë°ì´í„°ì…‹ì„ ì§ì ‘ ì œì‘í•˜ì—¬ ì‹¤í—˜í•œ ê²°ê³¼ë¥¼ ì œì‹œ
- ğŸ“œÂ [Meta] [The Unreasonable Ineffectiveness of the Deeper Layers](https://arxiv.org/abs/2403.17887v1)
    - single A100 gpuì—ì„œ ëŒë¦´ ìˆ˜ ìˆë„ë¡ PEFTë¥¼ ì´ìš©í•˜ì—¬ QA ë²¤ì¹˜ë§ˆí¬ ê²€ì¦. LLaMA íŒ¨ë°€ë¦¬ì˜ ê²½ìš° 40%ì˜ ë ˆì´ì–´ë¥¼ ì‚­ì œí•´ë„ ê¸°ì¡´ì˜ accuracyë¥¼ ìœ ì§€í•  ìˆ˜ ìˆë‹¤ëŠ” ê²°ê³¼.
- ğŸ§‘ğŸ»â€ğŸ’»Â [OpenAI] [Navigating the Challenges and Opportunities of Synthetic Voices](https://openai.com/blog/navigating-the-challenges-and-opportunities-of-synthetic-voices)
    - 15ì´ˆì§œë¦¬ referenceë§Œ ìˆìœ¼ë©´ ë™ì¼í•œ ëª©ì†Œë¦¬ë¡œ ë‹¤ë¥¸ ë¬¸ì¥ì„ ì½ëŠ” ë³´ì´ìŠ¤ë¥¼ ìƒì„±í•  ìˆ˜ ìˆëŠ” ëª¨ë¸. ì•…ìš© ê°€ëŠ¥ì„± ë•Œë¬¸ì— ê³µê°œí•˜ì§€ëŠ” ì•ŠìŒ
- ğŸ“œÂ [AI21labs] [Jamba: A Hybrid Transformer-Mamba Language Model](https://arxiv.org/abs/2403.19887)
    - transformer ì•„í‚¤í…ì³ì™€ structured State Space Model (SSM) ê¸°ìˆ ì„ ê²°í•©í•˜ì—¬ ë” ë†’ì€ throughputì„ ê°€ì§€ë©´ì„œë„ ì¢‹ì€ ì„±ëŠ¥ì„ ê°€ì§„ ëª¨ë¸ (256K ìœˆë„ìš° ì‚¬ì´ì¦ˆ)
- ğŸ“œÂ [Google DeepMind] [Gecko: Versatile Text Embeddings Distilled from Large Language Models](https://arxiv.org/abs/2403.20327)
    - LLMì˜ ì§€ì‹ì„ retriever ëª¨ë¸ì— distill í–ˆë‹¤ëŠ” ì»¨ì…‰ì„ ì§€ë‹Œ embedding ëª¨ë¸. MTEB ë²¤ì¹˜ë§ˆí¬ì—ì„œ 256 ì„ë² ë”© ì°¨ì›ìœ¼ë¡œ 768 ì°¨ì›ì˜ ëª¨ë¸ ì„±ëŠ¥ì„ ë„˜ì–´ì„°ìŒ
- ğŸ“œÂ [Apple] [ReALM: Reference Resolution As Language Modeling](https://arxiv.org/abs/2403.20329)
    - LLMì„ ë‹¤ì–‘í•œ ì¢…ë¥˜ì˜ referenceë¥¼ resolve í•˜ëŠ” ë° ì‚¬ìš© â†’ ì‹œë¦¬ê°€ ì´ì œ ìœ ì €ì˜ í™”ë©´ì„ ì¸ì‹í•˜ê³  ì§ˆì˜ì— ì‘ë‹µ ê°€ëŠ¥
- ğŸ—ï¸Â [Microsoft and OpenAI pledge $100 billion for â€˜Stargateâ€™ supercomputer facility](https://interestingengineering.com/culture/microsoft-and-openai-want-to-build-a-100-billion-datacenter)
    - MSì™€ OpenAIê°€ ìŠˆí¼ì»´í“¨í„°ì™€ ë°ì´í„°ì„¼í„° êµ¬ì¶•ì— 2028ë…„ê¹Œì§€ 1000ì–µ ë‹¬ëŸ¬(130ì¡° ì›)ì„ ë“¤ì¼ ì˜ˆì •
- ğŸ“œÂ [Microsoft] [Injecting New Knowledge into Large Language Models via Supervised Fine-Tuning](https://arxiv.org/abs/2404.00213)
    - GPT-4ë¥¼ ëŒ€ìƒìœ¼ë¡œ ì§ì ‘ êµ¬ì¶•í•œ ë°ì´í„°ì…‹ì— ëŒ€í•´ SFTë¥¼ ìˆ˜í–‰í•œ ê²°ê³¼, LLM responseì˜ factualityë¥¼ ë†’ì¼ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ì…ì¦. ì´ë•Œ ì‚¬ìš©ëœ â€˜dataset generation strategiesâ€™ê°€ í•µì‹¬.
- ğŸ“œÂ [Naver Cloud] [HyperCLOVA X Technical Report](https://arxiv.org/abs/2404.01954)
    - í•œêµ­ì–´, ì˜ì–´, ì½”ë“œ ë°ì´í„°ë¥¼ ì ì ˆíˆ í˜¼í•©í•˜ì—¬ í•™ìŠµí•œ HyperCLOVA X ëª¨ë¸ì˜ technical reportë¥¼ ê³µê°œ. í•œêµ­ì–´ì™€ í•œêµ­ì˜ ë¬¸í™”ì  ë‰˜ì•™ìŠ¤ì— ëŒ€í•œ ì´í•´ë„ê°€ ë†’ì€ ê²ƒìœ¼ë¡œ í™•ì¸ë¨
- ğŸ“œÂ [Anthropic] [Many-shot jailbreaking](https://www.anthropic.com/research/many-shot-jailbreaking)
    - Anthropic ë¿ë§Œ ì•„ë‹ˆë¼ íƒ€ì‚¬ì˜ LLMì—ë„ ì ìš© ê°€ëŠ¥í•œ jailbreakingì„ ì—°êµ¬í•œ ê²°ê³¼ë¥¼ ê³µê°œ. ê°„ë‹¨í•˜ë©´ì„œë„ íš¨ê³¼ì ì¸ attackì— ëŒ€í•´ ì—°êµ¬.
- ğŸ“œÂ [Efficient Prompting Methods for Large Language Models: A Survey](https://arxiv.org/abs/2404.01077)
    - í”„ë¡¬í”„íŠ¸ë¥¼ ì••ì¶•í•˜ëŠ” ë“±ì˜ computation ê´€ë ¨ ì—°êµ¬ì™€ ìµœì ì˜ í”„ë¡¬í”„íŠ¸ë¥¼ ì°¾ëŠ” optimization ê´€ë ¨ ì—°êµ¬ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ í•œ ì§§ì€ ì„œë² ì´ í˜ì´í¼
- ğŸ“œÂ [Beyond Accuracy: Evaluating the Reasoning Behavior of Large Language Models -- A Survey](https://arxiv.org/abs/2404.01869)
    - í‘œë©´ì ì¸ ì •í™•ë„ë¥¼ ê¸°ì¤€ìœ¼ë¡œ LLMì˜ ì¶”ë¡  ëŠ¥ë ¥ì„ í‰ê°€ê°€ ì´ë¤„ì¡Œì—ˆë˜ ê²ƒì„ ë¬¸ì œì ìœ¼ë¡œ ì§€ì . ì‚¬ëŒê³¼ LLMì˜ ì¶”ë¡  ë°©ì‹ ê°„ì˜ ì°¨ì´ë¥¼ ì„¤ëª…í•œ ì§§ì€ ì„œë² ì´ í˜ì´í¼.
- ğŸ“œÂ [University of Waterloo, CMU] [Long-context LLMs Struggle with Long In-context Learning](https://arxiv.org/abs/2404.02060)
    - perplexityë‚˜ í•©ì„± íƒœìŠ¤í¬ ì •ë„ë¡œëŠ” long sequenceë¥¼ ë‹¤ë£¨ëŠ” LLMì˜ ëŠ¥ë ¥ì„ ì œëŒ€ë¡œ í‰ê°€í•  ìˆ˜ ì—†ìŒ. ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ LongICLBenchë¥¼ ì œì‹œ. ëª¨ë“  ëª¨ë¸ë“¤ì´ â€˜ì—„ì²­ ê¸´â€™ í…ìŠ¤íŠ¸ëŠ” ì „í˜€ ë‹¤ë£¨ì§€ ëª»í•œë‹¤ëŠ” ê²ƒì„ í™•ì¸.
- ğŸ“œÂ [Tsinghua University, UIUC] [Advancing LLM Reasoning Generalists with Preference Trees](https://arxiv.org/abs/2404.02078)
    - Mistral-7Bì™€ CodeLlama-70Bì— fine-tuningëœ reasoning ìµœì í™” LLM, EURUSë¥¼ ê³µê°œ. ì´ëŠ” large-scale & high qualityì˜ alignment ë°ì´í„°ì…‹ UltraInteractë¥¼ êµ¬ì¶•í•¨ì— ê¸°ì¸.
- ğŸ“œÂ [Google DeepMind] [Mixture-of-Depths: Dynamically allocating compute in transformer-based language models](https://arxiv.org/abs/2404.02258)
    - transformer ê¸°ë°˜ì˜ ëª¨ë¸ë“¤ì€ ê¸°ì¡´ì— ì…ë ¥ ì‹œí€€ìŠ¤ ì „ì²´ì— ê±¸ì³ FLOPsì„ ê· ë“±í•˜ê²Œ ë¶„ë°° â†’ ì´ë¥¼ ëª¨ë¸ depthì— ë”°ë¼ dynamicí•˜ê²Œ í• ë‹¹í•¨ìœ¼ë¡œì¨ ìµœì í™”. top-k routing ë©”ì»¤ë‹ˆì¦˜ì„ ì´ìš©.
- ğŸ—ï¸Â [DALL-E now lets you edit images in ChatGPT](https://www.theverge.com/2024/4/3/24120181/openai-dall-e-chat-gpt-image-edit)
    - ChatGPTì—ì„œ DALLEë¡œ ìƒì„±í•œ ì´ë¯¸ì§€ì˜ ì˜ì—­ì„ ì§€ì •í•˜ì—¬ ë¶€ë¶„ ìˆ˜ì •ì´ ê°€ëŠ¥í•´ì§ (GPTs ì‚¬ìš©)
- ğŸ§‘ğŸ»â€ğŸ’»Â [Anthropic] [Claude can now use tools](https://docs.anthropic.com/claude/docs/tool-use)
    - Claudeì—ì„œ tool use ê¸°ëŠ¥ì„ betaë¡œ ê³µê°œ. ìì„¸í•œ ë‚´ìš©ì€ API doucmentë¥¼ ì°¸ê³ .
- ğŸ“œÂ [Google DeepMind, Anthropic] [Training LLMs over Neurally Compressed Text](https://arxiv.org/abs/2404.03626)
    - LLMì´ í•™ìŠµí•  textë¥¼ ì••ì¶•í•  ë•Œ, í…ìŠ¤íŠ¸ë¥¼ ì—¬ëŸ¬ segmentë¡œ ìª¼ê°œê³  ë™ì¼í•œ ê¸¸ì´ì˜ bitë¡œ ë§Œë“œëŠ” ë°©ì‹ì¸ Equal-Info Windowsë¥¼ ì œì•ˆ
</details>

<details>
  <summary>2nd week</summary>

- ğŸ§‘ğŸ»â€ğŸ’»Â [Stability AI] [Introducing Stable Audio 2.0](https://stability.ai/news/stable-audio-2-0)
    - text-to-audio ë¿ë§Œ ì•„ë‹ˆë¼ audio-to-audio ë„ ê°€ëŠ¥. ì¦‰, audioë¡œ ìƒˆë¡œìš´ audioë¥¼ ìƒì„±í•˜ëŠ” ê¸°ëŠ¥ì„ ì§€ì›. ì´ ëª¨ë¸ì€ Diffusion Transformer (DiT) ì•„í‚¤í…ì³ë¥¼ ë”°ë¥´ê³  ìˆìŒ
- ğŸ§‘ğŸ»â€ğŸ’»Â [MyShell, MIT-IBM, Princeton, Lepton AI] [JetMoE: Reaching LLaMA2 Performance with 0.1M Dollars](https://research.myshell.ai/jetmoe)
    - ì•½ 1ì–µ 3ì²œ ë§Œì› ì •ë„ì˜ ë¹„ìš©ìœ¼ë¡œ LLaMA2ë¥¼ ìƒíšŒí•˜ëŠ” ëŠ¥ë ¥ì˜ ëª¨ë¸ JetMoEë¥¼ í•™ìŠµí–ˆë‹¤ê³  ë°í˜. publicly ì´ìš© ê°€ëŠ¥í•œ ë°ì´í„°ë§Œìœ¼ë¡œ í•™ìŠµëœ ëª¨ë¸ì´ë¼ëŠ” ì ì„ ê°•ì¡°. í–¥í›„ technical report ê³µê°œ ì˜ˆì • (ì•„ì§ x)
- ğŸ“œÂ [University of Copenhagen, Google DeepMind] [MuLan: A Study of Fact Mutability in Language Models](https://arxiv.org/abs/2404.03036)
    - ì‹œê°„ê³¼ ê°™ì€ contingencyì— ë”°ë¼ ì •ë³´ê°€ mutable(ë³€ê²½ë ìˆ˜ë„) ìˆë‹¤. mutable factsëŠ” ê·¸ë ‡ì§€ ì•Šì€ ê²ƒê³¼ ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì¸ì½”ë”©ë˜ì–´ ì—…ë°ì´íŠ¸í•˜ê¸° ë” ì‰¬ìš¸ ê²ƒì´ë¼ëŠ” ê°€ì„¤ â†’ 1:1, 1:N ê´€ê³„ì— ëŒ€í•œ ë¶„ì„
- ğŸ“œÂ [Stanford, MIT] [Stream of Search (SoS): Learning to Search in Language](https://arxiv.org/abs/2404.03683)
    - ë¬¸ì œë¥¼ í’€ê¸° ìœ„í•´ searchê°€ í•„ìš”í•œ ë°ì´í„°ì…‹ì— ëŒ€í•´ transformer ê¸°ë°˜ì˜ ëª¨ë¸ì„ from scratch í•™ìŠµí•œ ëª¨ë¸
- ğŸ“œÂ [Stanford, Georgia] [Social Skill Training with Large Language Models](https://arxiv.org/abs/2404.04204)
    - ì‚¬ëŒì´ social skillsì— ì˜ì¡´í•˜ëŠ” ê²ƒì²˜ëŸ¼ LLMë„ ì´ëŸ¬í•œ ë©”ì»¤ë‹ˆì¦˜ì„ í™œìš©í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” í”„ë ˆì„ì›Œí¬, APAM(AI Partner, AI Mentor)ë¥¼ ì œì‹œ
- ğŸ“œÂ [Microsoft Research] [Models to Self-Improve with General Preferences](https://arxiv.org/abs/2404.03715)
    - Preferenceë¥¼ ìµœì í™”í•˜ê¸° ìœ„í•´ contrastive learningì˜ ë‹¨ìˆœí•¨ê³¼ ì•ˆì „ì„±ì„ theoretical generalityì™€ ê²°í•©í•œ Direct Nash Optimization(DNO)ë¥¼ ì œì‹œ. ì‘ì€ ì‚¬ì´ì¦ˆ(Orca-2 7B) ëª¨ë¸ì„ GPT-4ì™€ AlpacaEvalë¡œ í…ŒìŠ¤íŠ¸í–ˆì„ ë•Œ í° ì„±ê³¼ í–¥ìƒì´ ìˆì—ˆìŒ
- ğŸ§‘ğŸ»â€ğŸ’»Â [W&B] [Weight & Biases Docs](https://docs.wandb.ai/ko/?mkt_tok=MjYxLVFIUC04MjIAAAGSX8W79t-qKeYqkWAB6xTAK2R-027DfjjyAUi4hj32ywDET-u3DS8zoc8EGTXUmD6FeRTJjKotiQYg8qjBWT3683U-z133NpaQSmQJ8gRp)
    - W&Bì˜ documentê°€ í•œê¸€íŒìœ¼ë¡œ ê³µì‹ ë°°í¬ë¨
- ğŸ§‘ğŸ»â€ğŸ’»Â [Tesla] [Robotaxi](https://twitter.com/elonmusk/status/1776351450542768368)
    - ì¼ë¡  ë¨¸ìŠ¤í¬ê°€ Xì— Teslaì˜ Robotaxiê°€ 8ì›” 8ì¼ ì¶œì‹œë  ì˜ˆì •ì„ì„ ì•Œë¦¼
- ğŸ§‘ğŸ»â€ğŸ’»Â [Andrej Karpathy] [llm.c](https://github.com/karpathy/llm.c)
    - GPT-2 ëª¨ë¸ í•™ìŠµ ì½”ë“œ ì‘ì„±ì— pytorchë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  ì˜¤ì§ cë§Œ ì‚¬ìš©í•¨. 1,000ì—¬ ì¤„ì˜ ì½”ë“œë¡œ GPT-2ì˜ í•™ìŠµ ê³¼ì •ì„ íŒŒì•…í•  ìˆ˜ ìˆìŒ.
- ğŸ§‘ğŸ»â€ğŸ’»Â [3Blue1Brown] [Attention in transformers, visually explained](https://www.youtube.com/watch?v=eMlx5fFNoYc&t=27s)
    - ì§€ë‚œ ë²ˆ Transformer ì‹œê°í™” ì˜ìƒ ì´í›„ í›„ì† ì˜ìƒ ì—…ë¡œë“œ
- ğŸ“œÂ [Mila, McGil] [LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders](https://arxiv.org/abs/2404.05961)
    - decoder-only LLMì— 1) bidiriectional attention, 2) masked token next prediction, 3) unsupervised contrastive learningì„ ì ìš©í•˜ì—¬ ê¸°ì¡´ì˜ encoder ëª¨ë¸ë“¤ë³´ë‹¤ í›¨ì”¬ ë›°ì–´ë‚œ MTEB ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ë¥¼ ë‹¬ì„±í•¨
- ğŸ“œÂ [Google] [Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention](https://arxiv.org/abs/2404.07143)
    - ì••ì¶•ì ì¸ ì •ë³´ë¥¼ vanilla attention mechanismì— ë„£ê³ , single Transformer ë¸”ë¡ ë‚´ì—ì„œ masked local attentionê³¼ long-term linear attention ë©”ì»¤ë‹ˆì¦˜ì„ êµ¬í˜„í•˜ëŠ” ë°©ì‹, Infini-attentionì„ ì œì•ˆ. ì´ë¥¼ í†µí•´ LLMì´ long context íƒœìŠ¤í¬ë¥¼ ì˜ ìˆ˜í–‰í•  ìˆ˜ ìˆê²Œ ë¨
- ğŸ“œÂ [NVIDIA] [RULER: What's the Real Context Size of Your Long-Context Language Models?](https://arxiv.org/abs/2404.06654)
    - Needle-In-A-Haystack (NIAH) íƒœìŠ¤í¬ì— multi-hop tracingê³¼ aggregation ì¹´í…Œê³ ë¦¬ë¥¼ ìƒˆë¡œì´ ì¶”ê°€í•œ synthetic benchmark, Rulerë¥¼ ê³µê°œ
- ğŸ“œÂ [UIUC] [Graph Chain-of-Thought: Augmenting Large Language Models by Reasoning on Graphs](https://arxiv.org/abs/2404.07103)
    - ëŒ€ë¶€ë¶„ì˜ ë„ë©”ì¸ì—ì„œ í…ìŠ¤íŠ¸ëŠ” ìƒí˜¸ ê´€ê³„ë¥¼ ê°–ëŠ”ë‹¤ëŠ” ì ì— ê·¼ê±°í•˜ì—¬ Graph Reasoning Benchmark (GRBench)ë¥¼ ì§ì ‘ ì œì‘. 10ê°œì˜ ë„ë©”ì¸ì—ì„œ 1,740ê°œ QAë¥¼ ë‹¤ë£¸.
- ğŸ“œÂ [Apple] [Superposition Prompting: Improving and Accelerating Retrieval-Augmented Generation](https://arxiv.org/abs/2404.06910)
    - ì‚¬ì „í•™ìŠµëœ íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ì˜ ëª¨ë¸ì— fine-tuning ì—†ì´ ë°”ë¡œ ì ìš© ê°€ëŠ¥í•œ RAG prompting methodology, superposition promptingì„ ì œì•ˆ. ì…ë ¥ ë¬¸ì„œë¥¼ parallelí•œ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬í•˜ë©° ë¶ˆí•„ìš”í•œ ê²ƒì„ ë²„ë¦¬ë„ë¡ í•¨.
- ğŸ“œÂ [Tsinghua, Microsoft] [Rho-1: Not All Tokens Are What You Need](https://arxiv.org/abs/2404.07965)
    - ëª¨ë“  í† í°ì´ ë™ì¼í•œ ì¤‘ìš”ë„ë¥¼ ê°–ì§€ ì•Šìœ¼ë¯€ë¡œ, ì‚¬ì „í•™ìŠµ ë‹¨ê³„ì—ì„œ reference ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì¤‘ìš”ë„ê°€ ë†’ì€ í† í°ì— ëŒ€í•´ focused lossë¥¼ ì ìš©í•˜ëŠ” ë°©ì‹ì¸ Selective Language Modeling (SLM)ì„ ì œì•ˆ. ì´ ë°©ì‹ìœ¼ë¡œ í•™ìŠµëœ LLMì´ Rho-1 ëª¨ë¸.
- ğŸ“œÂ [Google DeepMind] [RecurrentGemma: Moving Past Transformers for Efficient Open Language Models](https://arxiv.org/abs/2404.07839)
    - Griffin ëª¨ë¸ì˜ ì•„í‚¤í…ì³ë¥¼ ê¸°ë°˜ìœ¼ë¡œ linear recurrenceì— local attentionì„ ê²°í•©í•˜ì—¬ í•™ìŠµí•œ ëª¨ë¸ RecurrentGemmaë¥¼ ê³µê°œ. 2B non-embedding parameters ë²„ì „ì˜ ëª¨ë¸ê³¼ instruction tuned ë²„ì „ì„ ì œê³µ
- ğŸ§‘ğŸ»â€ğŸ’»Â [IBM] [IBM watsonx chat](https://dataplatform.cloud.ibm.com/chat/login?redirect_url=%2Fchat%2F)
    - IBM [watsonx.ai](http://watsonx.ai) studioì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ LLM ì±— ëª¨ë¸ì„ ê³µê°œ. granite-13b-chat-v2, llama-2-13-chat, llama-2-70b-chat, ì„¸ ì¢…ë¥˜ì˜ ë²„ì „ì„ ê³µê°œí•¨.
</details>

<details>
  <summary>3rd week</summary>
  
- ğŸ§‘ğŸ»â€ğŸ’»Â [Mistral] [Mixtral-8x22B-v0.1-4bit](https://huggingface.co/mistral-community/Mixtral-8x22B-v0.1-4bit)
    - 176B íŒŒë¼ë¯¸í„°, 44B active íŒŒë¼ë¯¸í„° (ì¶”ë¡  ì‹œ), 65K context window, 8 experts & 2 per token, 32K vocab
- ğŸ§‘ğŸ»â€ğŸ’»Â [xAI] [Grok-1.5 Vision Preview](https://x.ai/blog/grok-1.5v)
    - xAIì—ì„œ ê³µê°œí•œ ì²« ë²ˆì§¸ ë©€í‹°ëª¨ë‹¬ ëª¨ë¸. zero-shot ê¸°ì¤€ìœ¼ë¡œ GPT-4Vì— í•„ì í•˜ê±°ë‚˜ ê·¸ ì´ìƒì˜ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ëŠ” ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ë„ ì¡´ì¬.
- ğŸ“œÂ [Google] [CodeGemma: Open Code Models Based on Gemma](https://storage.googleapis.com/deepmind-media/gemma/codegemma_report.pdf)
    - RecurrentGemmaì™€ í•¨ê»˜ ê³µê°œí•œ ì½”ë“œ ë°ì´í„°ë¥¼ í•™ìŠµí•œ Gemma ëª¨ë¸. 7B pretrained (PT) ë²„ì „ê³¼ instruction-tuned (IT) ë²„ì „ ë‘ ê°œë¥¼ ê³µê°œ.
- ğŸ—ï¸Â [Meta is testing an AI-powered search bar in Instagram](https://techcrunch.com/2024/04/12/meta-is-testing-an-ai-powered-search-bar-in-instagram/)
    - ì¸ìŠ¤íƒ€ê·¸ë¨ì—ì„œ ë¦´ìŠ¤, í¬ìŠ¤íŠ¸ë¥¼ ê²€ìƒ‰í•˜ê±°ë‚˜ ì§ˆë¬¸ì„ í•  ë•Œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” AI ê¸°ëŠ¥ ë„ì…ì„ í…ŒìŠ¤íŠ¸ ì¤‘ì´ë¼ê³  ì•Œë ¤ì§
- ğŸ§‘ğŸ»â€ğŸ’»Â [DeepLearning.AI] [Quantization Fundamentals with HuggingFace](https://www.deeplearning.ai/short-courses/quantization-fundamentals-with-hugging-face/)
    - Quanto ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™œìš©í•œ linear quantization, linear quantizationì´ ì‹¤í–‰ë˜ëŠ” ì „ë°˜ì ì¸ íë¦„, Transformer ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™œìš©í•˜ì—¬ quantizationì˜ ë‹¤ë¥¸ í˜•íƒœì¸ downcasting ì ìš©í•´ë³´ê¸°
- ğŸ“œÂ [Sample-Efficient Human Evaluation of Large Language Models via Maximum Discrepancy Competition](https://arxiv.org/abs/2404.08008)
    - LLMì— ëŒ€í•œ ì‚¬ëŒì˜ í‰ê°€ê°€ ì¢€ ë” ì‰½ê³  ê°„í¸í•´ì§ˆ ìˆ˜ ìˆë„ë¡ MAximum Discrepeancy (MAD) competitionì„ ë„ì…. instructionì˜ subsetì„ samplingí•˜ê³  ë‘ ê°œì˜ LLMì— adaptí•˜ì—¬ ì–»ì€ ê²°ê³¼ì— ëŒ€í•´ win, tie, lose ì…‹ ì¤‘ í•˜ë‚˜ë¥¼ ê³ ë¥´ë„ë¡ í•˜ëŠ” ë°©ì‹
- ğŸ“œÂ [Tinkoff] [Learn Your Reference Model for Real Good Alignment](https://arxiv.org/abs/2404.09656)
    - í•™ìŠµ ì¤‘ì— reference policyë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” Trust Region DPO (TR-DPO) ë°©ì‹ì„ ì œì•ˆ
- ğŸ“œÂ [Google] [TransformerFAM: Feedback attention is working memory](https://arxiv.org/abs/2404.09173)
    - feedback loopë¥¼ ì´ìš©í•˜ì—¬ ë„¤íŠ¸ì›Œí¬ê°€ ìŠ¤ìŠ¤ë¡œì˜ latent representationì— attend í•  ìˆ˜ ìˆë„ë¡ ë§Œë“  Feedback Attention Memory(FAM)ë¥¼ ì œì•ˆ. ì´ë¡ ìƒ unlimited lengthì˜ sequenceë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ í•¨
- ğŸ“œÂ [Meta, CMU] [Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context Length](https://arxiv.org/abs/2404.08801)
    - exponential moving average with gated attentionì„ ì‚¬ìš©í•˜ëŠ” Mega ì•„í‚¤í…ì³ì—, complex exponential moving average (CEMA), timestep normalization layer, normalized attention mechanism, pre-norm with two-hop residual configurationì„ ë”í•œ ëª¨ë¸ì¸ Megalodon ëª¨ë¸ì„ ê³µê°œ
- ğŸ—ï¸Â [Google] [Gemma-1.1 version released](https://huggingface.co/google/gemma-1.1-7b-it)
    - was trained using a novel RLHF method
- ğŸ“œÂ [Cambridge, Michigan, Oxford, Stanford, etc] [Foundational Challenges in Assuring Alignment and Safety of Large Language Models](https://arxiv.org/abs/2404.09932)
    - LLMì„ alignment í•˜ê±°ë‚˜ safetyë¥¼ ë³´ì¥í•¨ì— ìˆì–´ì„œ 18ê°œì˜ ê·¼ë³¸ì ì¸ ë¬¸ì œì ì„ ë‹¤ë£¨ëŠ” ì„œë² ì´ í˜ì´í¼
- ğŸ“œÂ [UT Austin] [Pre-training Small Base LMs with Fewer Tokens](https://arxiv.org/abs/2404.08634)
    - í° ì–¸ì–´ ëª¨ë¸ì—ì„œ transformer ë¸”ë¡ì„ ê°€ì ¸ì™€ raw pretraining dataì˜ ì¼ë¶€ì— ì¶”ê°€ í•™ìŠµí•˜ëŠ” ë°©ì‹ì„ ì œì•ˆ. ì´ë¥¼ í†µí•´ ì ì€ ìì›ìœ¼ë¡œ ì‘ì€ ëª¨ë¸ì„ í•™ìŠµì‹œì¼œ ì¤€ìˆ˜í•œ ì„±ëŠ¥ì„ ë‚¼ ìˆ˜ ìˆìŒ
- ğŸ“œÂ [KAIST] [Self-Explore to Avoid the Pit: Improving the Reasoning Capabilities of Language Models with Fine-grained Rewards](https://arxiv.org/abs/2404.10346)
    - LLMì´ ìŠ¤ìŠ¤ë¡œ reasoning ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆë„ë¡, LLMì—ê²Œ ì˜ëª»ëœ ìŠ¤í…(first pit)ì„ ì œê³µí•˜ê³  ì´ë¥¼ ê°œì„ í•˜ê¸° ìœ„í•œ fine-grained rewardsë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ì‹ì¸ Self-Exploreë¥¼ ì œì•ˆ
- ğŸ§‘ğŸ»â€ğŸ’»Â [Upstage] [Evalverse: Revolutionizing Large Language Model Evaluation with a Unified, User-Friendly Framework](https://www.upstage.ai/feed/tech/evalverse-llm-evaluation-opensource)
    - ì„œë¸Œëª¨ë“ˆì„ í†µí•œ í†µí•© í‰ê°€, slackì„ í†µí•œ ì½”ë“œ ì—†ëŠ” í‰ê°€ ìš”ì²­, LLM í‰ê°€ ë³´ê³ ì„œ ì œì‘ ê¸°ëŠ¥
- ğŸ§‘ğŸ»â€ğŸ’»Â [Microsoft] [VASA-1: Lifelike Audio-Driven Talking FacesGenerated in Real Time](https://www.microsoft.com/en-us/research/project/vasa-1/)
    - Single image + Audio clip (1ë¶„) + (optional) Control signalsë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ 1ë¶„ ê¸¸ì´ì˜ ê³ í€„ë¦¬í‹° ë”¥í˜ì´í¬ ì˜ìƒì„ ìƒì„±. ì—„ì²­ë‚˜ê²Œ ìì—°ìŠ¤ëŸ¬ìš´ ì…ëª¨ì–‘ê³¼ í‘œì •.. ë‹¤ì–‘í•œ ë°ëª¨ ì˜ìƒì´ ì—…ë¡œë“œë˜ì–´ ìˆìŒ
- ğŸ§‘ğŸ»â€ğŸ’»Â [Meta] [Build the future of AI with Meta Llama 3](https://llama.meta.com/llama3/)
    - 8B, 70B ì‚¬ì´ì¦ˆì˜ pretrained & instruction-tuned versionì˜ Llama 3 ëª¨ë¸ì„ ê³µê°œ. 70B ëª¨ë¸ì˜ ê²½ìš° Gemini Pro 1.5ì™€ Claude 3 Sonnetì˜ ì„±ëŠ¥ì„ ìƒíšŒí•˜ëŠ” ìˆ˜ì¤€ì´ë¼ê³  í•¨.
- ğŸ§‘ğŸ»â€ğŸ’»Â [Google] [Tune in for Google I/O](https://io.google/2024/)
    - 2024ë…„ êµ¬ê¸€ I/Oê°€ 25ì¼ ë’¤ ì—´ë¦´ ì˜ˆì •. ì‚¬ì „ ë“±ë¡ì„ ë°›ê³  ìˆìŒ
- ğŸ§‘ğŸ»â€ğŸ’»Â [AI2] [OLMo 1.7â€“7B: A 24 point improvement on MMLU](https://blog.allenai.org/olmo-1-7-7b-a-24-point-improvement-on-mmlu-92b43f7d269d)
    - OLMo 1.0ì˜ ì—…ê·¸ë ˆì´ë“œ ë²„ì „ ëª¨ë¸ì„ ê³µê°œ. MMLUì—ì„œëŠ” Llama 2-7Bì„ ë„˜ì–´ì„œê³  Llama 2-13Bì— ì¤€í•˜ëŠ” ì„±ëŠ¥ì„, GSM8Kì—ì„œëŠ” Llama 2-13Bì„ ë„˜ì–´ì„œëŠ” ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤ê³  ì„¤ëª…í•¨. [í—ˆê¹…í˜ì´ìŠ¤ ëª¨ë¸ ì¹´ë“œ](https://huggingface.co/allenai/OLMo-1.7-7B)
- ğŸ§‘ğŸ»â€ğŸ’»Â [PyTorch] [torchtune](https://github.com/pytorch/torchtune)
    - PyTorchì˜ native ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ, LLM fine-tuning ë° ì‹¤í—˜ì„ í¸ë¦¬í•˜ê²Œ ë„ì™€ì¤Œ. í˜„ì¬ Llama3 ëª¨ë¸ë„ ì§€ì›í•¨.
- ğŸ“œÂ [Google DeepMind] [Many-Shot In-Context Learning](https://arxiv.org/abs/2404.11018)
    - human rationaleì„ modelì´ ìƒì„±í•œ CoT rationaleë¡œ ëŒ€ì²´í•˜ëŠ” Reinforced ICL, promptì—ì„œ rationaleì„ ì™„ì „íˆ ì§€ìš°ê³  domain-specific inputë§Œ í™œìš©í•˜ë„ë¡ í•˜ëŠ” Unsupervised ICL, ë‘ ë°©ë²•ë¡ ì„ ì œì•ˆ
- ğŸ“œÂ [Microsoft Research] [Position Engineering: Boosting Large Language Models through Positional Information Manipulation](https://arxiv.org/abs/2404.11216)
    - prompt engineeringê³¼ ë‹¬ë¦¬ í”„ë¡¬í”„íŠ¸ ë‚´ í…ìŠ¤íŠ¸ë¥¼ ë³€ê²½í•˜ì§€ ì•Šê³  ìˆœì„œ ì •ë³´ë§Œ ë³€ê²½í•˜ëŠ” ë°©ì‹ì¸ position engineeringì„ ì œì‹œ
- ğŸ“œÂ [Tencent AI] [Toward Self-Improvement of LLMs via Imagination, Searching, and Criticizing](https://arxiv.org/abs/2404.12253)
    - Monte Carlo Tree Search(MCTS)ë¥¼ LLMê³¼ ê²°í•©í•˜ì—¬ self-improving loopë¥¼ êµ¬ì¶•í•œ AlphaLLMì„ ê³µê°œ. Imagination, Searching, Criticizing, ì„¸ ë‹¨ê³„ë¡œ loopê°€ êµ¬ì„±ë¨
- ğŸ—ï¸Â [Meta adds its AI chatbot, powered by Llama 3, to the search bar across its apps](https://techcrunch.com/2024/04/18/meta-adds-its-ai-chatbot-powered-by-llama-3-to-the-search-bar-across-its-apps/?utm_source=www.theaivalley.com&utm_medium=newsletter&utm_campaign=meta-ai-vs-chatgpt-begins-now)
    - ë©”íƒ€ê°€ ë„¤ ê°œì˜ ì£¼ìš” ì•±(Facebook, Messenger, Instagram, WhatsApp)ì˜ ê²€ìƒ‰ ì°½ì— Llama 3 ê¸°ë°˜ ì±—ë´‡ ëª¨ë¸ì„ íƒ‘ì¬í•¨. ì´ë¥¼ OpenAIì™€ì˜ ê²½ìŸ êµ¬ë„ë¡œ í•´ì„í•˜ëŠ” ë“¯í•¨.
- ğŸ“œÂ [CMU, Meta AI] [TriForce: Lossless Acceleration of Long Sequence Generation with Hierarchical Speculative Decoding](https://arxiv.org/abs/2404.11912)
    - auto-regressive LLMì´ ëª¨ë“  KV cacheë¥¼ í•œ ë²ˆì— loadí•´ì•¼ í•œë‹¤ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, dynamic sparse KV cacheë¥¼ retrieveí•˜ëŠ” ë°©ì‹ì„ ê³ ì•ˆ.
- ğŸ§‘ğŸ»â€ğŸ’»Â [OpenAI] [Introducing OpenAI Japan](https://openai.com/blog/introducing-openai-japan)
    - ì¼ë³¸ì–´ì— íŠ¹í™”ëœ GPT-4 ì»¤ìŠ¤í…€ ëª¨ë¸ì„ ê³µê°œ. ì•„ì‹œì•„ ë‚´ ìµœì´ˆ ì§€ì‚¬ë¡œ ë„ì¿„ ì§€ì—­ì„ ì„ íƒ.
</details>

<details>
  <summary>4th week</summary>

- ğŸ§‘ğŸ»â€ğŸ’»Â [HuggingFace] [FineWeb](https://huggingface.co/datasets/HuggingFaceFW/fineweb)
    - í—ˆê¹…í˜ì´ìŠ¤ì—ì„œ ì˜¤í”ˆì†ŒìŠ¤ë¡œ ê³µê°œí•œ 15T ê°œ í† í°ìœ¼ë¡œ êµ¬ì„±ëœ í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹. ODC-By 1.0 licenseì˜ ì €ì‘ê¶Œ(ìƒì—…ì ìœ¼ë¡œë„ ììœ ë¡­ê²Œ ì´ìš© ê°€ëŠ¥). 45TB ì˜ ì €ì¥ ê³µê°„ì„ í•„ìš”ë¡œ í•˜ë©° 223ì–µí–‰ìœ¼ë¡œ êµ¬ì„±ë¨..
- ğŸ“œÂ [Epoch AI] [Chinchilla Scaling: A replication attempt](https://arxiv.org/abs/2404.10102)
    - Chinchillaì—ì„œ ë°í˜”ë˜ scaling lawê°€ íƒ€ë‹¹í•œ ê²ƒì¸ì§€ ì‹¤í—˜ì„ í†µí•´ ì¬í˜„í•œ ë…¼ë¬¸. ë‹¹ì‹œ ì œì•ˆë˜ì—ˆë˜ ì„¸ ê°œì˜ ë°©ë²•ë¡  ì¤‘ ë‘ ê°œëŠ” ìœ íš¨í•˜ì§€ ì•Šìœ¼ë©° ì„¸ ë²ˆì§¸ ë°©ë²•ë¡ ì€ íƒ€ë‹¹í•œ ê²ƒìœ¼ë¡œ í™•ì¸ë˜ì—ˆë‹¤ê³  ì£¼ì¥í•¨
- ğŸ“œÂ [State Space Model for New-Generation Network Alternative to Transformers: A Survey](https://arxiv.org/abs/2404.09516)
    - State Space Model (SSM) ì„œë² ì´ í˜ì´í¼
- ğŸ“œÂ [Stanford] [How faithful are RAG models? Quantifying the tug-of-war between RAG and LLMs' internal prior](https://arxiv.org/abs/2404.10198)
    - LLMì˜ internal knowledgeì™€ retrieved information ê°„ì˜ ê´€ê³„ì— ëŒ€í•œ ì—°êµ¬. LLMì´ ë‚®ì€ ì‚¬ì „í™•ë¥ ì„ ê°–ëŠ” internal knowledgeì— ëŒ€í•´ì„œ retrieved informationì— perturbation(modification)ì„ ê°€í•˜ëŠ” ê²½ìš° ë” ì‰½ê²Œ ì˜í–¥ì„ ë°›ìŒì„ í™•ì¸ (ë°˜ëŒ€ëŠ” ì˜í–¥ì„ ëœ ë°›ìŒ, robust)
- ğŸ“œ [Stanford] [2024 AI Index Report](https://aiindex.stanford.edu/report/)
    - 500í˜ì´ì§€ ë¶„ëŸ‰ì— ë‹¬í•˜ëŠ” ìŠ¤íƒ í¬ë“œ AI ë³´ê³ ì„œ. ìŠ¤íƒ í¬ë“œê°€ ê¼½ì€ ì£¼ëª©í•´ì•¼ í•  50ê°œ ëª¨ë¸ ì¤‘ í•œêµ­ì–´ ëª¨ë¸ì€ ì—†ë‹¤ê³  í•œë‹¤.
- ğŸ“œÂ [Fudan University] [AutoCrawler: A Progressive Understanding Web Agent for Web Crawler Generation](https://arxiv.org/abs/2404.12753)
    - LLMì„ í¬ë¡¤ëŸ¬ì™€ ê²°í•©í•˜ì—¬ í¬ë¡¤ëŸ¬ê°€ ë‹¤ì–‘í•˜ë©´ì„œë„ ë³€í™”í•˜ê³  ìˆëŠ” ì›¹ í™˜ê²½ì„ ì˜ ë‹¤ë£° ìˆ˜ ìˆë„ë¡ ë•ëŠ” AutoCrawlerë¥¼ ì œì•ˆ. HTMLì˜ hierarchical êµ¬ì¡°ë¥¼ í™œìš©í•œ two-stage í”„ë ˆì„ì›Œí¬
- ğŸ“œÂ [Towards Logically Consistent Language Models via Probabilistic Reasoning](https://arxiv.org/abs/2404.12843)
    - LLMì„ factsì™€ rule í˜•íƒœì˜ ì™¸ë¶€ ì§€ì‹ì— consistentí•  ìˆ˜ ìˆë„ë¡ ê°€ë¥´ì¹˜ëŠ” fine-tuning ê¸°ë²•. ì €ìë“¤ì´ ê³ ì•ˆí•œ lossë¥¼ ì œí•œëœ ì–‘ì˜ fact í•™ìŠµì— ì‚¬ìš©í•¨ìœ¼ë¡œì¨ extrapolate ëŠ¥ë ¥ì„ í–¥ìƒ. ICLR 2024 Workshop paper.
- ğŸ“œÂ [Nanyang Technological University] [Relevant or Random: Can LLMs Truly Perform Analogical Reasoning?](https://arxiv.org/abs/2404.12728)
    - LLMì—ê²Œ analogical reasoning ëŠ¥ë ¥ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ëŠ” ì—°êµ¬. ë¬´ê´€í•œ ì˜ˆì‹œë¡œë¶€í„° ê´€ë ¨ ìˆëŠ” ì˜ˆì‹œë¥¼ LLMì´ ìŠ¤ìŠ¤ë¡œ ë– ì˜¬ë¦¬ê³  í™œìš©í•˜ëŠ” self-generated ë°©ì‹ì„ ì´ìš©í•˜ë©´ ì‹¤ì œë¡œ ì¶”ë¡  ì •í™•ë„ê°€ í–¥ìƒë˜ëŠ” ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŒ.
- ğŸ§‘ğŸ»â€ğŸ’»Â [DeepLearning.AI] [Getting Started with Mistral](https://www.deeplearning.ai/short-courses/getting-started-with-mistral/)
    - APIë¥¼ ì´ìš©í•˜ì—¬ Mistral ëª¨ë¸ì— ì ‘ê·¼í•˜ê³  í”„ë¡¬í”„íŒ… í•˜ëŠ” ë°©ë²•, Mistralì˜ native function calling, RAG ì‹œìŠ¤í…œ êµ¬ì¶•, chat interface êµ¬ì¶• ë“±ì— ëŒ€í•œ short course
- ğŸ§‘ğŸ»â€ğŸ’»Â <Cookbook> [Efficiently fine-tune Llama 3 with PyTorch FSDP and Q-Lora](https://www.philschmid.de/fsdp-qlora-llama3)
    - FSDPì™€ Q-LoRAë¥¼ í™œìš©í•˜ì—¬ Llama 3ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ fine-tuningí•˜ëŠ” ë°©ë²•ì„ ì•Œë ¤ì£¼ëŠ” íŠœí† ë¦¬ì–¼. ì§§ê³  ê°„ê²°í•˜ê²Œ ì‘ì„±ë˜ì–´ ìˆìŒ
- ğŸ“œÂ [Microsoft] [Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone](https://arxiv.org/abs/2404.14219)
    - 3.8B ì‚¬ì´ì¦ˆì˜ phi-3-mini ëª¨ë¸ì„ ê³µê°œ. ì‘ì€ ì‚¬ì´ì¦ˆì„ì—ë„ ë¶ˆêµ¬í•˜ê³  Mixtral 8x7B, GPT-3.5ì— ì¤€í•˜ëŠ” ì„±ëŠ¥ì„ ë³´ì„. ì´ëŠ” phi-2ë¥¼ í•™ìŠµí•  ë•Œ ì‚¬ìš©í–ˆë˜ ë°ì´í„°ì…‹ì˜ scaled-up versionì„ ì‚¬ìš©í•œ ë•ë¶„ì„. ë˜í•œ phi-3-small (7B), phi-3-medium (14B)ë¥¼ ê³µê°œ.
- ğŸ§‘ğŸ»â€ğŸ’»Â [Adobe] [Generative AI in Premiere Pro powered by Adobe Firefly | Adobe Video](https://www.youtube.com/watch?v=6de4akFiNYM)
    - í”„ë¦¬ë¯¸ì–´ í”„ë¡œì— ì‚¬ìš©ë  AI ê¸°ìˆ ì„ ì„ ë³´ì„. ì¼ë¶€ ì˜ì—­ì„ ë“œë˜ê·¸ í•œ ë’¤ ìì—°ì–´ë¡œ ì˜ìƒ ì¼ë¶€ë¥¼ í¸ì§‘í•˜ëŠ” ë“±ì˜ ì‘ì—…ì´ ê°€ëŠ¥
- ğŸ“œÂ [OpenAI] [The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions](https://arxiv.org/abs/2404.13208)
    - instruction hierarchyë¼ëŠ” ê°œë…ì„ ë„ì…í•˜ì—¬ ëª¨ë¸ì´ instruction ì‚¬ì´ì— ìš°ì„ ìˆœìœ„ë¥¼ ì¸ì‹í•˜ë„ë¡ í•¨. ì´ë¥¼í…Œë©´ ìœ ì €ì˜ queryë³´ë‹¤ëŠ” system messageë¥¼ ìš°ì„  ë”°ë¥´ë„ë¡ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒ.
- ğŸ“œÂ [CMU] [TREACLE: Thrifty Reasoning via Context-Aware LLM and Prompt Selection](https://arxiv.org/abs/2404.13082)
    - ê°•í™”í•™ìŠµì—ì„œ ìœ ì €ì˜ ì¬ì •ì  ìƒí™©ê³¼ latency ì œì•½ì„ ê³ ë ¤í•˜ì—¬ ëª¨ë¸ê³¼ í”„ë¡¬í”„íŠ¸ë¥¼ ì„ ì •í•˜ëŠ” policyë¥¼ í•™ìŠµì‹œí‚¤ëŠ” TREACLE (Thrify Reasoning via Context-Aware LLM and Prompt Selection)ì„ ì œì•ˆ
- ğŸ“œÂ [Zhejiang University] [Information Re-Organization Improves Reasoning in Large Language Models](https://arxiv.org/abs/2404.13985)
    - contextë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ê²Œ ë˜ë©´ í”¼ìƒì ì¸ ì´í•´ë¥¼ ë°”íƒ•ìœ¼ë¡œ reasoningì„ ìˆ˜í–‰í•˜ê²Œ ë¨ â†’ ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ context ì •ë³´ë¥¼ re-organization í•˜ëŠ” InfoRE ë©”ì„œë“œë¥¼ ì œì•ˆ.
- ğŸ§‘ğŸ»â€ğŸ’»Â [vals.ai] [Benchmarks for Industry](https://www.vals.ai/)
    - LegalBench, ContractLaw, TaxEval, CorpFin ë²¤ì¹˜ë§ˆí¬ì˜ ë¦¬ë”ë³´ë“œë¥¼ ìš´ì˜. ì •í™•ë„, cost, latencyë¥¼ ë¹„êµ
- ğŸ“œÂ [Achieving >97% on GSM8K: Deeply Understanding the Problems Makes LLMs Perfect Reasoners](https://arxiv.org/abs/2404.14963)
    - Deeply Understanding the Problems (DUP) promptingì„ ì œì•ˆ. í•µì‹¬ ì§ˆë¬¸ì„ ì¶”ì¶œí•˜ê³ , í•µì‹¬ ì§ˆë¬¸ì— ê·¼ê±°í•œ problem-solving informationì„ ì°¾ì•„ë‚¸ ë’¤, ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ë„ë¡ í•¨
- ğŸ“œÂ [Tsinghua University] [Multi-Head Mixture-of-Experts](https://arxiv.org/pdf/2404.15045)
    - ê° í† í°ì„ ì—¬ëŸ¬ ê°œì˜ sub-tokensìœ¼ë¡œ ë‚˜ëˆ„ëŠ” multi-head ë©”ì»¤ë‹ˆì¦˜ì„ ì´ìš©. ì´ sub-tokensëŠ” ë‹¤ì–‘í•œ experts setì— ì˜í•´ ë³‘ë ¬ì ìœ¼ë¡œ ì²˜ë¦¬ë¨
- ğŸ“œÂ [Apple] [OpenELM: An Efficient Language Model Family with Open-source Training and Inference Framework](https://arxiv.org/pdf/2404.14619)
    - layer-wise scalingì„ ì ìš©í•˜ì—¬ ì •í™•ë„ í–¥ìƒì„ ì´ëŒì–´ë‚¸ OpenELMì„ ê³µê°œ. training, evaluation í”„ë ˆì„ì›Œí¬, publicly available datasets, pre-training configuration ë“±ì„ ì˜¨ì „íˆ ê³µê°œ.
- ğŸ—ï¸Â [The Ray-Ban Meta Smart Glasses have multimodal AI now](https://www.theverge.com/2024/4/23/24138090/ray-ban-meta-smart-glasses-ai-wearables)
    - ë©”íƒ€ê°€ Rayban glassesì— ì–¸ì–´ ë²ˆì—­, ì‚¬ë¬¼ ì¸ì‹, ì‚¬ì§„ ìº¡ì³ ë“±ì˜ ë©€í‹°ëª¨íƒˆ AIì˜ ëŠ¥ë ¥ì„ íƒ‘ì¬í•  ê²ƒì„ì„ ë°œí‘œ
- ğŸ“œÂ [Adobe] [Beyond Chain-of-Thought: A Survey of Chain-of-X Paradigms for LLMs](https://arxiv.org/abs/2404.15676)
    - Chain-of-X(CoX)ì— ê´€í•œ ë‹¤ì–‘í•œ ì—°êµ¬ë“¤ì„ ì •ë¦¬í•œ survey paper. 8 í˜ì´ì§€ ë¶„ëŸ‰ì˜ ì§§ì€ ì„œë² ì´.
- ğŸ“œÂ [Microsoft] [Towards Systematic Evaluation of Logical Reasoning Ability of Large Language Models](https://arxiv.org/abs/2404.15522)
    - LLMì˜ logical reasoning ëŠ¥ë ¥ì„ í‰ê°€í•˜ëŠ” ë²¤ì¹˜ë§ˆí¬ë“¤ì€ ì¼ë¶€ inference rules(ê¸ì • ë…¼ë²•, ëŒ€ìš° ë“±)ì— ì§‘ì¤‘í•  ë¿ì„ â†’ 25ê°œì˜ reasoning patternì„ ì•„ìš°ë¥´ëŠ” ë²¤ì¹˜ë§ˆí¬, LogicBenchë¥¼ ê³µê°œ
- ğŸ“œÂ [Meta] [LayerSkip: Enabling Early Exit Inference and Self-Speculative Decoding](https://arxiv.org/abs/2404.16710)
    - í•™ìŠµ ë™ì•ˆ layer dropoutì„ ì ìš©. ì´ë•Œ earlier layersëŠ” ë‚®ì€ ë¹„ìœ¨, later layersì— ëŒ€í•´ ë†’ì€ ë¹„ìœ¨ì„ ì ìš©. ë˜í•œ early exit lossë¥¼ ì‚¬ìš©. decoding ë‹¨ê³„ì—ì„œëŠ” early layersì—ì„œ exit í›„ ë‚¨ì€ layerë¥¼ verify and correctí•˜ëŠ” self-speculative decodingì„ ë„ì….
- ğŸ§‘ğŸ»â€ğŸ’»Â [PyTorch] [PyTorch 2.3 Release Blog](https://pytorch.org/blog/pytorch2-3/)
    - torch.compileì—ì„œ ìœ ì €ê°€ ì •ì˜í•˜ëŠ” triton kernelì„ ì§€ì›í•˜ì—¬ ì„±ëŠ¥ì„ í–¥ìƒ. tensor parallelismì„ ì§€ì›í•˜ì—¬ 1.6ë°° ë¹ ë¥¸ í–‰ë ¬ ì—°ì‚°ì´ ê°€ëŠ¥.
- ğŸ§‘ğŸ»â€ğŸ’»Â [Snowflake] [snowflake-arctic-instruct](https://huggingface.co/Snowflake/snowflake-arctic-instruct)
    - 128ê°œì˜ expertsë¥¼ í¬í•¨í•˜ëŠ” Dense-MoE Hybrid ì•„í‚¤í…ì³ë¥¼ í™œìš©í•œ 480B ì‚¬ì´ì¦ˆì˜ LLMì„ ê³µê°œ. 17B active parametersê°€ íŠ¹ì§•.
- ğŸ“œÂ [Peking, Microsoft] [Make Your LLM Fully Utilize the Context](https://arxiv.org/abs/2404.16811)
    - long-contextë¥¼ ì˜ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ INformation-INtensive (IN2) trainingì„ ì ìš©. long context ë‚´ì˜ short segmentì— ëŒ€í•œ fine-grained information awarenessì™€ ì—¬ëŸ¬ segmentsì˜ intergrationì„ ìš”í•˜ëŠ” íƒœìŠ¤í¬ë¡œ í•™ìŠµ.
- ğŸ—ï¸Â [China Unveils Vidu: A Powerful Text-to-Video Generator](https://www.maginative.com/article/china-unveils-vidu-a-powerful-text-to-video-generator/)
    - ì¤‘êµ­ì˜ Shengshu Technologyì™€ Tsinghua Universityì—ì„œ Soraì— ë²„ê¸ˆê°€ëŠ” text-to-video ëª¨ë¸, Viduë¥¼ ê³µê°œ
</details>


## ğŸ•ï¸ May
<details>
  <summary>1st week</summary>
  
- ğŸ“œÂ [UIUC, Cohere, Princeton] [SnapKV: LLM Knows What You are Looking for Before Generation](https://arxiv.org/abs/2404.14469)
    - input ê¸¸ì´ì— ë¹„ë¡€í•˜ì—¬ ì¦ê°€í•˜ëŠ” Key-Value (KV) cache ì‚¬ì´ì¦ˆì— ê´€ë ¨ëœ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ SnapKVë¥¼ ì œì•ˆ. ê° attention headì— ì¡´ì¬í•˜ëŠ” ì¤‘ìš”í•œ KV positionsë¥¼ ì„ ë³„í•¨ìœ¼ë¡œì¨ KV cacheë¥¼ ìë™ì ìœ¼ë¡œ compress.
- ğŸ“œÂ [Meta] [AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs](https://arxiv.org/abs/2404.16873)
    - adversarial promptë¥¼ ìë™ì ìœ¼ë¡œ ìƒì„±í•´ì£¼ëŠ” ê²ƒì€ ê·¸ ìì²´ë¡œ ì˜ë¯¸ê°€ ì—†ê³  í•™ìŠµì´ ë˜ì–´ì•¼ í•¨. ì´ë¥¼ ìœ„í•œ target llm, AdvPrompterë¥¼ ì œì‹œ. AdvPrompterì˜ ì˜ˆì¸¡ ê²°ê³¼ ìµœì í™” ë° low-rank fine-tuning.
- ğŸ§‘ğŸ»â€ğŸ’»Â [DeepLearning.AI] [Prompt Engineering for Vision Models](https://www.deeplearning.ai/short-courses/prompt-engineering-for-vision-models/)
    - textì™€ ì¢Œí‘œ, bounding boxë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ëŠ” ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” ë°©ë²•, diffusion model ë“±ì˜ ì´ë¯¸ì§€ ì»¨íŠ¸ë¡¤ ë°©ë²• ë“±ì— ëŒ€í•´ í•™ìŠµí•˜ëŠ” 1ì‹œê°„ ë¶„ëŸ‰ì˜ short course
- ğŸ§‘ğŸ»â€ğŸ’»Â [MIT, MyShell] [OpenVoice](https://github.com/myshell-ai/OpenVoice)
    - ì§§ì€ ì˜¤ë””ì˜¤ ìƒ˜í”Œë¡œë¶€í„° ëª©ì†Œë¦¬ë¥¼ ë³µì‚¬í•˜ì—¬ ì•„ì£¼ í˜„ì‹¤ì ì¸ speechë¥¼ ìƒì„±í•  ìˆ˜ ìˆëŠ” OpenVoice V2ë¥¼ ê³µê°œ
- ğŸ“œÂ [Cohere] [Replacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse Models](https://arxiv.org/abs/2404.18796)
    - GPT-4ì™€ ê°™ì€ í•œ ê°œì˜ LLMì„ í‰ê°€ìë¡œ í™œìš©í•˜ëŠ” ê²ƒë³´ë‹¤ ì—¬ëŸ¬ ê°œì˜ ì‘ì€ ëª¨ë¸ë“¤ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë” ì¢‹ì€ í‰ê°€ ê²°ê³¼ë¡œ ì´ì–´ì§„ë‹¤ëŠ” ê²ƒì— ê´€í•œ ì—°êµ¬
- ğŸ—ï¸Â [Mystery â€˜Gpt2-Chatbotâ€™ And Cryptic Sam Altman Tweet Fuel Speculation Over OpenAIâ€™s Next ChatGPT Update](https://www.forbes.com/sites/roberthart/2024/04/30/mystery-gpt2-chatbot-and-cryptic-sam-altman-tweet-fuel-speculation-over-openais-next-chatgpt-update/?sh=19ea4686384d)
    - LMSYS Chatbot Arenaì— ë“±ì¥í•œ gpt2-chatbotì´ OpenAIì˜ ìƒˆë¡œìš´ ëª¨ë¸ì¼ ê²ƒì´ë¼ëŠ” ì¶”ì¸¡.
- ğŸ“œÂ [Baidu] [HFT: Half Fine-Tuning for Large Language Models](https://arxiv.org/abs/2404.18466)
    - catastrophic forgetting ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ full fine-tuning (FFT) ëŒ€ì‹  Half Fine-Tuning (HFT) ë¥¼ ì œì•ˆ. íŒŒë¼ë¯¸í„°ì˜ ì ˆë°˜ì€ ìƒˆë¡œìš´ ì •ë³´ë¥¼ í•™ìŠµí•˜ê³ , ë‚˜ë¨¸ì§€ ì ˆë°˜ì€ frozen í•˜ëŠ” ë°©ì‹.
- ğŸ§‘ğŸ»â€ğŸ’»Â [Gradient] [LLama-3-8B-Instruct-Gradient-1048K](https://huggingface.co/gradientai/Llama-3-8B-Instruct-Gradient-1048k)
    - GradientAIì—ì„œ ì²˜ë¦¬ ê°€ëŠ¥í•œ context lengthê°€ 1Mì— ë‹¬í•˜ëŠ” instruct versionì˜ ë¼ë§ˆ ëª¨ë¸ì„ í—ˆê¹…í˜ì´ìŠ¤ì— ê³µê°œ. ìŠ¤í™ê³¼ ì˜ˆì‹œ ì½”ë“œê°€ í•¨ê»˜ ì œì‹œë˜ì–´ ìˆìŒ
- ğŸ“œÂ [Bozewn-Bolzano] [When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively](https://arxiv.org/abs/2404.19705)
    - parametric memoryë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ê¸° ì¶©ë¶„í•œ ê²½ìš°, Information Retrievalì„ í•˜ì§€ ì•Šê³  special token <RET>ë¥¼ ìƒì„±í•˜ë„ë¡ í•™ìŠµí•˜ëŠ” ë°©ì‹ì„ ì œì•ˆ
- ğŸ“œÂ [UC Berkeley] [Is Bigger Edit Batch Size Always Better? - An Empirical Study on Model Editing with Llama-3](https://arxiv.org/abs/2405.00664)
    - model editingì— ìˆì–´ì„œ  edit batch-sizeë¥¼ í‚¤ìš°ëŠ” ê²ƒì´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í•˜ë½ì‹œí‚¤ëŠ” ê²ƒì„ì„ í™•ì¸í•œ ì‹¤í—˜
- ğŸ“œÂ [Meta] [Better & Faster Large Language Models via Multi-token Prediction](https://arxiv.org/abs/2404.19737)
    - nê°œì˜ ë…ë¦½ì ì¸ headë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œ ë²ˆì— nê°œì˜ í† í°ì„ ì˜ˆì¸¡í•˜ë„ë¡ í•¨. ì†ë„ ë¿ë§Œ ì•„ë‹ˆë¼ ì„±ëŠ¥ì ìœ¼ë¡œë„ í–¥ìƒì´ ìˆì—ˆë‹¤ëŠ” ì‹¤í—˜ ê²°ê³¼ë¥¼ ê³µê°œ.
- ğŸ“œÂ [Hong Kong University] [Mixture of insighTful Experts (MoTE): The Synergy of Thought Chains and Expert Mixtures in Self-Alignment](https://arxiv.org/abs/2405.00557)
    - Question Analysis, Answer Guidance, Safe Answer productionìœ¼ë¡œ êµ¬ì„±ëœ AlignCoTë¥¼ ì œì•ˆ. ì¶”ê°€ë¡œ Mixture of insighTful Experts(MoTE)ë¥¼ ì œì•ˆ.
- ğŸ“œÂ [KAIST AI] [Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models](https://arxiv.org/abs/2405.01535)
    - 4ê°œì˜ direct assessmentì™€ 4ê°œì˜ pair-wise rankingì„ ì´ìš©í•˜ì—¬ LMì´ í‰ê°€í•œ ê²°ê³¼ì™€ ì‚¬ëŒì˜ í‰ê°€ ê²°ê³¼ë¥¼ ìµœëŒ€í•œ aligní•  ìˆ˜ ìˆë„ë¡ í•¨
- ğŸ“œÂ [Virginia]  [Context-Aware Clustering using Large Language Models](https://arxiv.org/abs/2405.00988)
    - CACTUS(Context-Aware ClusTering with aUgmented triplet losS)ë¥¼ ì œì•ˆ. supervised clusteringì„ ìœ„í•œ triplet loss functionì„ ì œì•ˆ. text augmentation ê¸°ë°˜ì˜ self-supervised clustering taskë¥¼ ë„ì…
- ğŸ§‘ğŸ»â€ğŸ’»Â [Anthropic] [Introducing the Claude Team plan and iOS app](https://www.anthropic.com/news/team-plan-and-ios)
    - Claude 3 model familyë¥¼ íŒ€ ìš”ê¸ˆì œë¡œ ì´ìš© ê°€ëŠ¥. ì›¹ì—ì„œì™€ ë˜‘ê°™ì´ ì´ìš© ê°€ëŠ¥í•œ ì„œë¹„ìŠ¤ë¥¼ iOSë¡œ ì œê³µ.
- ğŸ“œÂ [Predibase] [LoRA Land: 310 Fine-tuned LLMs that Rival GPT-4, A Technical Report](https://arxiv.org/abs/2405.00732)
    - 10ê°œ ëª¨ë¸ì„ 31ê°œ íƒœìŠ¤í¬ì— ëŒ€í•´ QLoRAë¡œ fine-tuningí•œ ì„±ëŠ¥ì„ ë¹„êµ. GPT-4ë¥¼ ëŠ¥ê°€í•˜ëŠ” ê²°ê³¼ë„ ìˆì—ˆìŒ. ëª¨ë¸ì˜ í•™ìŠµ ê²°ê³¼ë¥¼ ì˜ˆì¸¡í•  ìˆ˜ ìˆë„ë¡ í•¨(ì–´ë–¤ ìˆ˜ì¤€ê¹Œì§€ í•™ìŠµì´ ë ì§€). LoRAXì˜ latencyì™€ concurrencyë¥¼ í‰ê°€.
</details>

<details>
  <summary>2nd week</summary>

- ğŸ“œÂ [MIT] [KAN: Kolmogorov-Arnold Networks](https://arxiv.org/abs/2404.19756)
    - Multi-Layer Perceptrons(MLPs)ë¥¼ ëŒ€ì‹ í•˜ëŠ” Kolmogorov-Arnold Networks(KAN)ë¥¼ ì œì•ˆ. linear weightë¥¼ ì „í˜€ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë©° ê° weight íŒŒë¼ë¯¸í„°ëŠ” univariate functionìœ¼ë¡œ ëŒ€ì²´ë¨.
- ğŸ“œÂ [Imperial College London] [Argumentative Large Language Models for Explainable and Contestable Decision-Making](https://arxiv.org/abs/2405.02079)
    - reasoning ê³¼ì •ì—ì„œ argumentationì„ ìƒì„±í•˜ëŠ” í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆ. ì´ë¥¼ í†µí•´ LLMì˜ ì„ íƒê³¼ íŒë‹¨ì— ëŒ€í•œ ê·¼ê±°ë¥¼ ëª…í™•í•˜ê²Œ íŒŒì•…í•  ìˆ˜ ìˆìŒ.
- ğŸ—ï¸Â [X] [X launches Stories, delivering news summarized by Grok AI](https://techcrunch.com/2024/05/03/x-launches-stories-on-x-delivering-news-summarized-by-grok-ai/)
    - ê°œì¸ ë§ì¶¤í™”ëœ ì´ì•¼ê¸°ë“¤ì„ Grok AI ëª¨ë¸ì´ ìš”ì•½í•˜ì—¬ ì œì‹œí•˜ëŠ” ì„œë¹„ìŠ¤ë¥¼ ë„ì…. [X ë§í¬](https://twitter.com/XEng/status/1786463531505799186?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1786463531505799186%7Ctwgr%5E75c9d4c38ea3f1bfdab9931eb077437796f87eaf%7Ctwcon%5Es1_c10&ref_url=https%3A%2F%2Ftechcrunch.com%2F2024%2F05%2F03%2Fx-launches-stories-on-x-delivering-news-summarized-by-grok-ai%2F). news ì‚°ì—…ì— í° ì˜í–¥ì„ ì¤„ ê²ƒìœ¼ë¡œ ì˜ˆìƒë¨.
- ğŸ§‘ğŸ»â€ğŸ’»Â [DeepLearning.AI & HuggingFace] [Quantization In Depth](https://www.deeplearning.ai/short-courses/quantization-in-depth/)
    - ë‹¤ì–‘í•œ ì¢…ë¥˜ì˜ quantization ê¸°ë²•ì— ëŒ€í•´ ê³µë¶€í•˜ê³  weightë¥¼ packing í•˜ëŠ” ë°©ë²•ì„ ìŠµë“.
- ğŸ§‘ğŸ»â€ğŸ’»Â [Meta-Llama-3-120B-Instruct](https://huggingface.co/mlabonne/Meta-Llama-3-120B-Instruct)
    - â€œself-mergeâ€ë¥¼ ì´ìš©í•˜ì—¬ 70B ì‚¬ì´ì¦ˆì˜ ëª¨ë¸ì„ 120Bê¹Œì§€ scaling upí•˜ì—¬ ê³µê°œ. ìë£Œí˜•ì„ float16ìœ¼ë¡œ ìœ ì§€í•˜ì—¬ ì„±ëŠ¥ì„ ìµœì í™”í•  ìˆ˜ ìˆë„ë¡ â€œpassthroughâ€ ë¨¸ì§€ ê¸°ë²•ì„ ì´ìš©.
- ğŸ—ï¸Â [Nvidia] [Nvidia Launches ChatRTX Chatbot for RTX GPUs](https://www.extremetech.com/computing/nvidia-launches-chatrtx-chatbot-for-rtx-gpus)
    - ì†Œë¹„ìë“¤ì—ê²Œ â€˜AI on your PCâ€™ ê²½í—˜ì„ ì œê³µí•˜ê¸° ìœ„í•´ RTX GPUë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë™ì‘í•˜ëŠ” ChatRTX ì±—ë´‡ì„ ê³µê°œ. í™•ì‹¤íˆ on-device, local LLM ë“±ì— ëŒ€í•œ ê´€ì‹¬ì´ ëœ¨ê±°ì›€.
- ğŸ§‘ğŸ»â€ğŸ’»Â [LMSYS] [gpt2-chatbot is Back Online](https://chat.lmsys.org/)
    - ì±—ë´‡ì•„ë ˆë‚˜ì—ì„œ gpt-2-chatbot ëª¨ë¸ì´ ë‹¤ì‹œ ë“±ì¥. ëª¨ë¸ì„ ì„ íƒí•  ìˆ˜ëŠ” ì—†ì§€ë§Œ í”„ë¡¬í”„íŠ¸ ì…ë ¥ í›„ ê²°ê³¼ë¥¼ í™•ì¸í•´ë³´ë©´ í•´ë‹¹ ëª¨ë¸ê³¼ì˜ ë¹„êµê°€ ì´ë¤„ì§€ê³  ìˆìŒì´ í™•ì¸ë¨.
- ğŸ§‘ğŸ»â€ğŸ’»Â [DeepSeek-AI] [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](https://github.com/deepseek-ai/DeepSeek-V2?tab=readme-ov-file)
    - 236B ì‚¬ì´ì¦ˆì˜ Mixture-of-Experts (MoE) ê¸°ë°˜ LLMì„ ê³µê°œ. activated parametersëŠ” 21B ìˆ˜ì¤€. í•™ìŠµ ë° ì¶”ë¡  ë‘˜ ë‹¤ êµ‰ì¥íˆ íš¨ìœ¨ì ì„ì„ ê°•ì¡°.
- ğŸ§‘ğŸ»â€ğŸ’»Â [DeepLearning.AI] [Building Agentic RAG with LlamaIndex](https://www.deeplearning.ai/short-courses/building-agentic-rag-with-llamaindex/)
    - ì£¼ì–´ì§„ ë¬¸ì„œë¥¼ ì´í•´í•˜ê³  ë³µì¡í•œ ì§ˆë¬¸ì— ë‹µë³€í•˜ë„ë¡ í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ í•™ìŠµ. íŠ¹íˆ ì—¬ëŸ¬ ê°œì˜ ë¬¸ì„œë¥¼ ë‹¤ë£¨ê±°ë‚˜ agentë¥¼ debug í•˜ëŠ” ë°©ë²• ë“±ì— ëŒ€í•´ì„œë„ í•™ìŠµ. ê°•ì˜ ë¶„ëŸ‰ì€ ê·¸ë ‡ê²Œ ë§ì§€ ì•Šì•„ ë³´ì„.
- ğŸ“œÂ [xLSTM: Extended Long Short-Term Memory](https://arxiv.org/abs/2405.04517)
    - exponential gatingì„ ë„ì…, LSTM ë©”ëª¨ë¦¬ êµ¬ì¡°ë¥¼ ë³€í˜•í•œ sLSTMê³¼ mLSTMì„ í†µí•©. ì´ ë‘˜ì„ í†µí•´ Transformersì™€ State Space Modelsì— ì¤€í•˜ëŠ” ì„±ëŠ¥ê³¼ scaling ê°€ëŠ¥ì„±ì„ ë³´ì—¬ì¤Œ.
- ğŸ“œÂ [MIT] [Co-design for Efficient LLM Serving](https://arxiv.org/abs/2405.04532)
    - í˜„ì¡´í•˜ëŠ” INT4 quantization ë°©ë²•ë¡ ì— ë‚˜íƒ€ë‚˜ëŠ” overhead ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ 4-bit weight, 8-bit activation, 4-bit KV cacheë¥¼ ì‚¬ìš©í•˜ëŠ” W4A8KV4, QoQ(quattuor-octo-quattuor)ë¥¼ ë„ì…
- ğŸ§‘ğŸ»â€ğŸ’»Â [Google] [Meet Pixel 8a: The Google AI phone at an unbeatable value](https://blog.google/products/pixel/pixel-8a-launch/)
    - Geminië¥¼ íƒ‘ì¬í•œ ìŠ¤ë§ˆíŠ¸í° Pixel 8, Pixel 8 Proë¥¼ ì¶œì‹œ. ì¹´ë©”ë¼ì˜ group shot, magic editor, ìŒì„±ì˜ audio magic eraser ë“±ì˜ ê¸°ëŠ¥ì„ íƒ‘ì¬
- ğŸ“œÂ [University of Texas] [Mitigating Exaggerated Safety in Large Language Models](https://arxiv.org/abs/2405.05418)
    - LLMì´ ìœ ì €ì˜ ì§ˆë¬¸ì„ harmfulí•œ ê²ƒìœ¼ë¡œ íŒë‹¨í•˜ê³  ê±°ì ˆí•˜ëŠ” ì¼€ì´ìŠ¤ ì¤‘ ì‹¤ì œë¡œ harmful í•˜ì§€ ì•Šì€ ê²ƒì„ â€˜ê³¼ì¥ëœ(exaggerated)â€™ ê²½ìš°ë¼ê³  í‘œí˜„. ì´ëŸ¬í•œ í˜„ìƒì„ ì™„í™”í•˜ê¸° ìœ„í•œ í”„ë¡¬í”„íŒ… ê¸°ë²•ì„ ì œì‹œí•¨ê³¼ ë™ì‹œì— ì´ëŸ¬í•œ í˜•ìƒì´ ì¡´ì¬í•¨ì„ í™•ì¸í•  ìˆ˜ ìˆëŠ” ë°ì´í„°ì…‹ì„ ì œì‹œ.
- ğŸ“œÂ [Google Research] [Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?](https://arxiv.org/abs/2405.05904)
    - LLMì´ ê¸°ì¡´ ì§€ì‹ê³¼ ê´€ë ¨ ì—†ëŠ” ë‚´ìš©ë“¤ì— ëŒ€í•´ ì¼ìœ¼í‚¤ëŠ” hallucination ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ controlled setupì„ ì„¤ê³„. closed-book QA í™˜ê²½ì—ì„œ ì‹¤í—˜í•œ ê²°ê³¼, fine-tuningì„ í†µí•´ ìƒˆë¡œìš´ ì§€ì‹ì„ ì£¼ì…í•˜ëŠ” ë°©ì‹ì˜ ìœ„í—˜ì„±ì„ ì…ì¦.
      
</details>

<details>
  <summary>3rd week</summary>

- ğŸ§‘ğŸ»â€ğŸ’»Â [Anthropic] [Prompt Generator](https://docs.anthropic.com/en/docs/prompt-generator)
    - íƒœìŠ¤í¬ì— ëŒ€í•œ ê°„ë‹¨í•œ ì„¤ëª…ì„ ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ìœ¼ë¡œ ë³€í™˜í•´ì£¼ëŠ” metapromptë¥¼ ê³µê°œ
- ğŸ§‘ğŸ»â€ğŸ’»Â [IBM] [Granite Code Models: A Family of Open Foundation Models for Code Intelligence](https://github.com/ibm-granite/granite-code-models)
    - 116ê°œ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë¡œ í•™ìŠµí•œ 3Bì—ì„œ 34Bì— ì´ë¥´ëŠ” 8ê°œì˜ ì½”ë“œ ëª¨ë¸ì„ ê³µê°œ. ì½”ë“œ ê´€ë ¨ íƒœìŠ¤í¬ì—ì„œ CodeGemmaë‚˜ Mistralì„ ëŠ¥ê°€í•˜ëŠ” ì„±ëŠ¥ì„ ë³´ì„
    - ë…¼ë¬¸ ë§í¬: https://arxiv.org/abs/2405.04324
- ğŸ§‘ğŸ»â€ğŸ’»Â [OpenAI] [Hello GPT-4o](https://openai.com/index/hello-gpt-4o/)
    - audio, vision, textë¥¼ real timeìœ¼ë¡œ ì²˜ë¦¬ ê°€ëŠ¥í•œ í”Œë˜ê·¸ì‹­ ëª¨ë¸ì„ ê³µê°œ. â€˜oâ€™ëŠ” ëª¨ë‘ë¥¼ ëœ»í•˜ëŠ” â€˜omniâ€™ì˜ ì•½ì. ì‚¬ëŒì˜ ê°ì •ì„ ì¶©ë¶„íˆ ì´í•´í•˜ëŠ” ë“¯í•œ ë°˜ì‘, ë‹¤ì–‘í•œ ìŒì„± ë³€ì£¼, ì¤‘ê°„ì— ë§ì„ ëŠì–´ë„ ì´í•´ê°€ ê°€ëŠ¥í•œ ì‹¤ì‹œê°„ ëŒ€í™” ì–‘ìƒ ë“± ì¶©ê²©ì ì¸ ë°ëª¨ë¥¼ ê³µê°œ.
    - ê°œì¸ì ì¸ êµìœ¡ ë¶„ì•¼ì—ì„œ íŠ¹íˆ í™œìš© ì—¬ì§€ê°€ ë§ì´ ì»¤ì§„ ê²ƒ ê°™ë‹¤ê³  ëŠë‚Œ.
    - [ìœ íŠœë¸Œì— ê³µê°œëœ ë°ëª¨ ë§í¬](https://www.youtube.com/watch?v=DQacCB9tDaw&t=3986s)
- ğŸ“œÂ [Baidu] [A Survey on RAG Meets LLMs: Towards Retrieval-Augmented Large Language Models](https://arxiv.org/abs/2405.06211)
    - RAGëŠ” ìƒì„±í˜• AIê°€ ì§€ë‹Œ ê¸°ì¡´ ì§€ì‹ì— ìƒˆë¡œìš´ ì§€ì‹ì„ ë”í•´ì¤„ ìˆ˜ ìˆëŠ” ë°©ì‹ì„. Retrieval-Augmented Large Language Models(RA-LLMs)ë¥¼ architecture, training strategies, applications, ì„¸ ê´€ì ì—ì„œ ì„œë² ì´í•œ í˜ì´í¼.
- ğŸ§‘ğŸ»â€ğŸ’»Â [TII] [Falcon 2](https://huggingface.co/tiiuae/falcon-11B)
    - 5,000B í† í°ì˜ RefinedWebìœ¼ë¡œ í•™ìŠµëœ 11B LLM. fine-tuned ë˜ì§€ ì•Šì€ raw ëª¨ë¸ì„ í—ˆê¹…í˜ì´ìŠ¤ì— ê³µê°œ.
- ğŸ“œÂ [Cohere] [Fishing for Magikarp: Automatically Detecting Under-trained Tokens in Large Language Models](https://arxiv.org/abs/2405.05417)
    - tokenizerì— í¬í•¨ëœ í† í° ì¤‘ì—ì„œ ì œëŒ€ë¡œ í•™ìŠµì´ ë˜ì§€ ì•Šì€ â€˜glitch tokensâ€™ê°€ ì¡´ì¬í•¨.
    - â€˜tokenizer analysis, model weight-based indicators, prompting techniquesâ€™ì˜ ì¡°í•©ì„ ì´ìš©í•˜ì—¬ ìœ„ì™€ ê°™ì€ problematic tokensë¥¼ ìë™ì ìœ¼ë¡œ detect í•˜ëŠ” ë°©ë²•ë¡ ì„ ì œì•ˆ.
- ğŸ§‘ğŸ»â€ğŸ’»Â [Google] [Google I/O 2024: An I/O for a new generation](https://blog.google/inside-google/message-ceo/google-io-2024-keynote-sundar-pichai/)
    - Gemini 1.5 Proì˜ context windowê°€ 2Mê¹Œì§€ ì¦ê°€. ê·¸ëŸ¬ë‚˜ 128K ì´í•˜ì— ëŒ€í•´ì„œëŠ” ê°€ê²©ì„ 50% ë‚®ì¶¤ (GPT-4o ëŒ€ë¹„ 30% ì €ë ´)
    - Geminië¥¼ êµ¬ê¸€ ì œí’ˆ(í¬í† , ì´ë¯¸ì§€ ê²€ìƒ‰, ì›Œí¬ ìŠ¤í˜ì´ìŠ¤, ì´ë©”ì¼ ë“±)ì— í†µí•©í•˜ê² ë‹¤ê³  ë°œí‘œ. (ë¼ì´ë¸Œ ë°ëª¨ x, ì—¬ë¦„ ë˜ëŠ” ì˜¬í•´ ë§ ì¶œì‹œ ì˜ˆì • ????)
    - GPT-4oì™€ ë§ˆì°¬ê°€ì§€ë¡œ multimodalityë¥¼ ê°•ì¡°. ê·¸ëŸ¬ë‚˜ ê·¸ë§Œí¼ì˜ ì„íŒ©íŠ¸ê°€ ìˆì§€ëŠ” ì•ŠìŒ.
- ğŸ§‘ğŸ»â€ğŸ’»Â [Salesforce] [SFR-Iterative-DPO-LLaMA-8B-R](https://huggingface.co/Salesforce/SFR-Iterative-DPO-LLaMA-3-8B-R)
    - Alpaca-Eval-V2, MT-Bench, Chat-Arena-Hard, ì„¸ ê°œì˜ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ì‘ì€ ì‚¬ì´ì¦ˆì˜ ëª¨ë¸ ì¤‘ ìµœê³  ì„±ëŠ¥ì„ ë‹¬ì„±. human-/GPT4-labeling ì—†ëŠ” open-sourced ë°ì´í„°ì…‹ìœ¼ë¡œ í•™ìŠµëœ ëª¨ë¸.
- ğŸ“œÂ [HuggingFace] [What matters when building vision-language models?](https://arxiv.org/abs/2405.02246)
    - vision-language models(VLMs)ì˜ í•™ìŠµ ë°©ì‹ì— ëŒ€í•´ì„œëŠ” ì•„ì§ ìë¦¬ì¡ì€ ê²ƒì´ ì—†ìŒ â†’ ì•„í‚¤í…ì³, ë°ì´í„°, í•™ìŠµ ë°©ì‹ ë“± ë‹¤ì–‘í•œ ì‹¤í—˜ì„ í†µí•´ ë§Œë“  8B ì‚¬ì´ì¦ˆì˜ VLM, Idefics2ë¥¼ ê³µê°œ. base, instructed, chat, ì„¸ ê°œ ë²„ì „ì˜ ëª¨ë¸ì„ í•™ìŠµ ë°ì´í„°ì…‹ê³¼ í•¨ê»˜ ê³µê°œ.
- ğŸ“œÂ [Salesforce, UIUC] [RLHF Workflow: From Reward Modeling to Online RLHF](https://arxiv.org/abs/2405.07863)
    - Reinforcement Learning from Human Feedback(RLHF)ì€ offline learning settingì—ì„œë§Œ ì‚¬ìš© ê°€ëŠ¥í•˜ë‹¤ëŠ” ë‹¨ì ì´ ì¡´ì¬ â†’ ë‹¤ì–‘í•œ ì˜¤í”ˆ ì†ŒìŠ¤ ë°ì´í„°ì…‹ê³¼ ì‚¬ì „ì— êµ¬ì¶•ëœ proxy preference modelì„ ì‚¬ìš©í•¨ìœ¼ë¡œì¨ preference modelì„ êµ¬ì¶•. ì´ë¥¼ ì´ìš©í•˜ì—¬ Online Iterative RLHFë¥¼ ìˆ˜í–‰.
- ğŸ“œÂ [Hwawei] [Beyond Scaling Laws: Understanding Transformer Performance with Associative Memory](https://arxiv.org/abs/2405.08707)
    - Transformer ê¸°ë°˜ì˜ ëª¨ë¸ë“¤ì˜ ì‚¬ì´ì¦ˆë¥¼ í‚¤ìš°ë©´ ì„±ëŠ¥ì´ ì¦ê°€í•œë‹¤ëŠ” scaling lawê°€ ë°˜ë“œì‹œ ì§€ì¼œì§€ëŠ” ê²ƒì€ ì•„ë‹˜ â†’ Hopfield ë„¤íŠ¸ì›Œí¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¡ ì  í”„ë ˆì„ì›Œí¬ë¥¼ ì œì‹œ. attention mechanismì— ëŒ€í•œ ì„¤ëª…ì´ ê°€ëŠ¥í•´ì§.
- ğŸ§‘ğŸ»â€ğŸ’»Â [DeepLearning.AI] [Multi AI Agent Systems with crewAI](https://www.deeplearning.ai/short-courses/multi-ai-agent-systems-with-crewai/)
    - multi agent ê´€ë ¨ ê°•ì˜. ì˜¤í”ˆì†ŒìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬ crewAIë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹„ì§€ë‹ˆìŠ¤ ìë™í™”ì— ê´€í•œ ë‚´ìš©ì„ í•™ìŠµ.
- ğŸ§‘ğŸ»â€ğŸ’»Â [OpenAI] [Improvements to data analysis in ChatGPT](https://openai.com/index/improvements-to-data-analysis-in-chatgpt/)
    - Google Driveì™€ Microsoft OneDriveë¡œë¶€í„° ì§ì ‘ í…Œì´ë¸”ê³¼ ì°¨íŠ¸ë¥¼ ì½ê³  ìƒí˜¸ì‘ìš©í•  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ì„ ê³µê°œ.
    - ì°¨ì£¼ë¶€í„° ChatGPT Plus, Team, Enterprise ìœ ì €ë“¤ì—ê²Œ ê³µê°œ.
- ğŸ“œÂ [University of Waterloo] [UniRAG: Universal Retrieval Augmentation for Multi-Modal Large Language Models](https://arxiv.org/abs/2405.10311)
    - Multi-Modal(MM) Large Language Models(LLMs)ì— í•„ìš”í•œ MM understandingì„ ê°•í™”í•˜ê¸° ìœ„í•´ ì¶”ë¡  ë‹¨ê³„ì—ì„œ few-shot examplesë¥¼ ì œê³µí•˜ëŠ” ë°©ë²•ë¡ ì„ ì œì•ˆ.
- ğŸ—ï¸Â [OpenAI & Reddit] [OpenAI strikes Reddit deal to train its AI on your posts](https://www.theverge.com/2024/5/16/24158529/reddit-openai-chatgpt-api-access-advertising)
    - Redditì˜ data APIë¡œë¶€í„° ì‹¤ì‹œê°„ ì»¨í…ì¸ ë¥¼ í™•ì¸í•  ìˆ˜ ìˆëŠ” ê³„ì•½ì„ ì²´ê²°. ì—°ì´ˆ Googleì´ Redditê³¼ ë§ºì€ ê³„ì•½ ê·œëª¨ëŠ” ì•½ $60M(í•œí™” ì•½ 8ë°±ì–µ)ì— ì´ë¥´ëŠ” ê²ƒìœ¼ë¡œ ì•Œë ¤ì§.
- ğŸ“œÂ [Columbia University] [LoRA Learns Less and Forgets Less](https://arxiv.org/pdf/2405.09673)
    - programmingê³¼ mathematics ë„ë©”ì¸ì—ì„œ LoRAì™€ full finetuningì„ ë¹„êµ. ë˜í•œ instruction finetuningê³¼ continued pretrainingì„ ë¹„êµ â†’ LoRAëŠ” full finetuning ëŒ€ë¹„ ì„±ëŠ¥ í–¥ìƒ í­ì€ ì‘ì§€ë§Œ, ê¸°ì¡´ì˜ ì§€ì‹ì„ ë” ì˜ ë³´ì¡´í•˜ëŠ” ê²½í–¥ì„ ë³´ì„.
- ğŸ§‘ğŸ»â€ğŸ’»Â [HuggingFace] [Hugging Face x LangChain : A new partner package in LangChain](https://huggingface.co/blog/langchain)
    - í—ˆê¹…í˜ì´ìŠ¤ì— ì—…ë¡œë“œëœ ëª¨ë¸ë“¤ì„ LangChainì„ í†µí•´ í™œìš© ê°€ëŠ¥í•˜ë„ë¡ ì—…ë°ì´íŠ¸í•œ ë‚´ì—­ì„ ê³µê°œ.
- ğŸ§‘ğŸ»â€ğŸ’»Â [TIGER-Lab] [MMLU-Pro](https://huggingface.co/datasets/TIGER-Lab/MMLU-Pro)
    - 12K ê°œì˜ ë³µì¡í•œ ì§ˆë¬¸ìœ¼ë¡œ êµ¬ì„±ëœ MMLU ì—…ê·¸ë ˆì´ë“œ ë²„ì „. ì„ íƒì§€ë¥¼ 4ê°œì—ì„œ 10ê°œë¡œ ëŠ˜ë¦¼. ë˜í•œ reasoning-focused problemsì— ì§‘ì¤‘.
- ğŸ“œÂ [MIT] [The Platonic Representation Hypothesis](https://arxiv.org/abs/2405.07987)
    - ì—¬ëŸ¬ ëª¨ë¸ë“¤ì˜ representationì´ ìˆ˜ë ´í•œë‹¤ëŠ” ì£¼ì¥. ì—¬ëŸ¬ ë„ë©”ì¸ ë° modalitiesì— ëŒ€í•œ ì‹¤í—˜ ê²°ê³¼ë¥¼ í¬í•¨.
    - ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì˜ ë°œì „ ë°©í–¥ì€ ë°ì´í„° íƒ€ì…(ì–¸ì–´ì˜ ì¢…ë¥˜, modality)ê³¼ ë¬´ê´€í•  ê²ƒì´ë¼ê³  ì£¼ì¥í–ˆë˜ ì‚¬ëŒì´ ìƒê°ë‚¨.
- ğŸ“œÂ [Meta] [Chameleon: Mixed-Modal Early-Fusion Foundation Models](https://arxiv.org/abs/2405.09818)
    - imageì™€ textë¥¼ ì–´ë–¤ ìˆœì„œë¡œ ì œê³µí•˜ë”ë¼ë„ ì´í•´í•˜ê³  ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìƒì„±í•  ìˆ˜ ìˆëŠ” foundation model, Chameleonì„ ê³µê°œ.
    - early-fusion, token-based, mixed-modal ì„¸íŒ…ì„ ìœ„í•´ í•„ìš”í•œ inception, alignment, architectural parameterization ë“±
</details>

<details>
  <summary>4th week</summary>

- ğŸ“œÂ [University of Cambridge] [Zero-Shot Tokenizer Transfer](https://arxiv.org/abs/2405.07883)
    - í•œ ì–¸ì–´ë¡œ í•™ìŠµëœ ì–¸ì–´ ëª¨ë¸ì´ ë‹¤ë¥¸ ì–¸ì–´ëŠ” ì „í˜€ ì²˜ë¦¬í•˜ì§€ ëª»í•œë‹¤ëŠ” í•œê³„ì ì´ ì¡´ì¬
    - tokenizerë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ê³  ì´ì— ëŒ€ì‘í•˜ëŠ” embeddingì„ ì˜ˆì¸¡í•˜ë„ë¡ í•™ìŠµí•˜ëŠ” hypernetworkë¥¼ ì œì•ˆ â†’ encoder & decoder ë‘˜ ë‹¤ì— ì¼ë°˜í™” ê°€ëŠ¥í•˜ë‹¤ëŠ” ê²ƒì„ ì‹¤í—˜ì ìœ¼ë¡œ ì…ì¦
- ğŸ“œÂ [Alibaba] [Language Models can Evaluate Themselves via Probability Discrepancy](https://arxiv.org/abs/2405.10516)
    - ê¸°ì¡´ ë‹µë³€ì„ revise â†’ revised ë‹µë³€ì— ëŒ€í•œ ì¡°ê±´ë¶€ í™•ë¥ ì´ ê¸°ì¡´ ë‹µë³€ì— ëŒ€í•œ ì¡°ê±´ë¶€ í™•ë¥ ë³´ë‹¤ ë†’ë‹¤ë©´ ì¢‹ì€ ë‹µë³€, ê·¸ë ‡ì§€ ì•Šë‹¤ë©´ ë‚˜ìœ ë‹µë³€ìœ¼ë¡œ self-evaluationí•˜ëŠ” ë°©ë²•ë¡ ì„ ì œì•ˆ
- ğŸ“œÂ [Stanford, Toronto] [Observational Scaling Laws and the Predictability of Language Model Performance](https://arxiv.org/abs/2405.10938)
    - ì–¸ì–´ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ scaleì— ë”°ë¼ ì–´ë–»ê²Œ ë³€í™”í• ì§€ë¥¼ ì´í•´í•˜ëŠ” ê²ƒì´ ì¤‘ìš” â†’ 80ê°œ ì˜ publicly available ëª¨ë¸ë“¤ì„ í†µí•´ observational approachë¥¼ í™•ì¸ â†’ ì‹¤í—˜ì„ í†µí•´ smooth, sigmoidal, predictable íŒ¨í„´ì„ ê²€ì¦
- ğŸ§‘ğŸ»â€ğŸ’»Â [Korea Univ.] [Horangi í•œêµ­ì–´ LLM ë¦¬ë”ë³´ë“œ](https://wandb.ai/wandb-korea/korean-llm-leaderboard/reports/-LLM---Vmlldzo3MzIyNDE2?accessToken=95bffmg3gwblgohulknz7go3h66k11uqn1l3ytjma1uj3w0l0dwh1fywgsgpbdyy)
    - W&Bì˜ í…Œì´ë¸” ê¸°ëŠ¥ì„ í™œìš©í•˜ì—¬ í‰ê°€ ê²°ê³¼ë¥¼ ì‰½ê²Œ ë¶„ì„ ê°€ëŠ¥
    - llm-jp-evalì„ ê¸°ë°˜ìœ¼ë¡œ llm-kr-evalì„ êµ¬ì¶•
    - Multi-turn ëŒ€í™”ë¥¼ í†µí•´ ìƒì„± ëŠ¥ë ¥ì„ í‰ê°€í•˜ëŠ” MT-Benchë¥¼ í¬í•¨
- ğŸ“œÂ [Microsoft] [MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2405.12130)
    - PEFTì˜ ëŒ€í‘œ ì£¼ìì¸ LoRAëŠ” LLMì´ ìƒˆë¡œìš´ ì§€ì‹ì„ ìŠµë“í•˜ê³  ê¸°ì–µí•˜ë„ë¡ í•˜ëŠ” ë° ëª…ë°±í•œ í•œê³„ê°€ ì¡´ì¬ â†’ í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ì˜ ìˆ«ìëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ë©´ì„œë„ high-rank updateê°€ ê°€ëŠ¥í•˜ë„ë¡ square matrixë¥¼ ì´ìš©í•˜ëŠ” ë°©ì‹, MoRAë¥¼ ì œì•ˆ
    - LoRAì™€ ë§ˆì°¬ê°€ì§€ë¡œ í•™ìŠµ ì´í›„ì—ëŠ” weight matrixì— merge ë˜ëŠ” ë°©ì‹ì„ ì·¨í•¨.
- ğŸ§‘ğŸ»â€ğŸ’»Â [DeepLearning.AI & Qualcomm] [Introduction to On-Device AI](https://www.deeplearning.ai/short-courses/introduction-to-on-device-ai/)
    - ëª¨ë¸ì„ deploy í•  ë•Œ ë‚®ì€ latencyë¥¼ ìœ ì§€í•˜ë©´ì„œë„ privacyë¥¼ ì§€í‚¬ ìˆ˜ ìˆëŠ” ë°©ë²• ë“±ì„ í•™ìŠµ
- ğŸ§‘ğŸ»â€ğŸ’»Â [llama3-from-scratch](https://github.com/naklecha/llama3-from-scratch)
    - Karpathyê°€ ì¹­ì°¬í•œ repo..?
    - llama3ì˜ êµ¬ì„± ìš”ì†Œë¥¼ í•˜ë‚˜ì”© ê°„ë‹¨íˆ ì‚´í´ë³¼ ìˆ˜ ìˆëŠ” ipynbì„ ì œê³µ. metaë¡œë¶€í„° weightë¥¼ ë°›ì„ ìˆ˜ ìˆëŠ” ê³µì‹ ë§í¬ë„ í¬í•¨ë˜ì–´ ìˆìŒ.
- ğŸ“œÂ [ByteDance, Alibaba] [OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework](https://arxiv.org/abs/2405.11143)
    - LLMì— RLHFë¥¼ í¸í•˜ê²Œ scaling í•˜ê¸° ìœ„í•œ ì˜¤í”ˆì†ŒìŠ¤ í”„ë ˆì„ì›Œí¬. 70B ì´ìƒ ëª¨ë¸ë“¤ë„ ê³ ë ¤.
    - Ray, vLLM, DeepSpeedì™€ ê°™ì€ ë‹¤ì–‘í•œ í•™ìŠµ ê¸°ë²•ë“¤ì„ ë™ì›í•˜ë©° Hugging Faceì™€ë„ í†µí•© ê°€ëŠ¥.
- ğŸ§‘ğŸ»â€ğŸ’»Â [Anthropic] [Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet](https://transformer-circuits.pub/2024/scaling-monosemanticity/)
    - ë¸”ë¡œê·¸ ê¸€ ì›ë³¸ ë§í¬: [Mapping the Mind of a Large Language Model](https://www.anthropic.com/research/mapping-mind-language-model)
    - Claude 3 Sonnetì„ í†µí•´ LLMì˜ interpretabilityì™€ ê´€ë ¨ëœ ì‹¤í—˜ì„ ì§„í–‰í•˜ê³  ê·¸ ê²°ê³¼ë¥¼ report
- ğŸ—ï¸Â [You can now buy a 4-foot-tall humanoid robot for $16K](https://arstechnica.com/gadgets/2024/05/unitree-starts-selling-16000-humanoid-robot/?utm_source=www.theaivalley.com)
    - Unitree G1 ìœ¼ë¡œ ë¶ˆë¦¬ëŠ” íœ´ë¨¸ë…¸ì´ë“œ ë¡œë´‡ì„ 16,000 ë‹¬ëŸ¬ì— êµ¬ë§¤ ê°€ëŠ¥
    - [ë°ëª¨ ì˜ìƒ](https://www.youtube.com/watch?v=GzX1qOIO1bE&t=58s)ì„ ë³´ë©´ êµ‰ì¥íˆ ìì—°ìŠ¤ëŸ½ê³  ë‹¤ì–‘í•œ ë™ì‘ì„ ì§€ì›í•¨ (ìƒë‹¹íˆ ìœ ì—°..;;)
- ğŸ§‘ğŸ»â€ğŸ’»Â [Google] [New AI tools to help merchants market brands and products](https://blog.google/products/shopping/google-generative-ai-marketing-features-may-2024/)
    - ë¸Œëœë“œ ê²€ìƒ‰ ì‹œ ë¸Œëœë“œì™€ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì¼ëª©ìš”ì—°í•˜ê²Œ ì •ë¦¬í•´ì£¼ëŠ” ê¸°ëŠ¥
    - Product Studioì—ì„œ ìƒí’ˆ ì´ë¯¸ì§€ë¥¼ ë‹¤ë¥¸ ë°°ê²½ì´ë‚˜ ìƒí™©ì— ë§ê²Œë” ìƒì„±í•˜ì—¬ ë‹¤ì–‘í•œ ì—°ì¶œì´ ê°€ëŠ¥
- ğŸ§‘ğŸ»â€ğŸ’»Â [Microsoft] [Whatâ€™s next: Microsoft Build continues the evolution and expansion of AI tools for developers](https://blogs.microsoft.com/blog/2024/05/21/whats-next-microsoft-build-continues-the-evolution-and-expansion-of-ai-tools-for-developers/)
    - Small Language Models: Phi-3-vision, Phi-3-small, New Phi-3 model, Phi-Sliica
    - Microsoft Copilots and GitHub Copilot
    - New Copilot + PCs: PyTorch and a new Web Neural Network
    - Real Time intelligence, partnerships with ADM, Khan Academy, Cognition AI
- ğŸ“œÂ [Google DeepMind] [Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context](https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf)
    - Gemini 1.5 Proì˜ technical report. í˜„ì¡´í•˜ëŠ” LLM ì¤‘ ìµœê°•ì´ë¼ê³  ì£¼ì¥
    - ê²½ëŸ‰í™”ëœ ëª¨ë¸, Gemini 1.5 Flashì— ëŒ€í•œ ì‹¤í—˜ ê²°ê³¼ë„ í•¨ê»˜ ì œì‹œ
- ğŸ“œÂ [University of Michigan] [A Turing test of whether AI chatbots are behaviorally similar to humans](https://www.pnas.org/doi/10.1073/pnas.2313925121)
    - ChatGPTì˜ ì¸ê°„ì  íŠ¹ì„±ì„ í™•ì¸í•˜ê¸° ìœ„í•œ Turing Test ê²°ê³¼
- ğŸ§‘ğŸ»â€ğŸ’»Â [Mistral AI] [Mistral-7B-Instruct-v0.3](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3)
    - 32768 vocab size, v3 Tokenizer ì§€ì›, function calling ê°€ëŠ¥
- ğŸ“œÂ [AIRI] [Your Transformer is Secretly Linear](https://arxiv.org/abs/2405.12250)
    - ì—°ì†ëœ layer ì‚¬ì´ì˜ embedding transformationì„ ë¶„ì„í•œ ê²°ê³¼ ê±°ì˜ ì™„ë²½í•œ ì„ í˜• ê´€ê³„ë¥¼ íŒŒì•…í•  ìˆ˜ ìˆì—ˆìŒ
    - ì´ëŸ¬í•œ linear blockì„ ì œê±°í•˜ë”ë¼ë„ ëª¨ë¸ì˜ ì„±ëŠ¥ì— ê±°ì˜ ì˜í–¥ì„ ì£¼ì§€ ì•ŠëŠ”ë‹¤ëŠ” ê²ƒì´ ê´€ì¸¡ë¨
    - pretraining ë‹¨ê³„ì—ì„œ linearityë¥¼ ìµœì†Œí™”í•˜ê¸° ìœ„í•´ cosine-similarity-based regularizationì„ ë„ì…
- ğŸ“œÂ [Xiâ€™an Jiaotong University] [Large Language Models Can Self-Correct with Minimal Effort](https://arxiv.org/abs/2405.14092)
    - ì˜ëª»ëœ responseë¥¼ ìŠ¤ìŠ¤ë¡œ í™•ì¸í•˜ê³  ê³ ì³ë‚˜ê°€ëŠ” verify-then-correct í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆ
- ğŸ“œÂ [MIT] [Not All Language Model Features Are Linear](https://arxiv.org/abs/2405.14860)
    - ìµœê·¼ ì–¸ì–´ ëª¨ë¸ì´ activation spaceì—ì„œ 1ì°¨ì›ì ì¸ representationì„ ê°–ëŠ”ë‹¤ê³  ì£¼ì¥í•˜ëŠ” ì—°êµ¬ë“¤ì´ ì œì‹œë¨
    - ì´ëŸ¬í•œ ì£¼ì¥ê³¼ ë‹¬ë¦¬ ì¼ë¶€ ì–¸ì–´ ëª¨ë¸ë“¤ì€ inherently multi-dimensional representationì„ ê°–ëŠ”ë‹¤ëŠ” ê²ƒì„ ì…ì¦
    â†’ ë…ë¦½ì ì¸ or ë™ì‹œ-ë°œìƒí•˜ì§€ ì•ŠëŠ” lower-dimensional featuresë¡œ decompose ê°€ëŠ¥
- ğŸ“œÂ [Xiâ€™an Jiaotong University] [Quantifying Emergence in Large Language Models](https://arxiv.org/abs/2405.12617v1)
    - ìµœê·¼ì—ëŠ” ì–¸ì–´ ëª¨ë¸ì˜ emergent abilityê°€ ì˜ëª»ëœ í‰ê°€ ì§€í‘œ ì •ì˜ì— ì˜í•œ ê²ƒì´ë¼ëŠ” ì—°êµ¬ê°€ ë§ìŒ
    - â†’ ë³¸ ì—°êµ¬ì—ì„œëŠ” macroscopic(semantic) & microscopic(token) levelì—ì„œ entropy reductionì„ ë¹„êµí•˜ì—¬ strength of emergenceë¥¼ quantify
    - metricì˜ varianceì™€ ICLì—ì„œ shotì˜ ê°œìˆ˜ ë“± ì‚¬ì´ì˜ ìƒê´€ ê³„ìˆ˜ ë“±ì„ ë°”íƒ•ìœ¼ë¡œ novel emergence patternì„ íŒŒì•…í•˜ê³ , ì´ë¥¼ í†µí•´ hallucinationì„ ìƒˆë¡œìš´ ê´€ì ì—ì„œ í•´ì„
- ğŸ§‘ğŸ»â€ğŸ’»Â [phidata](https://github.com/phidatahq/phidata)
    - Autonomous Assistantsë¥¼ êµ¬ì¶•í•˜ëŠ” framework
    - Assistant = LLM + Memory(Chat History, Summaries, ...) + Knowledge(PDF, Docs, â€¦ ) + Tools(Search Web, Send Email, â€¦)
- ğŸ§‘ğŸ»â€ğŸ’»Â [Mistral AI] [mistral-finetune](https://github.com/mistralai/mistral-finetune)
    - ì˜¤í”ˆì†ŒìŠ¤ ë¯¸ìŠ¤íŠ¸ë„ì˜ ëª¨ë¸ì„ LoRA ê¸°ë°˜ìœ¼ë¡œ fine-tuning í•  ìˆ˜ ìˆë„ë¡ ê³µê°œí•œ ì½”ë“œ ë² ì´ìŠ¤
    - ëŒ€ë¶€ë¶„ì˜ íŒŒë¼ë¯¸í„°ëŠ” frozen & 1-2% ì •ë„ì˜ ì¶”ê°€ íŒŒë¼ë¯¸í„°ë¡œ í•™ìŠµ â†’ A100 or H100 ê¶Œì¥
- ğŸ“œÂ [EluetherAI and others] [Lessons from the Trenches on Reproducible Evaluation of Language Models](https://arxiv.org/abs/2405.14782)
    - 3ë…„ ê°„ì˜ LLM í‰ê°€ ê²½í—˜ì„ ë°”íƒ•ìœ¼ë¡œ researcherë“¤ì„ ìœ„í•œ guidanceì™€ lessonì„ ì œê³µ
    - ì–¸ì–´ ëª¨ë¸ í‰ê°€ì˜ ê³µí†µëœ í•œê³„ì , researchì—ì„œì˜ ì–´ë ¤ì›€ì„ ìµœì†Œí™”í•˜ëŠ” ë°©ë²•, ì´ì™€ ê°™ì€ ì´ìŠˆë¥¼ í•´ì†Œí•˜ëŠ” ë° ì í•©í•œ ì˜¤í”ˆì†ŒìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬ Language Model Evaluation Harness (lm-eval)
 
</details>
<details>
  <summary>5th week</summary>

- ğŸ“œÂ [Fudan University] [Aggregation of Reasoning: A Hierarchical Framework for Enhancing Answer Selection in Large Language Models](https://arxiv.org/abs/2405.12939)
    - CoTì˜ í•œê³„ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ hierarchical reasoning aggregation framework, AoR (Aggregation or Reasoning)ì„ ì œì‹œ
    - reasoning chainì— ëŒ€í•œ í‰ê°€ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •ë‹µì„ ê³ ë¥´ëŠ” ë°©ì‹. dynamic sampling í™œìš©.
- ğŸ“œÂ [Cohere] [Cohere For AI Launches Aya 23, 8 and 35 Billion Parameter Open Weights Release](https://cohere.com/blog/aya23)
    - 23ê°œ ì–¸ì–´ë¥¼ ë‹¤ë£° ìˆ˜ ìˆëŠ” 8B, 35B ì‚¬ì´ì¦ˆì˜ ìƒì„±í˜• ì–¸ì–´ ëª¨ë¸ Aya 23ë¥¼ ê³µê°œ
    - ëŒ€ê·œëª¨ multilingual instruction fine-tuning datasetìœ¼ë¡œ í•™ìŠµëœ Aya ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ ë°œì „
    - [technical report on Aya 23](https://cohere.com/research/aya/aya-23-technical-report.pdf?ref=cohere-ai.ghost.io)
- ğŸ“œÂ [National University of Singapore, Salesforce] [Decompose and Aggregate: A Step-by-Step Interpretable Evaluation Framework](https://arxiv.org/abs/2405.15329)
    - LLMì˜ í‰ê°€ ëŠ¥ë ¥ì— ëŒ€í•œ interpretabilityê°€ ë¶€ì¡±
    - â†’ í‰ê°€ ê³¼ì •ì„ ì—¬ëŸ¬ ê°œì˜ ë‹¨ê³„ë¡œ decompose í›„ ê²°ê³¼ë¥¼ aggregate í•˜ëŠ” ë°©ë²•ë¡ ì„ ì œì•ˆ. ì´ë•Œ êµìœ¡í•™ì  ê´€í–‰ì„ ê·¼ê±°ë¡œ ì—¬ëŸ¬ ë‹¨ê³„ë¡œ êµ¬ë¶„.
- ğŸ“œÂ [University of Virginia, Princeton Language and Intelligence] [SimPO: Simple Preference Optimization with a Reference-Free Reward](https://arxiv.org/abs/2405.14734)
    - sequenceì˜ í‰ê·  ë¡œê·¸ í™•ë¥ ì„ implicit rewardë¡œ ì‚¬ìš©í•˜ì—¬ reference modelì„ ê³¼ì •ì—ì„œ ì œì™¸
    - target reward marginì„ ì‚¬ìš©í•˜ì—¬ winning & losing response ê°„ì˜ ê²©ì°¨ë¥¼ ë²Œë¦¼
- ğŸ“œÂ [IEEE] [Wav-KAN: Wavelet Kolmogorov-Arnold Networks](https://arxiv.org/abs/2405.12832)
    - ê¸°ì¡´ MLPë‚˜ Spl-KANì€ interpretability, í•™ìŠµ ì†ë„, robustness ë“±ì˜ ì´ìŠˆê°€ ì¡´ì¬
    - wavelet functionì„ KAN ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°ì— í†µí•©í•¨ìœ¼ë¡œì¨ ì…ë ¥ ë°ì´í„°ì˜ high-/low-frequency ìš”ì†Œë“¤ì„ íš¨ìœ¨ì ìœ¼ë¡œ capture í•  ìˆ˜ ìˆë„ë¡ í•¨
- ğŸ—ï¸Â [xAI] [Series B Funding Round](https://x.ai/blog/series-b)
    - Valor Euquity Partners, Vy Captial ë“±ìœ¼ë¡œë¶€í„° 60ì–µ ë‹¬ëŸ¬ (ì•½ 7-8ì¡°..)ì— í•´ë‹¹í•˜ëŠ” ì‹œë¦¬ì¦ˆ B í€ë”©ì„ í™•ë³´
- ğŸ“œÂ [Fudna University] [Tokenization Matters! Degrading Large Language Models through Challenging Their Tokenization](https://arxiv.org/abs/2405.17067)
    - LLMì´ íŠ¹ì • queryì— ëŒ€í•´ ë‹µë³€ì„ ì˜í•˜ì§€ ëª»í•˜ëŠ” ë¬¸ì œ â†’ tokenizationì´ ì›ì¸
    - ë‹¤ì–‘í•œ ì˜¤í”ˆì†ŒìŠ¤ LLMì´ tokenizationì—ì„œ ê²ªëŠ” ì–´ë ¤ì›€ì„ í…ŒìŠ¤íŠ¸í•˜ê¸° ìœ„í•œ ADT (Adversarial Dataset for Tokenizer) êµ¬ì¶•
- ğŸ“œÂ [Google] [Can Large Language Models Faithfully Express Their Intrinsic Uncertainty in Words?](https://arxiv.org/abs/2405.16908)
    - LLMì€ ë‹µë³€í•˜ê¸° ì• ë§¤í•œ ê²ƒë“¤ì— ëŒ€í•´ intrinsic uncertaintyë¥¼ í‘œí˜„í•´ì•¼ í•œë‹¤ê³  ì£¼ì¥
    - intrinsic uncertaintyë¥¼ í™•ì¸í•˜ê¸° ìœ„í•´ ëª¨ë¸ì˜ intrinsic confidenceì™€ ì‹¤ì œ ê²°ì • ê°„ì˜ ê°­ì„ ì¸¡ì •í•  ìˆ˜ ìˆëŠ” faithful response uncertaintyë¥¼ ê³µì‹í™”í•˜ì—¬ ì‹¤í—˜
- ğŸ“œÂ [Meta] [An Introduction to Vision-Language Modeling](https://arxiv.org/abs/2405.17247)
    - ë©”íƒ€ì—ì„œ ì œì‹œí•œ Vision-Language Modeling ê´€ë ¨ ì„œë² ì´ í˜ì´í¼
- ğŸ“œÂ [Microsoft] Matryoshka Multimodal Models
    - Large Multimodal Models(LMMs)ì´ ê³ í•´ìƒë„ ì´ë¯¸ì§€ë¥¼ ì²˜ë¦¬í•  ë•Œ ë„ˆë¬´ ë§ì€ visual tokenì„ í•™ìŠµí•´ì•¼ í•œë‹¤ëŠ” ë¬¸ì œì ì´ ì¡´ì¬
    - Matryoshka ì¸í˜•ì— ì°©ì•ˆ. visual contentë¥¼ ì—¬ëŸ¬ coarse-to-fine granularities ì •ë³´ë¡œë¶€í„°ì˜ nested sets of visual tokensë¡œ í‘œí˜„í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµ.
- ğŸ§‘ğŸ»â€ğŸ’»Â [DeepLearning.AI] [AI Agentic Design Patterns with AutoGen](https://www.deeplearning.ai/short-courses/ai-agentic-design-patterns-with-autogen/)
    - AutoGen í”„ë ˆì„ì›Œí¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ì–‘í•œ ì—­í• ì„ ìˆ˜í–‰í•˜ê³  ë›°ì–´ë‚œ ëŠ¥ë ¥ì„ ê°€ì§„ AI applicationì„ ë§Œë“œëŠ” ë°©ë²•ì„ í•™ìŠµ
    - Reflection, Tool use, Planning ë“± ë‹¤ì–‘í•œ agentic design patternì— ëŒ€í•´ í•™ìŠµ
- ğŸ“œÂ [National University of Singapore] [Faithful Logical Reasoning via Symbolic Chain-of-Thought](https://arxiv.org/abs/2405.18357)
    - LLMì˜ logical reasoning ëŠ¥ë ¥ì„ ê°•í™”í•˜ê¸° ìœ„í•´ SymbCoTë¥¼ ì œì•ˆ
    - 1) ìì—°ì–´ë¥¼ symbolic formatìœ¼ë¡œ ë³€ê²½ 2) ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ step-by-step planì„ êµ¬ì¶• 3) verifierê°€ translation & reasoning chainì˜ ê²°ê³¼ë¥¼ ê²€ì¦
- ğŸ§‘ğŸ»â€ğŸ’»Â [Karpathy] [Reproducing GPT-2 (124M) in llm.c in 90 minutes for $20](https://github.com/karpathy/llm.c/discussions/481)
    - 124M: 90m, $20 / 350M: 14h, $200 / 1.6B: 1w, $2.5k
    - 124M ì‚¬ì´ì¦ˆì˜ GPT-2ë¥¼ A100x8ë¥¼ ì‚¬ìš©í•˜ì—¬ ì—„ì²­ë‚˜ê²Œ íš¨ìœ¨ì ìœ¼ë¡œ í•™ìŠµí•˜ëŠ” ë°©ì‹ì„ ê³µê°œ
- ğŸ§‘ğŸ»â€ğŸ’»Â [Mistral AI] [Codestral: Hello, World!](https://mistral.ai/news/codestral/)
    - 80ê°œ ì´ìƒì˜ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë¥¼ ë‹¤ë£° ìˆ˜ ìˆëŠ” ì½”ë“œ íŠ¹í™” ì–¸ì–´ ëª¨ë¸ì„ ê³µê°œ
    - 22B ì‚¬ì´ì¦ˆì˜ ëª¨ë¸ì„ì—ë„ ë¶ˆêµ¬í•˜ê³  Llama 3 70B, CodeLlama 70B ë³´ë‹¤ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì„
    - [í—ˆê¹…í˜ì´ìŠ¤](https://huggingface.co/mistralai/Codestral-22B-v0.1)ì—ì„œ ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥
- ğŸ“œÂ [The University of Edinburgh] [2BP: 2-Stage Backpropagation](https://arxiv.org/abs/2405.18047)
    - Deep Neural Networks(DNNs)ë¥¼ í•™ìŠµì‹œí‚¤ê¸° ìœ„í•œ ê¸°ì¡´ì˜ pipeline parallelismì€ ML í”„ë ˆì„ì›Œí¬ì— ë‚´ì¥ëœ automatic differentiationì— ì˜í•œ ë³‘ëª©ì´ ë°œìƒ
    - â†’ 2-stage backporpagation(2BP)ì„ ì œì•ˆ. ì´ë¥¼ í†µí•´ 1.70x í–¥ìƒëœ throughputì„ í™•ì¸
- ğŸ—ï¸Â [OpenAI] [OpenAI makes ChatGPT-4o's advanced tools available to users in free tier](https://www.business-standard.com/technology/tech-news/openai-makes-chatgpt-4o-s-advanced-tools-available-to-users-in-free-tier-124053000880_1.html)
    - ì´ì œ êµ¬ë…ì„ í•˜ì§€ ì•ŠëŠ” ì¼ë°˜ ìœ ì €ë“¤ë„ GPT-4o ëª¨ë¸ì„ ì´ìš©í•  ìˆ˜ ìˆìŒ
    - ë˜í•œ browse, vision, data analysis, file uploads, GPTs ë“±ì˜ ê¸°ëŠ¥ë„ ì´ìš© ê°€ëŠ¥
- ğŸ“œÂ [Meta] [Nearest Neighbor Speculative Decoding for LLM Generation and Attribution](https://arc.net/l/quote/bobbepsa)
    - LLMì˜ hallucination ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ kNN-LMê³¼ ê°™ì€ semi-parametric LMì´ ë“±ì¥í•˜ì˜€ìœ¼ë‚˜ inference ì†ë„ê°€ ëŠë¦¬ê³  non-fluent textsë¥¼ ìƒì„±í•œë‹¤ëŠ” ë¬¸ì œì ì´ ì¡´ì¬
    - ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì„ì˜ ê¸¸ì´ì˜ real-world text spansë¥¼ LM ìƒì„± ê³¼ì •ì— í†µí•©í•˜ëŠ” Nearest Neighbor Speculative Decoding (NEST)ë¥¼ ì œì•ˆ â†’ token-levelì˜ retrievalì„ ë§¤ inference stepë§ˆë‹¤ ìˆ˜í–‰
- ğŸ“œÂ [Adobe] [Calibrating Reasoning in Language Models with Internal Consistency](https://arc.net/l/quote/tmcvuipx)
    - CoT reasoningì— ëŒ€í•œ ëª¨ë¸ì˜ internal representationì— ëŒ€í•œ ì—°êµ¬
    - â†’ rationaleì€ ì •ë‹µ accuracyë¥¼ í–¥ìƒì‹œí‚¤ì§€ë§Œ, ì¤‘ê°„ê³¼ ë§ˆì§€ë§‰ ë ˆì´ì–´ internal representation ê°„ì˜ inconsistencyë¥¼ ì•¼ê¸°í•¨
</details>


## ğŸŒ June
<details>
  <summary>1st week</summary>

- ğŸ“œÂ [Renmin University] [One Token Can Help! Learning Scalable and Pluggable Virtual Tokens for Retrieval-Augmented Large Language Models](https://arxiv.org/abs/2405.19670)
    - ê¸°ì¡´ LLMì€ fine-tuning í•  ê²½ìš° ê¸°ì¡´ ì§€ì‹ì´ ì†ìƒë  ê°€ëŠ¥ì„±ì´ ë†’ë‹¤ëŠ” ë¬¸ì œì ì´ ì¡´ì¬
    - RAGë¥¼ ìœ„í•œ scalable & pluggable ê°€ìƒ í† í°ì„ ì œì•ˆ. í•´ë‹¹ í† í°ì— ëŒ€í•œ ì„ë² ë”©ë§Œ fine-tuning
- ğŸ“œÂ [Jina AI] [Jina CLIP: Your CLIP Model Is Also Your Text Retriever](https://arxiv.org/abs/2405.20204)
    - Contrastive Language-Image Pretraining(CLIP)ì„ text-only taskì— ì ìš© ê°€ëŠ¥. í•˜ì§€ë§Œ text-only ë˜ëŠ” multimodal tasksì— ë”°ë¼ ë…ë¦½ëœ embeddingì„ ìœ ì§€í•´ì•¼ í•œë‹¤ëŠ” ë¬¸ì œì  ì¡´ì¬.
    - â†’ ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ multi-task contrastive training methodë¥¼ ì œì•ˆ
- ğŸ§‘ğŸ»â€ğŸ’»Â [Anthropic] [Claude can now use tools](https://www.anthropic.com/news/tool-use-ga)
    - Claudeì—ë„ ì™¸ë¶€ APIë‚˜ toolê³¼ ì—°ë™í•  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ì´ ì¶”ê°€ë¨
    - ì˜ˆë¥¼ ë“¤ì–´ êµ¬ì¡°í™”ëœ ë°ì´í„° ì¶”ì¶œ, DB ê¸°ë°˜ ê²€ìƒ‰ ë° ë‹µë³€, API ê¸°ëŠ¥ ìë™í™” ë“±ì— í™œìš© ê°€ëŠ¥
- ğŸ§‘ğŸ»â€ğŸ’»Â [Perplexity] [Introducing Perplexity Pages](https://www.perplexity.ai/hub/blog/perplexity-pages)
    - í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ìœ¼ë¡œ ì»¤ìŠ¤í…€ ê°€ëŠ¥í•œ ì›¹ í˜ì´ì§€ë¥¼ ì œì‘í•˜ëŠ” ê¸°ëŠ¥ Pagesë¥¼ ì˜¤í”ˆ

</details>

<details>
  <summary>2nd week</summary>
  
- [Meta] [Contextual Position Encoding: Learning to Count Whatâ€™s Important](https://arxiv.org/abs/2405.18719)
    - í˜„ì¬ì˜ Position Encoding (PE) ë°©ì‹ì€ í† í° ê°œìˆ˜ë¥¼ ì„¸ëŠ” ë°©ì‹ìœ¼ë¡œ ì¼ë°˜í™”ê°€ ì–´ë µë‹¤ëŠ” ë¬¸ì œì 
    - â†’ ëª¨ë¸ì— ì˜í•´ ê²°ì •ë˜ëŠ” íŠ¹ì • í† í°ì— ëŒ€í•œ positionë§Œ í™•ì¥í•¨ìœ¼ë¡œì¨ positionì´ contextì— conditioned ë  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” Contextual Position Encoding(CoPE)ë¥¼ ì œì•ˆ
- ğŸ—ï¸Â [Samsung] [Samsungâ€™s Galaxy S24 Series Dominates GenAI-capable Smartphone Market in Q1 2024](https://www.counterpointresearch.com/insights/global-top-10-best-selling-genai-smartphones-q1-2024/)
    - 2024ë…„ë„ 1ë¶„ê¸° ìŠ¤ë§ˆíŠ¸í° ì‹œì¥ì—ì„œ GenAI ìŠ¤ë§ˆíŠ¸í°ì˜ ë¹„ì¤‘ì€ ì•½ 6% ì •ë„. ì´ì— ëŒ€í•œ ì‚¼ì„±ì˜ ì§€ë¶„ì€ 50% ì´ìƒì„.
    - AI ê¸°ìˆ  ë°œì „ì„ ë‚´ì„¸ìš¸ ê²ƒìœ¼ë¡œ ì˜ˆìƒë˜ëŠ” ì• í”Œì˜ WWDCê°€ ë§ì€ ì´ë“¤ì˜ ê¸°ëŒ€ë¥¼ ë°›ê³  ìˆìŒ
- ğŸ“œÂ [Princeton, CMU] [Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality](https://arc.net/l/quote/avdoajmy)
    - Mambaì˜ ì €ìê°€ í›„ì† ì—°êµ¬ë¡œ ì œì‹œí•œ Mamba-2
    - í•µì‹¬ ë ˆì´ì–´ì˜ ì—°ì‚° ì†ë„ê°€ Mambaì˜ selective SSMë³´ë‹¤ 2-8ë°° ì •ë„ ë¹ ë¥´ë©´ì„œ, íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ì˜ ì–¸ì–´ ëª¨ë¸ê³¼ ê²¬ì¤„ ìˆ˜ ìˆëŠ” ì„±ëŠ¥ì„ ë‚´ì„¸ì›€
- ğŸ“œÂ [Perdue] [SaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales](https://arxiv.org/abs/2405.20974)
    - LLMì˜ confidenceì™€ ê´€ë ¨í•´ì„œ prompt-based ì—°êµ¬ì™€ supervised finetuning ì—°êµ¬ê°€ ì¡´ì¬
    - â†’ fine-grained confidence estimatesë¥¼ í‘œí˜„í•˜ë„ë¡ ê°€ë¥´ì¹˜ëŠ” SaySelf ë°©ë²•ë¡ ì„ ì œì•ˆ
    - ì¶”ê°€ì ìœ¼ë¡œ LLMì€ ìŠ¤ìŠ¤ë¡œì˜ parametric knowledgeë¥¼ ë‚˜íƒ€ë‚´ëŠ” self-reflective rationaleì„ ìƒì„±í•˜ê³ , ë°˜ëŒ€ë¡œ uncertaintyë¥¼ í‘œí˜„í•  ìˆ˜ ìˆê²Œ ë¨
- ğŸ§‘ğŸ»â€ğŸ’»Â [LlamaIndex] [Introducing the Property Graph Index: A Powerful New Way to Build Knowledge Graphs with LLMs](https://www.llamaindex.ai/blog/introducing-the-property-graph-index-a-powerful-new-way-to-build-knowledge-graphs-with-llms)
    - ê·¸ë˜í”„ë¥¼ êµ¬ì„±í•˜ëŠ” ë…¸ë“œ ë° ê´€ê³„ë¥¼ categorize
    - ê·¸ë˜í”„ë¥¼ hybrid searchë¥¼ ìœ„í•œ vector databaseë¡œ ì‚¬ìš© ê°€ëŠ¥
    - Cypher graph query languageë¥¼ ì´ìš©í•œ ë³µì¡í•œ query í‘œí˜„ ê°€ëŠ¥
- ğŸ§‘ğŸ»â€ğŸ’»Â [DeepLearning.AI] [AI Agents in LangGraph](https://www.deeplearning.ai/short-courses/ai-agents-in-langgraph/)
    - Pythonê³¼ LLMì„ ì´ìš©í•˜ì—¬ Agentë¥¼ êµ¬í˜„í•˜ëŠ” ê²ƒì„ scratchë¶€í„° í•™ìŠµ
    - ì¶”ê°€ë¡œ, ì—¬ëŸ¬ ê°œì˜ ë‹µë³€ì„ agent-friendly í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•˜ëŠ” agent serarchë„ ë‹¤ë£¸
- ğŸ“œÂ [ByteDance] [Exploring Mathematical Extrapolation of Large Language Models with Synthetic Data](https://arxiv.org/abs/2406.02100)
    - ìƒˆë¡œ ì œì‹œí•œ arithmetical puzzle problemì„ í†µí•´ LLMì´ ê³ í’ˆì§ˆ í•©ì„±ë°ì´í„°ë¡œ í•™ìŠµëœ ê²½ìš° multi-step reasoning ëŠ¥ë ¥ì„ í¬ê²Œ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŒì„ í™•ì¸
    - ë˜í•œ ì¶”ê°€ ì‹¤í—˜ì„ í†µí•´ out-of-domain ë°ì´í„°ì…‹ì— ëŒ€í•œ ì„±ëŠ¥ë„ ì¤€ìˆ˜í•˜ë‹¤ëŠ” ê²ƒì„ í™•ì¸
- ğŸ“œÂ [Google DeepMind] [To Believe or Not to Believe Your LLM](https://arxiv.org/abs/2406.02543)
    - ì–¸ì–´ ëª¨ë¸ ë‹µë³€ì˜ ë¶ˆí™•ì‹¤ì„±ì€ epistemic (ì§€ì‹ ë¶€ì¡±) & aleatoric (ëœë¤, í™•ë¥ ) uncertaintyë¡œ êµ¬ë¶„ë¨
    - information-theoretic metricì„ ì‚¬ìš©í•˜ì—¬ ì–¸ì œ epistemic uncertaintyê°€ ë†’ì€ì§€ë¥¼ íƒì§€
    - ì´ì „ì˜ ë‹µë³€ì„ ê¸°ë°˜ìœ¼ë¡œ ì‚¼ëŠ” iterative promptingì„ í†µí•´ metricì„ ê³„ì‚°. ì¦‰, log-likelihood ë“±ì„ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ.
- ğŸ§‘ğŸ»â€ğŸ’»Â [Google] [PlaiGemma](https://ai.google.dev/gemma/docs/paligemma)
    - SigLIP vision modelê³¼ Gemma language modelì„ ê¸°ë°˜ìœ¼ë¡œ ë§Œë“  lightweight open vision-language model (VLM), PaliGemmaë¥¼ ê³µê°œ
    - ë‹¤ì–‘í•œ íƒœìŠ¤í¬ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” PaliGemmaì™€ íŠ¹ì • research datasetì— fine-tuned PaliGemma-FTë¥¼ ê³µê°œ
    - [ìºê¸€](https://www.kaggle.com/models/google/paligemma)ì—ì„œ ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥
- ğŸ§‘ğŸ»â€ğŸ’»Â [Mistral AI] [My Tailor is Mistral](https://mistral.ai/news/customization/)
    - Mistral fine-tuning API & SDKë¥¼ ì´ìš©í•˜ì—¬ Mistral ëª¨ë¸ì„ fine-tuning í•˜ëŠ” ê¸°ëŠ¥ì„ ê³µê°œ
    - LoRAë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ì—¬ memory-efficient í•˜ë©´ì„œë„ performantí•œ fine-tuning ê¸°ë²•ì„ ë„ì…
- ğŸ“œÂ [KAIST, LG AI] [Block Transformer: Global-to-Local Language Modeling for Fast Inference](https://arxiv.org/abs/2406.02657)
    - LLMì˜ inferenceì—ì„œ KV cacheëŠ” ì‹¬ê°í•œ ë³‘ëª©ì˜ ì›ì¸ì´ ë¨
    - â†’ ë‚®ì€ layerì— ëŒ€í•œ global modelingì˜ ë³‘ëª©ì„ ê³ ë¦½ì‹œí‚¤ê³ , ìƒìœ„ layerì— ëŒ€í•´ fast local modelingì„ ì ìš©. ì…ë ¥ í† í°ì„ íŠ¹ì • ì‚¬ì´ì¦ˆì˜ ë¸”ë¡ìœ¼ë¡œ ì••ì¶•í•˜ê³  coarse levelë¡œ self attentionì„ ì ìš©.
- ğŸ§‘ğŸ»â€ğŸ’»ğŸ“œÂ [OpenAI] [Extracting Concepts from GPT-4](https://openai.com/index/extracting-concepts-from-gpt-4/)
    - ì•„ì¹´ì´ë¸Œ ë…¼ë¬¸ [ë§í¬](https://arxiv.org/abs/2406.04093v1) ğŸ”—
    - GPT-4ì˜ internal representationì„ 16M ê°œì˜ oft-interpretable patternìœ¼ë¡œ decomposeí•˜ê¸° ìœ„í•´ ê³ ì•ˆí•œ scalable methodë¥¼ ê³µê°œ
    - k-sparse autoencodersë¥¼ ì œì•ˆí•˜ì—¬ sparsityë¥¼ control í•¨ê³¼ ë™ì‹œì— reconstruction-sparsity frontierë¥¼ tuningí•˜ê³  ê°œì„ í•˜ëŠ” ê³¼ì •ì„ ê°„ì†Œí™”
    - autoencoderì˜ í¬ê¸°ì™€ sparsity ê°„ì˜ í™•ì—°í•œ scaling lawsë¥¼ ê´€ì¸¡
- ğŸ§‘ğŸ»â€ğŸ’»Â [Google] [NotebookLM goes global with Slides support and better ways to fact-check](https://blog.google/technology/ai/notebooklm-goes-global-support-for-websites-slides-fact-check/)
    - ì‘ë…„ ì—¬ë¦„ì— ê³µê°œí–ˆë˜ NotebookLMì„ Gemini 1.5 Pro ì—…ê·¸ë ˆì´ë“œ
    - Google Slide, web URL, Google Docs, PDFs, text filesë¥¼ ì§€ì›
    - [NotebookLM ë§í¬](https://notebooklm.google.com/?original_referer=https://blog.google%23&pli=1)ğŸ”—ì—ì„œ ê°€ì´ë“œ í™•ì¸ ë° ë…¸íŠ¸ë¶ ìƒì„± ê°€ëŠ¥
- ğŸ“œÂ [ELLIS] [Semantically Diverse Language Generation for Uncertainty Estimation in Language Models](https://arxiv.org/abs/2406.04306)
    - LLMì˜ ì˜ˆì¸¡ ë¶ˆí™•ì‹¤ì„±ì„ ì •ëŸ‰ì ìœ¼ë¡œ ì¸¡ì •í•˜ê¸° ìœ„í•´ Semantically Diverse Language Generation (SDLG)ë¥¼ ì œì•ˆ
    - ì´ë¥¼ í†µí•´ initial textê°€ hallucinated ì¸ì§€ ì•„ë‹Œì§€ íŒë‹¨í•  ìˆ˜ ìˆìŒ
- ğŸ“œÂ [Peking, Berkeley, Stanford] [Buffer of Thoughts: Thought-Augmented Reasoning with Large Language Models](https://arxiv.org/abs/2406.04271)
    - thought-augmented reasoning approach, Buffer of Thoughts (BoT)ë¥¼ ì œì•ˆ
    - meta-buffer: ìœ ìµí•œ high-level thoughtsë¥¼ ì €ì¥
    - buffer-manager: meta-bufferë¥¼ ë™ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•˜ì—¬ meta-bufferì˜ capacityë¥¼ í–¥ìƒ

</details>
